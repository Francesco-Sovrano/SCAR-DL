{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt -U\n",
    "!python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import core lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./core')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import os\n",
    "import tf_metrics\n",
    "from imblearn import over_sampling, under_sampling, combine\n",
    "from collections import Counter\n",
    "from tensorflow.core.util import event_pb2\n",
    "from tensorflow.python.lib.io import tf_record\n",
    "from misc.doc_reader import get_document_list\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#import random\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from models.role_pattern_extractor import RolePatternExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppress tensorflow warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spacy model <en_core_web_md>...\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 2\n",
    "N_SPLITS = 5\n",
    "USE_TEST_SET = True\n",
    "USE_PATTERN_EMBEDDING = True\n",
    "\n",
    "ZERO_CLASS = 'none'\n",
    "LABELS_TO_EXCLUDE = [\n",
    "\t#'cites',\n",
    "\t'cites_as_review',\n",
    "\t#'extends', \n",
    "\t#'uses_data_from', \n",
    "\t#'uses_method_in',\n",
    "]\n",
    "OVERSAMPLE = False\n",
    "UNDERSAMPLE = False\n",
    "\n",
    "TRAIN_EPOCHS = None\n",
    "EVALUATION_SECONDS = 5\n",
    "MAX_STEPS = (10**5)/2\n",
    "EVALUATION_STEPS = MAX_STEPS/10\n",
    "MODEL_DIR = './model'\n",
    "TF_MODEL = 'USE_MLQA'\n",
    "MODEL_MANAGER = RolePatternExtractor({'tf_model':TF_MODEL})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for converting input datasets from csv to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(dataset_file):\n",
    "\t# Load dataset\n",
    "\tdf = pd.read_csv(dataset_file, sep='\t')\n",
    "\t#print(df.dtypes)\n",
    "\n",
    "\t# Get target values list\n",
    "\tdf['citfunc'].replace(np.NaN, 'none', inplace=True)\n",
    "\tdf['citfunc'] = df['citfunc'].map(lambda x: x.strip())\n",
    "\t# Remove rows with excluded labels\n",
    "\tfor label in LABELS_TO_EXCLUDE:\n",
    "\t\tdf.loc[df.citfunc == label, 'citfunc'] = ZERO_CLASS\n",
    "\t# Remove bad rows\n",
    "\tdf['citfunc'].replace('ERROR', 'none', inplace=True)\n",
    "\tdf = df[df.citfunc != 'none']\n",
    "\t# Extract target list\n",
    "\ttarget_list = df.pop('citfunc').values.tolist()\n",
    "\n",
    "\t# Extract features from dataframe\n",
    "\tdf = df[['anchorsent','sectype']]\n",
    "\t\n",
    "\t# Remove null values\n",
    "\tdf['anchorsent'].replace(np.NaN, '', inplace=True)\n",
    "\tdf['sectype'].replace(np.NaN, 'none', inplace=True)\n",
    "\n",
    "\tdf = df[df.anchorsent != '']\n",
    "\tdf['anchorsent'] = df['anchorsent'].map(lambda x: re.sub(r'\\[\\[.*\\]\\]','',x))\n",
    "\tdf['anchorsent'] = df['anchorsent'].map(lambda x: re.sub(r'[^\\x00-\\x7F]+',' ',x))\n",
    "\n",
    "\tif USE_PATTERN_EMBEDDING:\n",
    "\t\textra_list = []\n",
    "\t\tfor text in df['anchorsent'].values:\n",
    "\t\t\textra = list(Counter(pattern['predicate'] for pattern in MODEL_MANAGER.get_role_pattern_list(text)).keys())\n",
    "\t\t\textra_list.append(extra[0] if len(extra)>0 else '')\n",
    "\t\tdf['main_predicate'] = extra_list\n",
    "\t\n",
    "\t# Print dataframe\n",
    "\tprint('Dataframe')\n",
    "\tprint(df)\n",
    "\t\n",
    "\t# Return dataset\n",
    "\tfeature_list = df.columns.values.tolist()\n",
    "\tx_dict = {feature: df[feature].tolist() for feature in feature_list}\n",
    "\ty_list = target_list\n",
    "\treturn {'x':x_dict, 'y':y_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for casting dataset to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpyfy_dataset(set):\n",
    "\tset['x'] = {k: np.array(v) for k,v in set['x'].items()}\n",
    "\tset['y'] = np.array(set['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for encoding a dataset, from string to numerical representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(dataset):\n",
    "\t# Embed anchor sentences into vectors\n",
    "\tfor key,value in dataset.items():\n",
    "\t\tdf = value['x']\n",
    "\t\t# Embed anchor sentences\n",
    "\t\tcache_file = f'{TF_MODEL}.{key}.anchorsent.embedding_cache.pkl'\n",
    "\t\tif os.path.isfile(cache_file):\n",
    "\t\t\twith open(cache_file, 'rb') as f:\n",
    "\t\t\t\tembedded_sentences = pickle.load(f)\n",
    "\t\telse:\n",
    "\t\t\tdf['anchorsent'] = list(df['anchorsent'])\n",
    "\t\t\tembedded_sentences = MODEL_MANAGER.embed(df['anchorsent'])\n",
    "\t\t\twith open(cache_file, 'wb') as f:\n",
    "\t\t\t\tpickle.dump(embedded_sentences, f)\n",
    "\t\tdf['anchorsent'] = embedded_sentences\n",
    "\t\t# Embed extra info\n",
    "\t\tif USE_PATTERN_EMBEDDING:\n",
    "\t\t\tcache_file = f'{TF_MODEL}.{key}.extra.embedding_cache.pkl'\n",
    "\t\t\tif os.path.isfile(cache_file):\n",
    "\t\t\t\twith open(cache_file, 'rb') as f:\n",
    "\t\t\t\t\tembedded_extra = pickle.load(f)\n",
    "\t\t\telse:\n",
    "\t\t\t\tdf['main_predicate'] = list(df['main_predicate'])\n",
    "\t\t\t\tembedded_extra = MODEL_MANAGER.embed(df['main_predicate'])\n",
    "\t\t\t\twith open(cache_file, 'wb') as f:\n",
    "\t\t\t\t\tpickle.dump(embedded_extra, f)\n",
    "\t\t\tdf['main_predicate'] = embedded_extra\n",
    "\n",
    "\t# Encode labels\n",
    "\tlabel_encoder_target = LabelEncoder()\n",
    "\tlabel_encoder_target.fit([e for set in dataset.values() for e in set['y']])\n",
    "\tprint('Label classes:', list(label_encoder_target.classes_))\n",
    "\tfor set in dataset.values():\n",
    "\t\tset['y'] = label_encoder_target.transform(set['y'])\n",
    "\n",
    "\t# Encode sectypes\n",
    "\tall_sectypes = [e for set in dataset.values() for e in set['x']['sectype']]\n",
    "\tlabel_encoder_sectype = LabelEncoder()\n",
    "\tall_sectypes = label_encoder_sectype.fit_transform(all_sectypes)\n",
    "\tonehot_encoder_sectype = OneHotEncoder()\n",
    "\tonehot_encoder_sectype.fit(all_sectypes.reshape(-1, 1))\n",
    "\tprint('Sectype classes:', list(label_encoder_sectype.classes_))\n",
    "\tfor set in dataset.values():\n",
    "\t\tlabeled_sectypes = label_encoder_sectype.transform(set['x']['sectype'])\n",
    "\t\tset['x']['sectype'] = onehot_encoder_sectype.transform(labeled_sectypes.reshape(-1, 1)).toarray()[:,1:]\n",
    "\n",
    "\t# Input features to numpy array\n",
    "\tfor set in dataset.values():\n",
    "\t\tnumpyfy_dataset(set)\n",
    "\t# Return number of target classes\n",
    "\treturn len(label_encoder_target.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for resampling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_dataset(set, oversample=True, undersample=True):\n",
    "\t#numpyfy_dataset(set)\n",
    "\tprint('Dataset size before re-sampling:', len(set['y']))\n",
    "\n",
    "\t# Build combined features\n",
    "\tcombined_features_sizes = {}\n",
    "\tcombined_features_list = []\n",
    "\tfor feature in zip(*set['x'].values()):\n",
    "\t\tcombined_features = []\n",
    "\t\tfor e,data in enumerate(feature):\n",
    "\t\t\tif type(data) in [np.ndarray,list,tuple]:\n",
    "\t\t\t\tdata_list = list(data)\n",
    "\t\t\t\tcombined_features.extend(data_list)\n",
    "\t\t\t\tcombined_features_sizes[e] = (len(data_list), type(data[0]))\n",
    "\t\t\telse:\n",
    "\t\t\t\tcombined_features.append(data)\n",
    "\t\t\t\tcombined_features_sizes[e] = (1, type(data))\n",
    "\t\tcombined_features_list.append(combined_features)\n",
    "\t#print(combined_features_list[0])\n",
    "\n",
    "\t# Oversample data\n",
    "\tcombined_features_list = np.array(combined_features_list, dtype=np.object)\n",
    "\t#combined_features_list, set['y'] = over_sampling.RandomOverSampler(sampling_strategy='all').fit_sample(combined_features_list, set['y'])\n",
    "\tif oversample and undersample:\n",
    "\t\tcombined_features_list, set['y'] = combine.SMOTETomek().fit_sample(combined_features_list, set['y'])\n",
    "\telif oversample:\n",
    "\t\tcombined_features_list, set['y'] = over_sampling.ADASYN().fit_sample(combined_features_list, set['y'])\n",
    "\telif undersample:\n",
    "\t\tcombined_features_list, set['y'] = under_sampling.NeighbourhoodCleaningRule().fit_sample(combined_features_list, set['y'])\n",
    "\n",
    "\t# Separate features\n",
    "\tnew_combined_features_list = []\n",
    "\tfor combined_features in combined_features_list:\n",
    "\t\tnew_combined_features = []\n",
    "\t\tstart = 0\n",
    "\t\tfor e,(size,dtype) in combined_features_sizes.items():\n",
    "\t\t\tfeature = combined_features[start:start+size]\n",
    "\t\t\tif size > 1:\n",
    "\t\t\t\t#feature = np.array(feature, dtype=dtype)\n",
    "\t\t\t\tfeature = np.array(feature, dtype=np.float32)\n",
    "\t\t\telse:\n",
    "\t\t\t\tfeature = feature[0]\n",
    "\t\t\tnew_combined_features.append(feature)\n",
    "\t\t\tstart += size\n",
    "\t\tnew_combined_features_list.append(new_combined_features)\n",
    "\t#print(new_combined_features_list[0])\n",
    "\tseparated_features = list(zip(*new_combined_features_list))\n",
    "\n",
    "\tfor feature, value in zip(set['x'].keys(), separated_features):\n",
    "\t\tset['x'][feature] = value\n",
    "\tprint('Dataset size after re-sampling:', len(set['y']))\n",
    "\tnumpyfy_dataset(set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for getting the dataframe feature shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_feature_shape(df, feature):\n",
    "\tfirst_element = df[feature][0]\n",
    "\tshape = first_element.shape if type(first_element) is np.ndarray else ()\n",
    "\treturn tf.feature_column.numeric_column(feature, shape=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to convert a data-set into a data-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify_dataset(dataset):\n",
    "\tdataset_xs = zip(*dataset['x'].values())\n",
    "\tdataset_xs = map(lambda x: tuple((k,v) for k,v in zip(dataset['x'].keys(),x)), dataset_xs)\n",
    "\treturn list(zip(dataset_xs, dataset['y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to convert a data-set into a data-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictify_datalist(datalist):\n",
    "\txs, y = zip(*datalist)\n",
    "\ty_list = np.array(y)\n",
    "\txs = zip(*xs)\n",
    "\txs_dict = {}\n",
    "\tfor x_tuples in xs:\n",
    "\t\tfeature_names, x_tuples = zip(*x_tuples)\n",
    "\t\tfeature = feature_names[0]\n",
    "\t\txs_dict[feature] = np.array(x_tuples)\n",
    "\t\t#print(feature, len(xs_dict[feature]))\n",
    "\t#print('y', len(y_list))\n",
    "\treturn {\n",
    "\t\t'x': xs_dict,\n",
    "\t\t'y': y_list\n",
    "\t}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the DNN classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_fn(feature_columns, n_classes):\n",
    "\tdef model_fn(\n",
    "\t\tfeatures, # This is batch_features from input_fn\n",
    "\t\tlabels,   # This is batch_labels from input_fn\n",
    "\t\tmode):\t# And instance of tf.estimator.ModeKeys, see below\n",
    "\n",
    "\t\tif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "\t\t\ttf.logging.info(\"my_model_fn: PREDICT, {}\".format(mode))\n",
    "\t\telif mode == tf.estimator.ModeKeys.EVAL:\n",
    "\t\t\ttf.logging.info(\"my_model_fn: EVAL, {}\".format(mode))\n",
    "\t\telif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\t\t\ttf.logging.info(\"my_model_fn: TRAIN, {}\".format(mode))\n",
    "\n",
    "\t\t# Create the layer of input\n",
    "\t\tinput_layer = tf.feature_column.input_layer(features, feature_columns)\n",
    "\t\t#input_layer = tf.expand_dims(input_layer, 1)\n",
    "\n",
    "\t\tinput_layer = tf.layers.Dense(16, #3, padding='same',\n",
    "\t\t\tactivation=tf.nn.tanh, \n",
    "\t\t\t#kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.003)\n",
    "\t\t)(input_layer)\n",
    "\n",
    "\t\tinput_layer = tf.layers.Dropout()(input_layer)\n",
    "\t\t#input_layer = tf.layers.Flatten()(input_layer)\n",
    "\n",
    "\t\tlogits = tf.layers.Dense(n_classes, \n",
    "\t\t\t#kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.003)\n",
    "\t\t)(input_layer)\n",
    "\n",
    "\t\t# class_ids will be the model prediction for the class (Iris flower type)\n",
    "\t\t# The output node with the highest value is our prediction\n",
    "\t\tdef sample(logits, random=True):\n",
    "\t\t\tif random:\n",
    "\t\t\t\tu = tf.random_uniform(tf.shape(logits), dtype=logits.dtype)\n",
    "\t\t\t\tlogits -= tf.log(-tf.log(u))\n",
    "\t\t\treturn tf.argmax(logits, axis=1)\n",
    "\n",
    "\t\tpredictions = { 'class_ids': sample(logits, random=False) }\n",
    "\n",
    "\t\t# 1. Prediction mode\n",
    "\t\t# Return our prediction\n",
    "\t\tif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "\t\t\treturn tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "\t\t# Evaluation and Training mode\n",
    "\n",
    "\t\t# Calculate the loss\n",
    "\t\tloss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\t\tloss += tf.losses.get_regularization_loss()\n",
    "\n",
    "\t\t# Calculate the accuracy between the true labels, and our predictions\n",
    "\t\ty_true=labels\n",
    "\t\ty_pred=predictions['class_ids']\n",
    "\t\taverage_type_list = ['micro','macro','weighted']\n",
    "\t\tmetrics = {}\n",
    "\t\tfor average in average_type_list:\n",
    "\t\t\tmetrics[f'precision_{average}'] = tf_metrics.precision(y_true, y_pred, n_classes, average=average)\n",
    "\t\t\tmetrics[f'recall_{average}'] = tf_metrics.recall(y_true, y_pred, n_classes, average=average)\n",
    "\t\t\tmetrics[f'f1_{average}'] = tf_metrics.f1(y_true, y_pred, n_classes, average=average)\n",
    "\n",
    "\t\t# 2. Evaluation mode\n",
    "\t\t# Return our loss (which is used to evaluate our model)\n",
    "\t\t# Set the TensorBoard scalar my_accurace to the accuracy\n",
    "\t\t# Obs: This function only sets value during mode == ModeKeys.EVAL\n",
    "\t\t# To set values during training, see tf.summary.scalar\n",
    "\t\tif mode == tf.estimator.ModeKeys.EVAL:\n",
    "\t\t\treturn tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "\t\t# If mode is not PREDICT nor EVAL, then we must be in TRAIN\n",
    "\t\tassert mode == tf.estimator.ModeKeys.TRAIN, \"TRAIN is only ModeKey left\"\n",
    "\n",
    "\t\t# 3. Training mode\n",
    "\n",
    "\t\t# Default optimizer for DNNClassifier: Adagrad with learning rate=0.05\n",
    "\t\t# Our objective (train_op) is to minimize loss\n",
    "\t\t# Provide global step counter (used to count gradient updates)\n",
    "\t\t#optimizer = tf.train.AdagradOptimizer(0.05)\n",
    "\t\t#optimizer = tf.train.AdamOptimizer()\n",
    "\t\toptimizer = tf.train.ProximalAdagradOptimizer(learning_rate=0.01, l2_regularization_strength=0.003)\n",
    "\t\ttrain_op = optimizer.minimize(loss, global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "\t\t# For Tensorboard\n",
    "\t\tfor metric_name, metric in metrics.items():\n",
    "\t\t\ttf.summary.scalar(metric_name, metric[1])\n",
    "\n",
    "\t\t# Return training operations: loss and train_op\n",
    "\t\treturn tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\treturn model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for extracting summaries (statistics) from tensorboard events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_results(summary_dir):\n",
    "\tdef my_summary_iterator(path):\n",
    "\t\tfor r in tf_record.tf_record_iterator(path):\n",
    "\t\t\tyield event_pb2.Event.FromString(r)\n",
    "\n",
    "\tresult_list = []\n",
    "\tdocument_list = get_document_list(summary_dir)\n",
    "\t#print(document_list)\n",
    "\tfor filename in document_list:\n",
    "\t\tprint(filename)\n",
    "\t\tif not os.path.basename(filename).startswith('events.'):\n",
    "\t\t\tcontinue\n",
    "\t\tvalue_dict = {}\n",
    "\t\tfor event in my_summary_iterator(filename):\n",
    "\t\t\tfor value in event.summary.value:\n",
    "\t\t\t\ttag = value.tag\n",
    "\t\t\t\tif tag not in value_dict:\n",
    "\t\t\t\t\tvalue_dict[tag]=[]\n",
    "\t\t\t\tvalue_dict[tag].append((event.step, value.simple_value))\n",
    "\t\tresult_list.append({'event_name':filename, 'results':value_dict})\n",
    "\treturn result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for training and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(trainset, testset, num_epochs, batch_size, max_steps, model_dir, feature_columns, n_classes):\n",
    "\t# Create a custom estimator using model_fn to define the model\n",
    "\ttf.logging.info(\"Before classifier construction\")\n",
    "\trun_config = tf.estimator.RunConfig(\n",
    "\t\tmodel_dir=model_dir,\n",
    "\t\t#save_checkpoints_secs=EVALUATION_SECONDS, \n",
    "\t\tsave_checkpoints_steps=EVALUATION_STEPS,\n",
    "\t\t#keep_checkpoint_max=3,\n",
    "\t)\n",
    "\testimator = tf.estimator.Estimator(\n",
    "\t\tmodel_fn=build_model_fn(feature_columns, n_classes),\n",
    "\t\tconfig=run_config,\n",
    "\t)\n",
    "\ttf.logging.info(\"...done constructing classifier\")\n",
    "\n",
    "\t# Build train input callback\n",
    "\ttrain_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "\t\tx=trainset['x'],\n",
    "\t\ty=trainset['y'],\n",
    "\t\tnum_epochs=num_epochs,\n",
    "\t\tbatch_size=batch_size,\n",
    "\t\tshuffle=True\n",
    "\t)\n",
    "\t# Build test input callback\n",
    "\ttest_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "\t\tx=testset['x'],\n",
    "\t\ty=testset['y'],\n",
    "\t\tnum_epochs=1,\n",
    "\t\tbatch_size=batch_size,\n",
    "\t\tshuffle=False\n",
    "\t)\n",
    "\n",
    "\ttrain_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=max_steps)\n",
    "\teval_spec = tf.estimator.EvalSpec(input_fn=test_input_fn, throttle_secs=EVALUATION_SECONDS)\n",
    "\n",
    "\ttf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe\n",
      "                                             anchorsent       sectype  \\\n",
      "0     'In summary, the open nature of the Internet a...  introduction   \n",
      "1     'Finally, a common data transformation method ...          none   \n",
      "3     'Hucaljuk and Rakipovic [15] included an exper...       results   \n",
      "4     'The first step in the modelling process is to...       results   \n",
      "5     'Hucaljuk and Rakipovic [15] used a separate e...       results   \n",
      "...                                                 ...           ...   \n",
      "1552  'To support feature computation and combinatio...          none   \n",
      "1553  'To query the lexical databases we use the ope...          data   \n",
      "1554  'To compute the score, we use a series of feat...          data   \n",
      "1555  'Using the strategy described in Section 3.3, ...          none   \n",
      "1556  'It is computed by comparing all patents in th...          none   \n",
      "\n",
      "     main_predicate  \n",
      "0            create  \n",
      "1         is called  \n",
      "3          included  \n",
      "4      will be used  \n",
      "5              used  \n",
      "...             ...  \n",
      "1552     To support  \n",
      "1553       To query  \n",
      "1554     To compute  \n",
      "1555          Using  \n",
      "1556    is computed  \n",
      "\n",
      "[1376 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "trainset = get_dataframe('training_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe\n",
      "                                            anchorsent       sectype  \\\n",
      "0    'This is common in software design when the UI...  introduction   \n",
      "1    'The most related items intersect first, and t...          none   \n",
      "2    'In this study, participants performed a hiera...          none   \n",
      "3    'Card sorting software, xSort (Arroz, 2008) wa...          none   \n",
      "4    'The concept of using PA to analyze SD evaluat...  introduction   \n",
      "..                                                 ...           ...   \n",
      "295  'Data words are commonly studied in XML litera...          none   \n",
      "296  'It was shown in [7] that the language L={(ad1...          none   \n",
      "297  'Originally they were defined on words over in...          none   \n",
      "298  'What all of these languages (with the sole ex...  introduction   \n",
      "299  'Note that besides the operators in Definition...          none   \n",
      "\n",
      "    main_predicate  \n",
      "0      is assigned  \n",
      "1                   \n",
      "2        performed  \n",
      "3         was used  \n",
      "4     was proposed  \n",
      "..             ...  \n",
      "295    are studied  \n",
      "296      was shown  \n",
      "297   were defined  \n",
      "298           rely  \n",
      "299       includes  \n",
      "\n",
      "[289 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "testset = get_dataframe('test_groundtruth_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label classes: ['cites', 'extends', 'uses_data_from', 'uses_method_in']\n",
      "Sectype classes: ['acknowledgements', 'background', 'conclusion', 'data', 'discussion', 'introduction', 'materials', 'methods', 'model', 'motivation', 'none', 'related work', 'results', 'scenario']\n"
     ]
    }
   ],
   "source": [
    "n_classes = encode_dataset({'train':trainset, 'test':testset})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [get_dataframe_feature_shape(trainset['x'],feature) for feature in trainset['x'].keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge dataset 1 and 2, because they have different distributions and thus we have to build new train and test sets. Before mergin we convert the datasets into datalists, this way we can easily shuffle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlist = listify_dataset(trainset)\n",
    "if USE_TEST_SET:\n",
    "\ttestlist = listify_dataset(testset)\n",
    "\tdatalist = trainlist + testlist\n",
    "else:\n",
    "\tdatalist = trainlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for plotting summary results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summary_results(summary_results):\n",
    "\tplt.clf()\n",
    "\tplt_height = len(summary_results)\n",
    "\t_, axes = plt.subplots(nrows=plt_height, sharex=True, figsize=(14,15*plt_height))\n",
    "\tfor e, (stat, value_list) in enumerate(summary_results.items()):\n",
    "\t\tax = axes[e]\n",
    "\t\t#ax.set_ylim([0, 1])\n",
    "\t\t#ax.set_yticks(value_list)\n",
    "\t\tstep_list,value_list=zip(*value_list)\n",
    "\t\tax.plot(step_list, value_list)\n",
    "\t\tax.set(xlabel='step', ylabel=stat)\n",
    "\t\tax.grid()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for cross-validating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cross_validate_model(datalist):\n",
    "\tdef get_best_stat_dict(summary_results_list):\n",
    "\t\tbest_stat_dict = {}\n",
    "\t\tfor summary_results in summary_results_list:\n",
    "\t\t\tfor stat, value_list in summary_results.items():\n",
    "\t\t\t\t_,value_list=zip(*value_list)\n",
    "\t\t\t\tif not re.search(r'(f1|precision|recall)', stat):\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tif stat not in best_stat_dict:\n",
    "\t\t\t\t\tbest_stat_dict[stat] = []\n",
    "\t\t\t\tbest_stat_dict[stat].append(max(value_list))\n",
    "\t\treturn best_stat_dict\n",
    "\n",
    "\tdef cross_validate_model(config, reporter):\n",
    "\t\tsummary_results_list = []\n",
    "\t\tcross_validation = KFold(n_splits=config[\"N_SPLITS\"], shuffle=True, random_state=1)\n",
    "\t\tfor e, (train_index, test_index) in enumerate(cross_validation.split(datalist)):\n",
    "\t\t\t#print(f'-------- Fold {e} --------')\n",
    "\t\t\t#print(f'Train-set {e} indexes {train_index}')\n",
    "\t\t\t#print(f'Test-set {e} indexes {test_index}')\n",
    "\t\t\t# Split training and test set\n",
    "\t\t\ttrainlist = [datalist[u] for u in train_index]\n",
    "\t\t\ttrainset = dictify_datalist(trainlist)\n",
    "\t\t\t# Oversample training set (after sentences embedding)\n",
    "\t\t\tif config[\"OVERSAMPLE\"] or config[\"UNDERSAMPLE\"]:\n",
    "\t\t\t\tresample_dataset(trainset, oversample=config[\"OVERSAMPLE\"], undersample=config[\"UNDERSAMPLE\"])\n",
    "\t\t\t#print(f'Train-set {e} distribution', Counter(trainset['y']))\n",
    "\t\t\ttestlist = [datalist[u] for u in test_index]\n",
    "\t\t\ttestset = dictify_datalist(testlist)\n",
    "\t\t\t#print(f'Test-set {e} distribution', Counter(testset['y']))\n",
    "\n",
    "\t\t\tconfig_str = '_'.join(f'{key}={value}' for key,value in config.items())\n",
    "\t\t\tmodel_dir = f'{MODEL_DIR}{e}-{config_str}'\n",
    "\t\t\ttrain_and_evaluate(\n",
    "\t\t\t\ttrainset=trainset, \n",
    "\t\t\t\ttestset=testset, \n",
    "\t\t\t\tnum_epochs=TRAIN_EPOCHS, \n",
    "\t\t\t\tbatch_size=config[\"BATCH_SIZE\"], \n",
    "\t\t\t\tmax_steps=MAX_STEPS, \n",
    "\t\t\t\tmodel_dir=model_dir, \n",
    "\t\t\t\tfeature_columns=feature_columns, \n",
    "\t\t\t\tn_classes=n_classes\n",
    "\t\t\t)\n",
    "\t\t\tsummary_results = get_summary_results(f'./{model_dir}/eval')\n",
    "\t\t\tsummary_results = summary_results[-1]['results']\n",
    "\t\t\tsummary_results_list.append(summary_results)\n",
    "\t\t\t#print(f'Test-set {e} results:', summary_results)\n",
    "\t\t\tbest_stat_dict = get_best_stat_dict(summary_results_list)\n",
    "\t\t\treporter(timesteps_total=e, reward=best_stat_dict[\"f1_macro\"])\n",
    "\treturn cross_validate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform automatic hyper-parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-22 21:13:51,058\tINFO resource_spec.py:205 -- Starting Ray with 1.61 GiB memory available for workers and up to 0.82 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2019-10-22 21:13:51,522\tWARNING hyperband.py:96 -- `reward_attr` is deprecated and will be removed in a future version of Tune. Setting `metric=reward` and `mode=max`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using HyperBand: num_stopped=0 total_brackets=0\n",
      "Round #0:\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/1.61 GiB heap, 0.0/0.54 GiB objects\n",
      "Memory usage on this node: 5.3/8.0 GiB\n",
      "\n",
      "== Status ==\n",
      "Using HyperBand: num_stopped=0 total_brackets=10\n",
      "Round #0:\n",
      "  Bracket(Max Size (n)=5, Milestone (r)=81, completed=0.0%): {PENDING: 4, RUNNING: 1} \n",
      "  Bracket(Max Size (n)=8, Milestone (r)=27, completed=0.0%): {PENDING: 8} \n",
      "  Bracket(Max Size (n)=15, Milestone (r)=9, completed=0.0%): {PENDING: 15} \n",
      "  Bracket(Max Size (n)=34, Milestone (r)=3, completed=0.0%): {PENDING: 34} \n",
      "  Bracket(Max Size (n)=81, Milestone (r)=1, completed=0.0%): {PENDING: 81} \n",
      "Round #1:\n",
      "  Bracket(Max Size (n)=5, Milestone (r)=81, completed=0.0%): {PENDING: 5} \n",
      "  Bracket(Max Size (n)=8, Milestone (r)=27, completed=0.0%): {PENDING: 8} \n",
      "  Bracket(Max Size (n)=15, Milestone (r)=9, completed=0.0%): {PENDING: 15} \n",
      "  Bracket(Max Size (n)=34, Milestone (r)=3, completed=0.0%): {PENDING: 34} \n",
      "  Bracket(Max Size (n)=81, Milestone (r)=1, completed=0.0%): {PENDING: 51} \n",
      "Resources requested: 1/4 CPUs, 0/0 GPUs, 0.0/1.61 GiB heap, 0.0/0.54 GiB objects\n",
      "Memory usage on this node: 5.1/8.0 GiB\n",
      "Result logdir: /Users/toor/ray_results/my_experiment\n",
      "Number of trials: 256 ({'RUNNING': 1, 'PENDING': 255})\n",
      "PENDING trials:\n",
      " - cross_validate_model_1_BATCH_SIZE=2,N_SPLITS=3,OVERSAMPLE=True,UNDERSAMPLE=True:\tPENDING\n",
      " - cross_validate_model_2_BATCH_SIZE=3,N_SPLITS=3,OVERSAMPLE=True,UNDERSAMPLE=True:\tPENDING\n",
      " - cross_validate_model_3_BATCH_SIZE=4,N_SPLITS=3,OVERSAMPLE=True,UNDERSAMPLE=True:\tPENDING\n",
      " - cross_validate_model_4_BATCH_SIZE=5,N_SPLITS=3,OVERSAMPLE=True,UNDERSAMPLE=True:\tPENDING\n",
      " - cross_validate_model_5_BATCH_SIZE=6,N_SPLITS=3,OVERSAMPLE=True,UNDERSAMPLE=True:\tPENDING\n",
      " - cross_validate_model_6_BATCH_SIZE=7,N_SPLITS=3,OVERSAMPLE=True,UNDERSAMPLE=True:\tPENDING\n",
      " - cross_validate_model_7_BATCH_SIZE=8,N_SPLITS=3,OVERSAMPLE=True,UNDERSAMPLE=True:\tPENDING\n",
      " - cross_validate_model_8_BATCH_SIZE=1,N_SPLITS=4,OVERSAMPLE=True,UNDERSAMPLE=True:\tPENDING\n",
      " - cross_validate_model_9_BATCH_SIZE=2,N_SPLITS=4,OVERSAMPLE=True,UNDERSAMPLE=True:\tPENDING\n",
      "  ... 237 not shown\n",
      " - cross_validate_model_247_BATCH_SIZE=8,N_SPLITS=9,OVERSAMPLE=False,UNDERSAMPLE=False:\tPENDING\n",
      " - cross_validate_model_248_BATCH_SIZE=1,N_SPLITS=10,OVERSAMPLE=False,UNDERSAMPLE=False:\tPENDING\n",
      " - cross_validate_model_249_BATCH_SIZE=2,N_SPLITS=10,OVERSAMPLE=False,UNDERSAMPLE=False:\tPENDING\n",
      " - cross_validate_model_250_BATCH_SIZE=3,N_SPLITS=10,OVERSAMPLE=False,UNDERSAMPLE=False:\tPENDING\n",
      " - cross_validate_model_251_BATCH_SIZE=4,N_SPLITS=10,OVERSAMPLE=False,UNDERSAMPLE=False:\tPENDING\n",
      " - cross_validate_model_252_BATCH_SIZE=5,N_SPLITS=10,OVERSAMPLE=False,UNDERSAMPLE=False:\tPENDING\n",
      " - cross_validate_model_253_BATCH_SIZE=6,N_SPLITS=10,OVERSAMPLE=False,UNDERSAMPLE=False:\tPENDING\n",
      " - cross_validate_model_254_BATCH_SIZE=7,N_SPLITS=10,OVERSAMPLE=False,UNDERSAMPLE=False:\tPENDING\n",
      " - cross_validate_model_255_BATCH_SIZE=8,N_SPLITS=10,OVERSAMPLE=False,UNDERSAMPLE=False:\tPENDING\n",
      "RUNNING trials:\n",
      " - cross_validate_model_0_BATCH_SIZE=1,N_SPLITS=3,OVERSAMPLE=True,UNDERSAMPLE=True:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m Dataset size before re-sampling: 1110\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m Dataset size before re-sampling: 1110\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m Dataset size before re-sampling: 1110\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m Dataset size before re-sampling: 1110\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m Dataset size after re-sampling: 2724\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m WARNING: Logging before flag parsing goes to stderr.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m W1022 21:14:21.460005 123145569439744 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m W1022 21:14:21.476681 123145569439744 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m W1022 21:14:21.477961 123145569439744 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m W1022 21:14:21.487870 123145569439744 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:205: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m W1022 21:14:21.488062 123145569439744 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:2115: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m W1022 21:14:21.488230 123145569439744 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:206: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m W1022 21:14:21.513864 123145569439744 deprecation.py:506] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m Dataset size after re-sampling: 2724\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m WARNING: Logging before flag parsing goes to stderr.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m W1022 21:14:21.566956 123145510055936 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m W1022 21:14:21.584146 123145510055936 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m W1022 21:14:21.585640 123145510055936 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m W1022 21:14:21.595927 123145510055936 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:205: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m W1022 21:14:21.596204 123145510055936 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:2115: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m W1022 21:14:21.596385 123145510055936 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:206: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m W1022 21:14:21.621958 123145510055936 deprecation.py:506] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m Dataset size after re-sampling: 2722\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m WARNING: Logging before flag parsing goes to stderr.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m W1022 21:14:22.072638 123145488019456 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m W1022 21:14:22.094228 123145488019456 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m W1022 21:14:22.095767 123145488019456 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m W1022 21:14:22.109760 123145488019456 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:205: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m W1022 21:14:22.109995 123145488019456 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:2115: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m W1022 21:14:22.110173 123145488019456 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:206: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m W1022 21:14:22.142330 123145488019456 deprecation.py:506] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m W1022 21:14:22.238053 123145569439744 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m W1022 21:14:22.372982 123145569439744 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tf_metrics/__init__.py:152: The name tf.diag_part is deprecated. Please use tf.linalg.tensor_diag_part instead.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m W1022 21:14:22.374345 123145510055936 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m W1022 21:14:22.384202 123145569439744 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tf_metrics/__init__.py:140: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m W1022 21:14:22.484138 123145510055936 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tf_metrics/__init__.py:152: The name tf.diag_part is deprecated. Please use tf.linalg.tensor_diag_part instead.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m W1022 21:14:22.493370 123145510055936 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tf_metrics/__init__.py:140: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m Dataset size after re-sampling: 2722\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m WARNING: Logging before flag parsing goes to stderr.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m W1022 21:14:22.621400 123145476014080 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m W1022 21:14:22.640667 123145476014080 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m W1022 21:14:22.642343 123145476014080 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m W1022 21:14:22.655329 123145476014080 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:205: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m W1022 21:14:22.655551 123145476014080 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:2115: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m W1022 21:14:22.655733 123145476014080 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:206: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m W1022 21:14:22.692220 123145476014080 deprecation.py:506] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m W1022 21:14:22.891736 123145488019456 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m W1022 21:14:23.004464 123145488019456 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tf_metrics/__init__.py:152: The name tf.diag_part is deprecated. Please use tf.linalg.tensor_diag_part instead.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m W1022 21:14:23.014734 123145488019456 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tf_metrics/__init__.py:140: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m W1022 21:14:23.619440 123145476014080 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m W1022 21:14:23.761065 123145476014080 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tf_metrics/__init__.py:152: The name tf.diag_part is deprecated. Please use tf.linalg.tensor_diag_part instead.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m W1022 21:14:23.776201 123145476014080 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tf_metrics/__init__.py:140: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m 2019-10-22 21:14:27.713550: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m 2019-10-22 21:14:27.787646: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m W1022 21:14:28.139266 123145569439744 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:875: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m W1022 21:14:28.176844 123145510055936 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:875: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m 2019-10-22 21:14:28.335301: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m W1022 21:14:28.764392 123145488019456 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:875: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m 2019-10-22 21:14:28.829534: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m W1022 21:14:29.259895 123145476014080 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:875: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m W1022 21:14:32.754837 123145510055936 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 201 vs previous value: 201. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m W1022 21:14:32.898374 123145510055936 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 301 vs previous value: 301. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m W1022 21:14:33.752394 123145488019456 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 454 vs previous value: 454. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m W1022 21:14:36.258138 123145476014080 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 1438 vs previous value: 1438. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m W1022 21:14:36.318464 123145510055936 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 2300 vs previous value: 2300. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m W1022 21:14:37.716746 123145510055936 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 3087 vs previous value: 3087. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m W1022 21:14:37.715466 123145488019456 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 2701 vs previous value: 2701. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m W1022 21:14:38.397891 123145510055936 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 3501 vs previous value: 3501. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m W1022 21:14:39.324110 123145476014080 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 3092 vs previous value: 3092. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m W1022 21:14:40.905765 123145476014080 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 3942 vs previous value: 3942. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m W1022 21:14:46.168151 123145569439744 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m Use standard file APIs to check for files with this prefix.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m W1022 21:14:46.335248 123145510055936 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5312)\u001b[0m Use standard file APIs to check for files with this prefix.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m W1022 21:14:46.815412 123145488019456 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m Use standard file APIs to check for files with this prefix.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m W1022 21:14:47.955301 123145476014080 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m Use standard file APIs to check for files with this prefix.\n",
      "\u001b[2m\u001b[36m(pid=5310)\u001b[0m W1022 21:14:49.817854 123145488019456 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 5063 vs previous value: 5063. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m W1022 21:14:54.782291 123145569439744 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 8281 vs previous value: 8281. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "\u001b[2m\u001b[36m(pid=5309)\u001b[0m W1022 21:14:56.184294 123145569439744 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 8801 vs previous value: 8801. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "\u001b[2m\u001b[36m(pid=5311)\u001b[0m W1022 21:14:58.254462 123145476014080 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 8701 vs previous value: 8701. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import ray.tune as tune\n",
    "from ray.tune.schedulers import HyperBandScheduler\n",
    "\n",
    "ray.init(num_cpus=4)\n",
    "tune.run(build_cross_validate_model(datalist),\n",
    "    name=\"my_experiment\",\n",
    "    config={ \n",
    "        \"N_SPLITS\": tune.grid_search([3, 4, 5, 6, 7, 8, 9, 10]), \n",
    "        \"OVERSAMPLE\": tune.grid_search([True, False]),\n",
    "        \"UNDERSAMPLE\": tune.grid_search([True, False]),\n",
    "        \"BATCH_SIZE\": tune.grid_search([1, 2, 3, 4, 5, 6, 7, 8]),\n",
    "    },\n",
    "    scheduler=HyperBandScheduler(reward_attr=\"reward\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
