{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to change auto indent to tab instead of 4 spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "IPython.tab_as_tab_everywhere = function(use_tabs) {\n",
    "    if (use_tabs === undefined) {\n",
    "        use_tabs = true; \n",
    "    }\n",
    "\n",
    "    // apply setting to all current CodeMirror instances\n",
    "    IPython.notebook.get_cells().map(\n",
    "        function(c) {  return c.code_mirror.options.indentWithTabs=use_tabs;  }\n",
    "    );\n",
    "    // make sure new CodeMirror instances created in the future also use this setting\n",
    "    CodeMirror.defaults.indentWithTabs=use_tabs;\n",
    "\n",
    "    };\n",
    "\n",
    "IPython.tab_as_tab_everywhere()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip3 install -r requirements.txt -U\n",
    "!python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import core lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./core')\n",
    "from models.predicate_extractor import PredicateExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "#import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.tune as tune\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler, HyperBandScheduler\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_metrics\n",
    "from tensorflow.core.util import event_pb2\n",
    "from tensorflow.python.lib.io import tf_record\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from imblearn import over_sampling, under_sampling, combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SCICITE = True\n",
    "USE_ACLARC = True\n",
    "USE_PATTERN_EMBEDDING = True\n",
    "USE_TEST_SET = False\n",
    "\n",
    "ZERO_CLASS = 'none'\n",
    "CITATION_INTENTS = set([\n",
    "\t'cites',\n",
    "\t'extends',\n",
    "\t'uses_data_from',\n",
    "\t'uses_method_in',\n",
    "\t#'cites_as_review',\n",
    "])\n",
    "\n",
    "TRAIN_EPOCHS = None\n",
    "MAX_STEPS = 10**4\n",
    "EVALUATION_PER_TRAINING = 30\n",
    "EVALUATION_STEPS = MAX_STEPS/EVALUATION_PER_TRAINING\n",
    "MODEL_DIR = './model'\n",
    "DATA_DIR = './dataset'\n",
    "TF_MODEL = 'USE_MLQA'\n",
    "MODEL_OPTIONS = {'tf_model':TF_MODEL, 'use_lemma':False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for extracting probabilities from scicite models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_with_scicite_model(df, dataset_file, model_name):\n",
    "\tfilename = dataset_file+'.'+model_name+'.json'\n",
    "\tif not os.path.isfile(filename):\n",
    "\t\treturn df\n",
    "\tprint(f'Reading {filename}..')\n",
    "\textra_df = pd.read_json(filename)\n",
    "\textra_df = extra_df[['probabilities','string']]\n",
    "\tfeature_name = model_name+'_prediction'\n",
    "\textra_df = extra_df.rename(columns={\n",
    "\t\t'probabilities': feature_name, \n",
    "\t\t'string': 'anchorsent',\n",
    "\t})\n",
    "\tdf = pd.merge(extra_df, df, on='anchorsent', how='inner', sort=True).drop_duplicates(subset=['anchorsent'])\n",
    "\tclass_size = max(map(lambda x: len(x), filter(lambda x: type(x) in [list,tuple,np.array,np.ndarray], df[feature_name].to_list())))\n",
    "\tprint(f'{model_name} has class size {class_size}')\n",
    "\tdf[feature_name] = df[feature_name].map(lambda x: np.zeros(class_size)+1/class_size if not(type(x) in [list,tuple,np.array] and len(x)== class_size) else x)\n",
    "\tfor e in df[feature_name].to_list():\n",
    "\t\tassert(len(e)==class_size)\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for converting input datasets from csv to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(dataset_file):\n",
    "\t# Load dataset\n",
    "\tdf = pd.read_csv(dataset_file+'.csv', sep='\t')\n",
    "\t#print(df.dtypes)\n",
    "\n",
    "\t# Get target values list\n",
    "\tdf['citfunc'].replace(np.NaN, 'none', inplace=True)\n",
    "\tdf['citfunc'] = df['citfunc'].map(lambda x: x.strip())\n",
    "\t# Remove rows with excluded labels\n",
    "\tmask = [x not in CITATION_INTENTS for x in df.citfunc]\n",
    "\tdf.loc[mask, 'citfunc'] = ZERO_CLASS\n",
    "\t# Remove bad rows\n",
    "\tdf['citfunc'].replace('ERROR', 'none', inplace=True)\n",
    "\tdf = df[df.citfunc != 'none']\n",
    "\n",
    "\t# Extract features from dataframe\n",
    "\tdf = df[['anchorsent','sectype','citfunc']]\n",
    "\n",
    "\t# Remove null values\n",
    "\tdf['anchorsent'].replace(np.NaN, '', inplace=True)\n",
    "\tdf = df[df.anchorsent != '']\n",
    "\tdf['sectype'].replace(np.NaN, 'none', inplace=True)\n",
    "\n",
    "\tdf['anchorsent'] = df['anchorsent'].map(lambda x: re.sub(r'\\[\\[.*\\]\\]','',x))\n",
    "\tdf['anchorsent'] = df['anchorsent'].map(lambda x: re.sub(r'[^\\x00-\\x7F]+',' ',x))\n",
    "\tdf['anchorsent'] = df['anchorsent'].map(lambda x: re.sub(r\"^'(.*)'$\",r'\\1',x))\n",
    "    \n",
    "\t# Join with scicite output\n",
    "\tif USE_SCICITE:\n",
    "\t\tdf = fuse_with_scicite_model(df, dataset_file, 'scicite')\n",
    "        \n",
    "\t# Join with ac_larc output\n",
    "\tif USE_ACLARC:\n",
    "\t\tdf = fuse_with_scicite_model(df, dataset_file, 'aclarc')\n",
    "\n",
    "\t# Print dataframe\n",
    "\tprint('Dataframe')\n",
    "\tprint(df)\n",
    "\t\n",
    "\t# Return dataset\n",
    "\tdf.drop_duplicates(subset=['anchorsent'], inplace=True)\n",
    "\tfor citfunc in CITATION_INTENTS:\n",
    "\t\tprint('Amount of {}: {}'.format(citfunc, len(df.loc[df['citfunc'] == citfunc])))\n",
    "\ty_list = df.pop('citfunc').values.tolist() # Extract target list\n",
    "\tfeature_list = df.columns.values.tolist()\n",
    "\tx_dict = {feature: df[feature].to_list() for feature in feature_list}\n",
    "\treturn {'x':x_dict, 'y':y_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for casting dataset to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpyfy_dataset(set):\n",
    "\tset['x'] = {k:np.array(v) for k,v in set['x'].items()}\n",
    "\tset['y'] = np.array(set['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for encoding a dataset, from string to numerical representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(dataset):\n",
    "\t# Embed anchor sentences into vectors\n",
    "\tfor key,value in dataset.items():\n",
    "\t\tdf = value['x']\n",
    "\t\tif USE_PATTERN_EMBEDDING:\n",
    "\t\t\tdf['main_predicate'] = df['anchorsent']\n",
    "\t\t# Embed anchor sentences\n",
    "\t\t#df['anchorsent'] = list(df['anchorsent'])\n",
    "\t\tcache_file = f'cache/{TF_MODEL}.{key}.anchorsent.embedding_cache.pkl'\n",
    "\t\tif os.path.isfile(cache_file):\n",
    "\t\t\twith open(cache_file, 'rb') as f:\n",
    "\t\t\t\tembedded_sentences_dict = pickle.load(f)\n",
    "\t\t\t\tembedded_sentences = [embedded_sentences_dict[s] for s in df['anchorsent']]\n",
    "\t\telse:\n",
    "\t\t\tMODEL_MANAGER = PredicateExtractor(MODEL_OPTIONS)\n",
    "\t\t\tembedded_sentences = MODEL_MANAGER.embed(df['anchorsent'])\n",
    "\t\t\twith open(cache_file, 'wb') as f:\n",
    "\t\t\t\tpickle.dump(dict(zip(df['anchorsent'],embedded_sentences)), f)\n",
    "\t\tdf['anchorsent_embedding'] = embedded_sentences\n",
    "\t\t# Embed extra info\n",
    "\t\tif USE_PATTERN_EMBEDDING:\n",
    "\t\t\tcache_file = f'cache/{TF_MODEL}.{key}.extra.embedding_cache.pkl'\n",
    "\t\t\tif os.path.isfile(cache_file):\n",
    "\t\t\t\twith open(cache_file, 'rb') as f:\n",
    "\t\t\t\t\tembedded_extra_dict = pickle.load(f)\n",
    "\t\t\t\t\tembedded_extra = [embedded_extra_dict[s] for s in df['main_predicate']]\n",
    "\t\t\telse:\n",
    "\t\t\t\tMODEL_MANAGER = PredicateExtractor(MODEL_OPTIONS)\n",
    "\t\t\t\textra_list = []\n",
    "\t\t\t\tfor text in df['main_predicate']:\n",
    "\t\t\t\t\textra = list(Counter(pattern['predicate'] for pattern in MODEL_MANAGER.get_pattern_list(text)).keys())\n",
    "\t\t\t\t\textra_list.append(extra[0] if len(extra)>0 else '')\n",
    "\t\t\t\tembedded_extra = MODEL_MANAGER.embed(extra_list)\n",
    "\t\t\t\twith open(cache_file, 'wb') as f:\n",
    "\t\t\t\t\tpickle.dump(dict(zip(df['main_predicate'],embedded_extra)), f)\n",
    "\t\t\tdf['main_predicate'] = embedded_extra\n",
    "\n",
    "\t# Encode labels\n",
    "\tlabel_encoder_target = LabelEncoder()\n",
    "\tlabel_encoder_target.fit([e for set in dataset.values() for e in set['y']])\n",
    "\tprint('Label classes:', list(label_encoder_target.classes_))\n",
    "\tfor set in dataset.values():\n",
    "\t\tset['y'] = label_encoder_target.transform(set['y'])\n",
    "\n",
    "\t# Encode sectypes\n",
    "\tall_sectypes = [e for set in dataset.values() for e in set['x']['sectype']]\n",
    "\tlabel_encoder_sectype = LabelEncoder()\n",
    "\tall_sectypes = label_encoder_sectype.fit_transform(all_sectypes)\n",
    "\tonehot_encoder_sectype = OneHotEncoder()\n",
    "\tonehot_encoder_sectype.fit(all_sectypes.reshape(-1, 1))\n",
    "\tprint('SCAR classes:', list(label_encoder_sectype.classes_))\n",
    "\tfor set in dataset.values():\n",
    "\t\tlabeled_sectypes = label_encoder_sectype.transform(set['x']['sectype'])\n",
    "\t\tset['x']['sectype'] = onehot_encoder_sectype.transform(labeled_sectypes.reshape(-1, 1)).toarray()[:,1:]\n",
    "\n",
    "\t# Input features to numpy array\n",
    "\tfor set in dataset.values():\n",
    "\t\tnumpyfy_dataset(set)\n",
    "\t# Return number of target classes\n",
    "\treturn len(label_encoder_target.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for resampling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_dataset(set, resampling_fn=None):\n",
    "\tif resampling_fn is None:\n",
    "\t\treturn\n",
    "\t#numpyfy_dataset(set)\n",
    "\tprint('Dataset size before re-sampling:', len(set['y']))\n",
    "\n",
    "\t# Build combined features\n",
    "\tcombined_features_sizes = {}\n",
    "\tcombined_features_list = []\n",
    "\tfor feature in zip(*set['x'].values()):\n",
    "\t\tcombined_features = []\n",
    "\t\tfor e,data in enumerate(feature):\n",
    "\t\t\tif type(data) in [np.ndarray,list,tuple]:\n",
    "\t\t\t\tdata_list = list(data)\n",
    "\t\t\t\tcombined_features.extend(data_list)\n",
    "\t\t\t\tcombined_features_sizes[e] = (len(data_list), type(data[0]))\n",
    "\t\t\telse:\n",
    "\t\t\t\tcombined_features.append(data)\n",
    "\t\t\t\tcombined_features_sizes[e] = (1, type(data))\n",
    "\t\tcombined_features_list.append(combined_features)\n",
    "\t#print(combined_features_list[0])\n",
    "\n",
    "\t# Re-sample data\n",
    "\tcombined_features_list = np.array(combined_features_list, dtype=np.object)\n",
    "\t#combined_features_list, set['y'] = over_sampling.RandomOverSampler(sampling_strategy='all').fit_sample(combined_features_list, set['y'])\n",
    "\tcombined_features_list, set['y'] = resampling_fn().fit_sample(combined_features_list, set['y'])\n",
    "\n",
    "\t# Separate features\n",
    "\tnew_combined_features_list = []\n",
    "\tfor combined_features in combined_features_list:\n",
    "\t\tnew_combined_features = []\n",
    "\t\tstart = 0\n",
    "\t\tfor e,(size,dtype) in combined_features_sizes.items():\n",
    "\t\t\tfeature = combined_features[start:start+size]\n",
    "\t\t\tif size > 1:\n",
    "\t\t\t\t#feature = np.array(feature, dtype=dtype)\n",
    "\t\t\t\tfeature = np.array(feature, dtype=np.float32)\n",
    "\t\t\telse:\n",
    "\t\t\t\tfeature = feature[0]\n",
    "\t\t\tnew_combined_features.append(feature)\n",
    "\t\t\tstart += size\n",
    "\t\tnew_combined_features_list.append(new_combined_features)\n",
    "\t#print(new_combined_features_list[0])\n",
    "\tseparated_features = list(zip(*new_combined_features_list))\n",
    "\n",
    "\tfor feature, value in zip(set['x'].keys(), separated_features):\n",
    "\t\tset['x'][feature] = value\n",
    "\tprint('Dataset size after re-sampling:', len(set['y']))\n",
    "\tnumpyfy_dataset(set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for getting the dataframe feature shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_feature_shape(df, feature):\n",
    "\tfirst_element = df[feature][0]\n",
    "\tif type(first_element) not in [np.array,np.ndarray]:\n",
    "\t\treturn None    \n",
    "\t#print(type(first_element), first_element)\n",
    "\treturn tf.feature_column.numeric_column(feature, shape=first_element.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to convert a data-set into a data-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify_dataset(dataset):\n",
    "\tdataset_xs = zip(*dataset['x'].values())\n",
    "\tdataset_xs = map(lambda x: tuple((k,v) for k,v in zip(dataset['x'].keys(),x)), dataset_xs)\n",
    "\treturn list(zip(dataset_xs, dataset['y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to convert a data-set into a data-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictify_datalist(datalist):\n",
    "\txs, y = zip(*datalist)\n",
    "\ty_list = np.array(y)\n",
    "\txs = zip(*xs)\n",
    "\txs_dict = {}\n",
    "\tfor x_tuples in xs:\n",
    "\t\tfeature_names, x_tuples = zip(*x_tuples)\n",
    "\t\tfeature = feature_names[0]\n",
    "\t\txs_dict[feature] = np.array(x_tuples)\n",
    "\t\t#print(feature, len(xs_dict[feature]))\n",
    "\t#print('y', len(y_list))\n",
    "\treturn {\n",
    "\t\t'x': xs_dict,\n",
    "\t\t'y': y_list\n",
    "\t}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the DNN classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_fn(feature_columns, n_classes, config):\n",
    "\tdef model_fn(\n",
    "\t\tfeatures, # This is batch_features from input_fn\n",
    "\t\tlabels,   # This is batch_labels from input_fn\n",
    "\t\tmode):\t# And instance of tf.estimator.ModeKeys, see below\n",
    "\n",
    "\t\tif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "\t\t\ttf.logging.info(\"my_model_fn: PREDICT, {}\".format(mode))\n",
    "\t\telif mode == tf.estimator.ModeKeys.EVAL:\n",
    "\t\t\ttf.logging.info(\"my_model_fn: EVAL, {}\".format(mode))\n",
    "\t\telif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\t\t\ttf.logging.info(\"my_model_fn: TRAIN, {}\".format(mode))\n",
    "\n",
    "\t\t# Create the layer of input\n",
    "\t\tinput_layer = tf.feature_column.input_layer(features, feature_columns)\n",
    "\t\t#input_layer = tf.expand_dims(input_layer, 1)\n",
    "\n",
    "\t\tinput_layer = tf.layers.Dense(config['UNITS'], #3, padding='same',\n",
    "\t\t\tactivation=config['ACTIVATION_FUNCTION'], \n",
    "\t\t\t#kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.003)\n",
    "\t\t)(input_layer)\n",
    "\n",
    "\t\tinput_layer = tf.layers.Dropout()(input_layer)\n",
    "\t\t#input_layer = tf.layers.Flatten()(input_layer)\n",
    "\n",
    "\t\tlogits = tf.layers.Dense(n_classes, \n",
    "\t\t\t#kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.003)\n",
    "\t\t)(input_layer)\n",
    "\n",
    "\t\t# class_ids will be the model prediction for the class (Iris flower type)\n",
    "\t\t# The output node with the highest value is our prediction\n",
    "\t\tdef sample(logits, random=True):\n",
    "\t\t\tif random:\n",
    "\t\t\t\tu = tf.random_uniform(tf.shape(logits), dtype=logits.dtype)\n",
    "\t\t\t\tlogits -= tf.log(-tf.log(u))\n",
    "\t\t\treturn tf.argmax(logits, axis=1)\n",
    "\n",
    "\t\tpredictions = { 'class_ids': sample(logits, random=False), 'probabilities': tf.nn.softmax(logits) }\n",
    "\n",
    "\t\t# 1. Prediction mode\n",
    "\t\t# Return our prediction\n",
    "\t\tif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "\t\t\treturn tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "\t\t# Evaluation and Training mode\n",
    "\n",
    "\t\t# Calculate the loss\n",
    "\t\tloss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\t\tloss += tf.losses.get_regularization_loss()\n",
    "\n",
    "\t\t# Calculate the accuracy between the true labels, and our predictions\n",
    "\t\ty_true=labels\n",
    "\t\ty_pred=predictions['class_ids']\n",
    "\t\taverage_type_list = ['micro','macro','weighted']\n",
    "\t\tmetrics = {}\n",
    "\t\tfor average in average_type_list:\n",
    "\t\t\tmetrics[f'precision_{average}'] = tf_metrics.precision(y_true, y_pred, n_classes, average=average)\n",
    "\t\t\tmetrics[f'recall_{average}'] = tf_metrics.recall(y_true, y_pred, n_classes, average=average)\n",
    "\t\t\tmetrics[f'f1_{average}'] = tf_metrics.f1(y_true, y_pred, n_classes, average=average)\n",
    "\n",
    "\t\t# 2. Evaluation mode\n",
    "\t\t# Return our loss (which is used to evaluate our model)\n",
    "\t\t# Set the TensorBoard scalar my_accurace to the accuracy\n",
    "\t\t# Obs: This function only sets value during mode == ModeKeys.EVAL\n",
    "\t\t# To set values during training, see tf.summary.scalar\n",
    "\t\tif mode == tf.estimator.ModeKeys.EVAL:\n",
    "\t\t\treturn tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "\t\t# If mode is not PREDICT nor EVAL, then we must be in TRAIN\n",
    "\t\tassert mode == tf.estimator.ModeKeys.TRAIN, \"TRAIN is only ModeKey left\"\n",
    "\n",
    "\t\t# 3. Training mode\n",
    "\n",
    "\t\t# Default optimizer for DNNClassifier: Adagrad with learning rate=0.05\n",
    "\t\t# Our objective (train_op) is to minimize loss\n",
    "\t\t# Provide global step counter (used to count gradient updates)\n",
    "\t\t#optimizer = tf.train.AdagradOptimizer(0.05)\n",
    "\t\t#optimizer = tf.train.AdamOptimizer()\n",
    "\t\toptimizer = tf.train.ProximalAdagradOptimizer(learning_rate=config['LEARNING_RATE'], l2_regularization_strength=config['REGULARIZATION_STRENGTH'])\n",
    "\t\ttrain_op = optimizer.minimize(loss, global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "\t\t# For Tensorboard\n",
    "\t\tfor metric_name, metric in metrics.items():\n",
    "\t\t\ttf.summary.scalar(metric_name, metric[1])\n",
    "\n",
    "\t\t# Return training operations: loss and train_op\n",
    "\t\treturn tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\treturn model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for training and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(config, trainset, testset, num_epochs, batch_size, max_steps, model_dir, feature_columns, n_classes):\n",
    "    # Create a custom estimator using model_fn to define the model\n",
    "    tf.logging.info(\"Before classifier construction\")\n",
    "    run_config = tf.estimator.RunConfig(\n",
    "        model_dir=model_dir,\n",
    "        #save_checkpoints_secs=EVALUATION_SECONDS, \n",
    "        save_checkpoints_steps=EVALUATION_STEPS,\n",
    "        keep_checkpoint_max=1,\n",
    "    )\n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn=build_model_fn(feature_columns, n_classes, config),\n",
    "        config=run_config,\n",
    "    )\n",
    "    tf.logging.info(\"...done constructing classifier\")\n",
    "\n",
    "    # Build train input callback\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x=trainset['x'],\n",
    "        y=trainset['y'],\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    # Build train specifics\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=train_input_fn, \n",
    "        max_steps=max_steps\n",
    "    )\n",
    "    # Build test input callback\n",
    "    test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x=testset['x'],\n",
    "        y=testset['y'],\n",
    "        num_epochs=1,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    # Build best_exporter\n",
    "    feature_spec = tf.feature_column.make_parse_example_spec(feature_columns)\n",
    "    exporter = tf.estimator.BestExporter(\n",
    "        name=\"best_exporter\",\n",
    "        serving_input_receiver_fn=tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec),\n",
    "        exports_to_keep=1 # this will keep the N best checkpoints\n",
    "    )\n",
    "    # Build eval specifics\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=test_input_fn, \n",
    "        steps=EVALUATION_STEPS, \n",
    "        start_delay_secs=0, \n",
    "        throttle_secs=0,\n",
    "        exporters=[exporter],\n",
    "    )\n",
    "\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for extracting summaries (statistics) from tensorboard events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_results(summary_dir):\n",
    "\tdef get_document_list(directory):\n",
    "\t\tdoc_list = []\n",
    "\t\tfor obj in os.listdir(directory):\n",
    "\t\t\tobj_path = os.path.join(directory, obj)\n",
    "\t\t\tif os.path.isfile(obj_path):\n",
    "\t\t\t\tdoc_list.append(obj_path)\n",
    "\t\t\telif os.path.isdir(obj_path):\n",
    "\t\t\t\tdoc_list.extend(get_document_list(obj_path))\n",
    "\t\treturn doc_list\n",
    "    \n",
    "\tdef my_summary_iterator(path):\n",
    "\t\tfor r in tf_record.tf_record_iterator(path):\n",
    "\t\t\tyield event_pb2.Event.FromString(r)\n",
    "\n",
    "\tresult_list = []\n",
    "\tdocument_list = get_document_list(summary_dir)\n",
    "\t#print(document_list)\n",
    "\tfor filename in document_list:\n",
    "\t\t#print(filename)\n",
    "\t\tif not os.path.basename(filename).startswith('events.'):\n",
    "\t\t\tcontinue\n",
    "\t\tvalue_dict = {}\n",
    "\t\tfor event in my_summary_iterator(filename):\n",
    "\t\t\tfor value in event.summary.value:\n",
    "\t\t\t\ttag = value.tag\n",
    "\t\t\t\tif tag not in value_dict:\n",
    "\t\t\t\t\tvalue_dict[tag]=[]\n",
    "\t\t\t\tvalue_dict[tag].append((event.step, value.simple_value))\n",
    "\t\tresult_list.append({'event_name':filename, 'results':value_dict})\n",
    "\treturn result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for cross-validating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(config, feature_columns, n_classes):\n",
    "\tdef get_best_estimator(model_dir):\n",
    "\t\texporter_dir = os.path.join(model_dir,'export','best_exporter')\n",
    "\t\tbest_model_name = os.listdir(exporter_dir)[0]\n",
    "\t\tbest_model_path = os.path.join(exporter_dir, best_model_name)\n",
    "\t\tif not os.path.exists(best_model_path):\n",
    "\t\t\treturn None\n",
    "\t\treturn tf.estimator.Estimator(\n",
    "\t\t\tmodel_fn=build_model_fn(feature_columns, n_classes, config),\n",
    "\t\t\twarm_start_from=best_model_path,\n",
    "\t\t)\n",
    "\n",
    "\tdef save_prediction_results(path, datalist, estimator):\n",
    "\t\tif estimator is None:\n",
    "\t\t\treturn\n",
    "\t\tdataset = dictify_datalist(datalist)\n",
    "\t\tinput_fn = tf.estimator.inputs.numpy_input_fn(x=dataset['x'], y=dataset['y'], shuffle=False)\n",
    "\t\tdataset['x']['probabilities'] = list(map(lambda x: x['probabilities'], estimator.predict(input_fn)))\n",
    "\t\txs,ys = zip(*listify_dataset(dataset))\n",
    "\t\txs = [\n",
    "\t\t\t{\n",
    "\t\t\t\tk: v.tolist() if type(v) in [np.array,np.ndarray] else v\n",
    "\t\t\t\tfor k,v in x\n",
    "\t\t\t\tif k in ['anchorsent','probabilities']\n",
    "\t\t\t}\n",
    "\t\t\tfor x in xs\n",
    "\t\t]\n",
    "\t\twith open(path,'w') as f:\n",
    "\t\t\tjson.dump(xs, f, indent=4)\n",
    "\n",
    "\t# Perform k-fold cross-validation\n",
    "\tfeature_set = set(f.key for f in feature_columns)\n",
    "\tcross_validation = KFold(n_splits=config[\"N_SPLITS\"], shuffle=True, random_state=1)\n",
    "\tfor e, (train_index, test_index) in enumerate(cross_validation.split(datalist)):\n",
    "\t\tprint(f'-------- Fold {e} --------')\n",
    "\t\tprint(f'Train-set {e} indexes {train_index}')\n",
    "\t\tprint(f'Test-set {e} indexes {test_index}')\n",
    "\t\t# Split training and test set\n",
    "\t\ttrainlist = [datalist[u] for u in train_index]\n",
    "\t\ttrainset = dictify_datalist(trainlist)\n",
    "\t\ttrainset['x'] = {\n",
    "\t\t\tk:v\n",
    "\t\t\tfor k,v in trainset['x'].items()\n",
    "\t\t\tif k in feature_set\n",
    "\t\t}\n",
    "\t\t# Re-sample training set (after sentences embedding)\n",
    "\t\tresample_dataset(trainset, resampling_fn=config[\"RESAMPLING_FN\"])\n",
    "\t\tprint(f'Train-set {e} distribution', Counter(trainset['y']))\n",
    "\t\ttestlist = [datalist[u] for u in test_index]\n",
    "\t\ttestset = dictify_datalist(testlist)\n",
    "\t\tprint(f'Test-set {e} distribution', Counter(testset['y']))\n",
    "\n",
    "\t\t#config_str = '_'.join(f'{key}={value if not callable(value) else value.__name__}' for key,value in config.items())\n",
    "\t\tmodel_dir = f'{MODEL_DIR}{e}'#'-{config_str}'\n",
    "\t\testimator = train_and_evaluate(\n",
    "\t\t\tconfig=config,\n",
    "\t\t\ttrainset=trainset, \n",
    "\t\t\ttestset=testset, \n",
    "\t\t\tnum_epochs=TRAIN_EPOCHS, \n",
    "\t\t\tbatch_size=config[\"BATCH_SIZE\"], \n",
    "\t\t\tmax_steps=MAX_STEPS, \n",
    "\t\t\tmodel_dir=model_dir, \n",
    "\t\t\tfeature_columns=feature_columns, \n",
    "\t\t\tn_classes=n_classes\n",
    "\t\t)\n",
    "\n",
    "\t\tbest_estimator = get_best_estimator(model_dir)\n",
    "\t\tsave_prediction_results(os.path.join(model_dir,'trainset_predictions.json'), trainlist, best_estimator)\n",
    "\t\tsave_prediction_results(os.path.join(model_dir,'testset_predictions.json'), testlist, best_estimator)\n",
    "        \n",
    "\t\tyield get_summary_results(os.path.join('.',model_dir,'eval'))[-1]['results'] # iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for distributed cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ray_cross_validation(datalist,feature_columns,n_classes):\n",
    "\tdef get_best_stat_dict(summary_results_list):\n",
    "\t\tbest_stat_dict = {}\n",
    "\t\tfor summary_results in summary_results_list:\n",
    "\t\t\tfor stat, value_list in summary_results.items():\n",
    "\t\t\t\t_,value_list=zip(*value_list)\n",
    "\t\t\t\tif not re.search(r'(f1|precision|recall)', stat):\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tif stat not in best_stat_dict:\n",
    "\t\t\t\t\tbest_stat_dict[stat] = []\n",
    "\t\t\t\tbest_stat_dict[stat].append(np.mean(sorted(value_list, reverse=True)[:3]))\n",
    "\t\tfor stat,best_list in best_stat_dict.items():\n",
    "\t\t\tbest_stat_dict[stat] = {'mean':np.mean(best_list), 'std':np.std(best_list)}\n",
    "\t\treturn best_stat_dict\n",
    "            \n",
    "\tdef ray_cross_validate_model(config, reporter):\n",
    "\t\twarnings.filterwarnings('ignore')\n",
    "\t\ttf.get_logger().setLevel('ERROR')\n",
    "\t\tsummary_results_list = []\n",
    "\t\tfor e,summary_results in enumerate(cross_validate_model(config,feature_columns,n_classes)):\n",
    "\t\t\tsummary_results_list.append(summary_results)\n",
    "\t\t\tprint(f'Test-set {e} results:', summary_results)\n",
    "\t\t\tbest_stat_dict = get_best_stat_dict(summary_results_list)\n",
    "\t\t\treporter(\n",
    "\t\t\t\ttimesteps_total=e, \n",
    "\t\t\t\t# F1 scores\n",
    "\t\t\t\tf1_macro_mean=best_stat_dict[\"f1_macro\"][\"mean\"],\n",
    "\t\t\t\tf1_macro_std=best_stat_dict[\"f1_macro\"][\"std\"],\n",
    "\t\t\t\tf1_micro_mean=best_stat_dict[\"f1_micro\"][\"mean\"],\n",
    "\t\t\t\tf1_micro_std=best_stat_dict[\"f1_micro\"][\"std\"],\n",
    "\t\t\t\tf1_weighted_mean=best_stat_dict[\"f1_weighted\"][\"mean\"],\n",
    "\t\t\t\tf1_weighted_std=best_stat_dict[\"f1_weighted\"][\"std\"],\n",
    "\t\t\t\t# Precision scores\n",
    "\t\t\t\tprecision_macro_mean=best_stat_dict[\"precision_macro\"][\"mean\"],\n",
    "\t\t\t\tprecision_macro_std=best_stat_dict[\"precision_macro\"][\"std\"],\n",
    "\t\t\t\tprecision_micro_mean=best_stat_dict[\"precision_micro\"][\"mean\"],\n",
    "\t\t\t\tprecision_micro_std=best_stat_dict[\"precision_micro\"][\"std\"],\n",
    "\t\t\t\tprecision_weighted_mean=best_stat_dict[\"precision_weighted\"][\"mean\"],\n",
    "\t\t\t\tprecision_weighted_std=best_stat_dict[\"precision_weighted\"][\"std\"],\n",
    "\t\t\t\t# Recall scores\n",
    "\t\t\t\trecall_macro_mean=best_stat_dict[\"recall_macro\"][\"mean\"],\n",
    "\t\t\t\trecall_macro_std=best_stat_dict[\"recall_macro\"][\"std\"],\n",
    "\t\t\t\trecall_micro_mean=best_stat_dict[\"recall_micro\"][\"mean\"],\n",
    "\t\t\t\trecall_micro_std=best_stat_dict[\"recall_micro\"][\"std\"],\n",
    "\t\t\t\trecall_weighted_mean=best_stat_dict[\"recall_weighted\"][\"mean\"],\n",
    "\t\t\t\trecall_weighted_std=best_stat_dict[\"recall_weighted\"][\"std\"],\n",
    "\t\t\t)\n",
    "\t\t\tprint(f'Average best statistics at fold {e}: {best_stat_dict}')\n",
    "\treturn ray_cross_validate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = get_dataframe(os.path.join(DATA_DIR,'training_all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = get_dataframe(os.path.join(DATA_DIR,'test_groundtruth_all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = encode_dataset({'train':trainset, 'test':testset})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    get_dataframe_feature_shape(trainset['x'],feature) \n",
    "    for feature in trainset['x'].keys()\n",
    "    if get_dataframe_feature_shape(trainset['x'],feature) is not None\n",
    "    #and feature in ['aclarc_prediction','scicite_prediction']\n",
    "]\n",
    "print(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge dataset 1 and 2, because they have different distributions and thus we have to build new train and test sets. Before mergin we convert the datasets into datalists, this way we can easily shuffle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlist = listify_dataset(trainset)\n",
    "if USE_TEST_SET:\n",
    "\ttestlist = listify_dataset(testset)\n",
    "\tdatalist = trainlist + testlist\n",
    "else:\n",
    "\tdatalist = trainlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. Do not use code imported with sys.path.append inside ray distributed code: https://stackoverflow.com/questions/54338013/parallel-import-a-python-file-from-sibling-folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform automatic hyper-parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'hp_tuning'\n",
    "local_dir = os.path.join('.','ray_results')\n",
    "analysis = tune.run( # https://ray.readthedocs.io/en/latest/tune-package-ref.html#ray.tune.run\n",
    "    ray_cross_validation(datalist,feature_columns,n_classes),\n",
    "    num_samples=1, # Number of times to sample from the hyperparameter space. Defaults to 1. If grid_search is provided as an argument, the grid will be repeated num_samples of times.\n",
    "    name=experiment_name,\n",
    "    local_dir=local_dir,\n",
    "    resume=os.path.isdir(os.path.join(local_dir,experiment_name)),\n",
    "    #global_checkpoint_period=15*60,\n",
    "    #keep_checkpoints_num=3,\n",
    "    verbose=1, # 0, 1, or 2. Verbosity mode. 0 = silent, 1 = only status updates, 2 = status and trial results.\n",
    "    config={ \n",
    "        \"N_SPLITS\": tune.grid_search([\n",
    "            #3,\n",
    "            #4,\n",
    "            5,\n",
    "        ]), \n",
    "        \"RESAMPLING_FN\": tune.grid_search([\n",
    "            None,\n",
    "            #combine.SMOTEENN, \n",
    "            combine.SMOTETomek, \n",
    "            #over_sampling.RandomOverSampler,\n",
    "            over_sampling.SMOTE,\n",
    "            over_sampling.ADASYN,\n",
    "            #under_sampling.RandomUnderSampler,\n",
    "            #under_sampling.EditedNearestNeighbours,\n",
    "            under_sampling.TomekLinks,\n",
    "        ]),\n",
    "        \"BATCH_SIZE\": tune.grid_search([\n",
    "            2,\n",
    "            #3, \n",
    "            4,\n",
    "        ]),\n",
    "        'UNITS': tune.grid_search([\n",
    "            4, \n",
    "            #6, \n",
    "            8, \n",
    "            #10,\n",
    "            12,\n",
    "        ]),\n",
    "        'ACTIVATION_FUNCTION': tune.grid_search([\n",
    "            #None,\n",
    "            tf.nn.relu,\n",
    "            #tf.nn.leaky_relu,\n",
    "            tf.nn.selu,\n",
    "            tf.nn.tanh,\n",
    "        ]),\n",
    "        #'LEARNING_RATE': tune.sample_from(lambda spec: 0.1*3*random.random()),\n",
    "        'LEARNING_RATE': tune.grid_search([\n",
    "            0.3,\n",
    "            0.1,\n",
    "            0.03,\n",
    "            0.01,\n",
    "        ]),\n",
    "        'REGULARIZATION_STRENGTH': tune.grid_search([\n",
    "            0.01,\n",
    "            0.003,\n",
    "            0.001,\n",
    "            0.0003,\n",
    "            0.0001,\n",
    "        ]),\n",
    "    },\n",
    "    scheduler=AsyncHyperBandScheduler(\n",
    "        metric='f1_macro_mean',\n",
    "        mode='max',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Best config: \", analysis.get_best_config(metric='f1_macro_mean'))\n",
    "analysis_df = analysis.dataframe()\n",
    "#analysis_df['f1_macro_min'] = analysis_df['f1_macro_mean']-analysis_df['f1_macro_std']\n",
    "#analysis_df['f1_macro_max'] = analysis_df['f1_macro_mean']+analysis_df['f1_macro_std']\n",
    "analysis_df['config/RESAMPLING_FN'] = analysis_df['config/RESAMPLING_FN'].map(lambda x: x.split('.')[-1][:-2] if x is not None else x)\n",
    "best_stats = analysis_df.sort_values(['timesteps_total','f1_macro_mean'], ascending=[False,False]).filter(regex='timesteps_total|macro|config|logdir').iloc[:10]\n",
    "best_stats.style"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
