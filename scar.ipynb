{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt -U\n",
    "!python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import core lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./core')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import os\n",
    "import tf_metrics\n",
    "from imblearn import over_sampling, under_sampling, combine\n",
    "from collections import Counter\n",
    "from tensorflow.core.util import event_pb2\n",
    "from tensorflow.python.lib.io import tf_record\n",
    "from misc.doc_reader import get_document_list\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#import random\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from models.role_pattern_extractor import RolePatternExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppress tensorflow warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TEST_SET = True\n",
    "USE_PATTERN_EMBEDDING = True\n",
    "\n",
    "ZERO_CLASS = 'none'\n",
    "LABELS_TO_EXCLUDE = [\n",
    "\t#'cites',\n",
    "\t'cites_as_review',\n",
    "\t#'extends', \n",
    "\t#'uses_data_from', \n",
    "\t#'uses_method_in',\n",
    "]\n",
    "\n",
    "TRAIN_EPOCHS = None\n",
    "MAX_STEPS = (10**4)*2\n",
    "EVALUATION_STEPS = MAX_STEPS/10\n",
    "MODEL_DIR = './model'\n",
    "TF_MODEL = 'USE_MLQA'\n",
    "MODEL_MANAGER = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for converting input datasets from csv to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(dataset_file):\n",
    "\t# Load dataset\n",
    "\tdf = pd.read_csv(dataset_file, sep='\t')\n",
    "\t#print(df.dtypes)\n",
    "\n",
    "\t# Get target values list\n",
    "\tdf['citfunc'].replace(np.NaN, 'none', inplace=True)\n",
    "\tdf['citfunc'] = df['citfunc'].map(lambda x: x.strip())\n",
    "\t# Remove rows with excluded labels\n",
    "\tfor label in LABELS_TO_EXCLUDE:\n",
    "\t\tdf.loc[df.citfunc == label, 'citfunc'] = ZERO_CLASS\n",
    "\t# Remove bad rows\n",
    "\tdf['citfunc'].replace('ERROR', 'none', inplace=True)\n",
    "\tdf = df[df.citfunc != 'none']\n",
    "\t# Extract target list\n",
    "\ttarget_list = df.pop('citfunc').values.tolist()\n",
    "\n",
    "\t# Extract features from dataframe\n",
    "\tdf = df[['anchorsent','sectype']]\n",
    "\t\n",
    "\t# Remove null values\n",
    "\tdf['anchorsent'].replace(np.NaN, '', inplace=True)\n",
    "\tdf['sectype'].replace(np.NaN, 'none', inplace=True)\n",
    "\n",
    "\tdf = df[df.anchorsent != '']\n",
    "\tdf['anchorsent'] = df['anchorsent'].map(lambda x: re.sub(r'\\[\\[.*\\]\\]','',x))\n",
    "\tdf['anchorsent'] = df['anchorsent'].map(lambda x: re.sub(r'[^\\x00-\\x7F]+',' ',x))\n",
    "\n",
    "\tif USE_PATTERN_EMBEDDING:\n",
    "\t\tif MODEL_MANAGER is None:\n",
    "\t\t\tMODEL_MANAGER = RolePatternExtractor({'tf_model':TF_MODEL})\n",
    "\t\textra_list = []\n",
    "\t\tfor text in df['anchorsent'].values:\n",
    "\t\t\textra = list(Counter(pattern['predicate'] for pattern in MODEL_MANAGER.get_role_pattern_list(text)).keys())\n",
    "\t\t\textra_list.append(extra[0] if len(extra)>0 else '')\n",
    "\t\tdf['main_predicate'] = extra_list\n",
    "\t\n",
    "\t# Print dataframe\n",
    "\tprint('Dataframe')\n",
    "\tprint(df)\n",
    "\t\n",
    "\t# Return dataset\n",
    "\tfeature_list = df.columns.values.tolist()\n",
    "\tx_dict = {feature: df[feature].tolist() for feature in feature_list}\n",
    "\ty_list = target_list\n",
    "\treturn {'x':x_dict, 'y':y_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for casting dataset to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpyfy_dataset(set):\n",
    "\tset['x'] = {k: np.array(v) for k,v in set['x'].items()}\n",
    "\tset['y'] = np.array(set['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for encoding a dataset, from string to numerical representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(dataset):\n",
    "\t# Embed anchor sentences into vectors\n",
    "\tfor key,value in dataset.items():\n",
    "\t\tdf = value['x']\n",
    "\t\t# Embed anchor sentences\n",
    "\t\tcache_file = f'{TF_MODEL}.{key}.anchorsent.embedding_cache.pkl'\n",
    "\t\tif os.path.isfile(cache_file):\n",
    "\t\t\twith open(cache_file, 'rb') as f:\n",
    "\t\t\t\tembedded_sentences = pickle.load(f)\n",
    "\t\telse:\n",
    "\t\t\tif MODEL_MANAGER is None:\n",
    "\t\t\t\tMODEL_MANAGER = RolePatternExtractor({'tf_model':TF_MODEL})\n",
    "\t\t\tdf['anchorsent'] = list(df['anchorsent'])\n",
    "\t\t\tembedded_sentences = MODEL_MANAGER.embed(df['anchorsent'])\n",
    "\t\t\twith open(cache_file, 'wb') as f:\n",
    "\t\t\t\tpickle.dump(embedded_sentences, f)\n",
    "\t\tdf['anchorsent'] = embedded_sentences\n",
    "\t\t# Embed extra info\n",
    "\t\tif USE_PATTERN_EMBEDDING:\n",
    "\t\t\tcache_file = f'{TF_MODEL}.{key}.extra.embedding_cache.pkl'\n",
    "\t\t\tif os.path.isfile(cache_file):\n",
    "\t\t\t\twith open(cache_file, 'rb') as f:\n",
    "\t\t\t\t\tembedded_extra = pickle.load(f)\n",
    "\t\t\telse:\n",
    "\t\t\t\tdf['main_predicate'] = list(df['main_predicate'])\n",
    "\t\t\t\tembedded_extra = MODEL_MANAGER.embed(df['main_predicate'])\n",
    "\t\t\t\twith open(cache_file, 'wb') as f:\n",
    "\t\t\t\t\tpickle.dump(embedded_extra, f)\n",
    "\t\t\tdf['main_predicate'] = embedded_extra\n",
    "\n",
    "\t# Encode labels\n",
    "\tlabel_encoder_target = LabelEncoder()\n",
    "\tlabel_encoder_target.fit([e for set in dataset.values() for e in set['y']])\n",
    "\tprint('Label classes:', list(label_encoder_target.classes_))\n",
    "\tfor set in dataset.values():\n",
    "\t\tset['y'] = label_encoder_target.transform(set['y'])\n",
    "\n",
    "\t# Encode sectypes\n",
    "\tall_sectypes = [e for set in dataset.values() for e in set['x']['sectype']]\n",
    "\tlabel_encoder_sectype = LabelEncoder()\n",
    "\tall_sectypes = label_encoder_sectype.fit_transform(all_sectypes)\n",
    "\tonehot_encoder_sectype = OneHotEncoder()\n",
    "\tonehot_encoder_sectype.fit(all_sectypes.reshape(-1, 1))\n",
    "\tprint('Sectype classes:', list(label_encoder_sectype.classes_))\n",
    "\tfor set in dataset.values():\n",
    "\t\tlabeled_sectypes = label_encoder_sectype.transform(set['x']['sectype'])\n",
    "\t\tset['x']['sectype'] = onehot_encoder_sectype.transform(labeled_sectypes.reshape(-1, 1)).toarray()[:,1:]\n",
    "\n",
    "\t# Input features to numpy array\n",
    "\tfor set in dataset.values():\n",
    "\t\tnumpyfy_dataset(set)\n",
    "\t# Return number of target classes\n",
    "\treturn len(label_encoder_target.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for resampling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_dataset(set, oversample=True, undersample=True):\n",
    "\t#numpyfy_dataset(set)\n",
    "\tprint('Dataset size before re-sampling:', len(set['y']))\n",
    "\n",
    "\t# Build combined features\n",
    "\tcombined_features_sizes = {}\n",
    "\tcombined_features_list = []\n",
    "\tfor feature in zip(*set['x'].values()):\n",
    "\t\tcombined_features = []\n",
    "\t\tfor e,data in enumerate(feature):\n",
    "\t\t\tif type(data) in [np.ndarray,list,tuple]:\n",
    "\t\t\t\tdata_list = list(data)\n",
    "\t\t\t\tcombined_features.extend(data_list)\n",
    "\t\t\t\tcombined_features_sizes[e] = (len(data_list), type(data[0]))\n",
    "\t\t\telse:\n",
    "\t\t\t\tcombined_features.append(data)\n",
    "\t\t\t\tcombined_features_sizes[e] = (1, type(data))\n",
    "\t\tcombined_features_list.append(combined_features)\n",
    "\t#print(combined_features_list[0])\n",
    "\n",
    "\t# Oversample data\n",
    "\tcombined_features_list = np.array(combined_features_list, dtype=np.object)\n",
    "\t#combined_features_list, set['y'] = over_sampling.RandomOverSampler(sampling_strategy='all').fit_sample(combined_features_list, set['y'])\n",
    "\tif oversample and undersample:\n",
    "\t\tcombined_features_list, set['y'] = combine.SMOTETomek().fit_sample(combined_features_list, set['y'])\n",
    "\telif oversample:\n",
    "\t\tcombined_features_list, set['y'] = over_sampling.ADASYN().fit_sample(combined_features_list, set['y'])\n",
    "\telif undersample:\n",
    "\t\tcombined_features_list, set['y'] = under_sampling.NeighbourhoodCleaningRule().fit_sample(combined_features_list, set['y'])\n",
    "\n",
    "\t# Separate features\n",
    "\tnew_combined_features_list = []\n",
    "\tfor combined_features in combined_features_list:\n",
    "\t\tnew_combined_features = []\n",
    "\t\tstart = 0\n",
    "\t\tfor e,(size,dtype) in combined_features_sizes.items():\n",
    "\t\t\tfeature = combined_features[start:start+size]\n",
    "\t\t\tif size > 1:\n",
    "\t\t\t\t#feature = np.array(feature, dtype=dtype)\n",
    "\t\t\t\tfeature = np.array(feature, dtype=np.float32)\n",
    "\t\t\telse:\n",
    "\t\t\t\tfeature = feature[0]\n",
    "\t\t\tnew_combined_features.append(feature)\n",
    "\t\t\tstart += size\n",
    "\t\tnew_combined_features_list.append(new_combined_features)\n",
    "\t#print(new_combined_features_list[0])\n",
    "\tseparated_features = list(zip(*new_combined_features_list))\n",
    "\n",
    "\tfor feature, value in zip(set['x'].keys(), separated_features):\n",
    "\t\tset['x'][feature] = value\n",
    "\tprint('Dataset size after re-sampling:', len(set['y']))\n",
    "\tnumpyfy_dataset(set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for getting the dataframe feature shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_feature_shape(df, feature):\n",
    "\tfirst_element = df[feature][0]\n",
    "\tshape = first_element.shape if type(first_element) is np.ndarray else ()\n",
    "\treturn tf.feature_column.numeric_column(feature, shape=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to convert a data-set into a data-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify_dataset(dataset):\n",
    "\tdataset_xs = zip(*dataset['x'].values())\n",
    "\tdataset_xs = map(lambda x: tuple((k,v) for k,v in zip(dataset['x'].keys(),x)), dataset_xs)\n",
    "\treturn list(zip(dataset_xs, dataset['y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to convert a data-set into a data-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictify_datalist(datalist):\n",
    "\txs, y = zip(*datalist)\n",
    "\ty_list = np.array(y)\n",
    "\txs = zip(*xs)\n",
    "\txs_dict = {}\n",
    "\tfor x_tuples in xs:\n",
    "\t\tfeature_names, x_tuples = zip(*x_tuples)\n",
    "\t\tfeature = feature_names[0]\n",
    "\t\txs_dict[feature] = np.array(x_tuples)\n",
    "\t\t#print(feature, len(xs_dict[feature]))\n",
    "\t#print('y', len(y_list))\n",
    "\treturn {\n",
    "\t\t'x': xs_dict,\n",
    "\t\t'y': y_list\n",
    "\t}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the DNN classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_fn(feature_columns, n_classes, config):\n",
    "\tdef model_fn(\n",
    "\t\tfeatures, # This is batch_features from input_fn\n",
    "\t\tlabels,   # This is batch_labels from input_fn\n",
    "\t\tmode):\t# And instance of tf.estimator.ModeKeys, see below\n",
    "\n",
    "\t\tif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "\t\t\ttf.logging.info(\"my_model_fn: PREDICT, {}\".format(mode))\n",
    "\t\telif mode == tf.estimator.ModeKeys.EVAL:\n",
    "\t\t\ttf.logging.info(\"my_model_fn: EVAL, {}\".format(mode))\n",
    "\t\telif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\t\t\ttf.logging.info(\"my_model_fn: TRAIN, {}\".format(mode))\n",
    "\n",
    "\t\t# Create the layer of input\n",
    "\t\tinput_layer = tf.feature_column.input_layer(features, feature_columns)\n",
    "\t\t#input_layer = tf.expand_dims(input_layer, 1)\n",
    "\n",
    "\t\tinput_layer = tf.layers.Dense(config['UNITS'], #3, padding='same',\n",
    "\t\t\tactivation=config['ACTIVATION_FUNCTION'], \n",
    "\t\t\t#kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.003)\n",
    "\t\t)(input_layer)\n",
    "\n",
    "\t\tinput_layer = tf.layers.Dropout()(input_layer)\n",
    "\t\t#input_layer = tf.layers.Flatten()(input_layer)\n",
    "\n",
    "\t\tlogits = tf.layers.Dense(n_classes, \n",
    "\t\t\t#kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.003)\n",
    "\t\t)(input_layer)\n",
    "\n",
    "\t\t# class_ids will be the model prediction for the class (Iris flower type)\n",
    "\t\t# The output node with the highest value is our prediction\n",
    "\t\tdef sample(logits, random=True):\n",
    "\t\t\tif random:\n",
    "\t\t\t\tu = tf.random_uniform(tf.shape(logits), dtype=logits.dtype)\n",
    "\t\t\t\tlogits -= tf.log(-tf.log(u))\n",
    "\t\t\treturn tf.argmax(logits, axis=1)\n",
    "\n",
    "\t\tpredictions = { 'class_ids': sample(logits, random=False) }\n",
    "\n",
    "\t\t# 1. Prediction mode\n",
    "\t\t# Return our prediction\n",
    "\t\tif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "\t\t\treturn tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "\t\t# Evaluation and Training mode\n",
    "\n",
    "\t\t# Calculate the loss\n",
    "\t\tloss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\t\tloss += tf.losses.get_regularization_loss()\n",
    "\n",
    "\t\t# Calculate the accuracy between the true labels, and our predictions\n",
    "\t\ty_true=labels\n",
    "\t\ty_pred=predictions['class_ids']\n",
    "\t\taverage_type_list = ['micro','macro','weighted']\n",
    "\t\tmetrics = {}\n",
    "\t\tfor average in average_type_list:\n",
    "\t\t\tmetrics[f'precision_{average}'] = tf_metrics.precision(y_true, y_pred, n_classes, average=average)\n",
    "\t\t\tmetrics[f'recall_{average}'] = tf_metrics.recall(y_true, y_pred, n_classes, average=average)\n",
    "\t\t\tmetrics[f'f1_{average}'] = tf_metrics.f1(y_true, y_pred, n_classes, average=average)\n",
    "\n",
    "\t\t# 2. Evaluation mode\n",
    "\t\t# Return our loss (which is used to evaluate our model)\n",
    "\t\t# Set the TensorBoard scalar my_accurace to the accuracy\n",
    "\t\t# Obs: This function only sets value during mode == ModeKeys.EVAL\n",
    "\t\t# To set values during training, see tf.summary.scalar\n",
    "\t\tif mode == tf.estimator.ModeKeys.EVAL:\n",
    "\t\t\treturn tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "\t\t# If mode is not PREDICT nor EVAL, then we must be in TRAIN\n",
    "\t\tassert mode == tf.estimator.ModeKeys.TRAIN, \"TRAIN is only ModeKey left\"\n",
    "\n",
    "\t\t# 3. Training mode\n",
    "\n",
    "\t\t# Default optimizer for DNNClassifier: Adagrad with learning rate=0.05\n",
    "\t\t# Our objective (train_op) is to minimize loss\n",
    "\t\t# Provide global step counter (used to count gradient updates)\n",
    "\t\t#optimizer = tf.train.AdagradOptimizer(0.05)\n",
    "\t\t#optimizer = tf.train.AdamOptimizer()\n",
    "\t\toptimizer = tf.train.ProximalAdagradOptimizer(learning_rate=config['LEARNING_RATE'], l2_regularization_strength=config['REGULARIZATION_STRENGTH'])\n",
    "\t\ttrain_op = optimizer.minimize(loss, global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "\t\t# For Tensorboard\n",
    "\t\tfor metric_name, metric in metrics.items():\n",
    "\t\t\ttf.summary.scalar(metric_name, metric[1])\n",
    "\n",
    "\t\t# Return training operations: loss and train_op\n",
    "\t\treturn tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\treturn model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for extracting summaries (statistics) from tensorboard events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_results(summary_dir):\n",
    "\tdef my_summary_iterator(path):\n",
    "\t\tfor r in tf_record.tf_record_iterator(path):\n",
    "\t\t\tyield event_pb2.Event.FromString(r)\n",
    "\n",
    "\tresult_list = []\n",
    "\tdocument_list = get_document_list(summary_dir)\n",
    "\t#print(document_list)\n",
    "\tfor filename in document_list:\n",
    "\t\tprint(filename)\n",
    "\t\tif not os.path.basename(filename).startswith('events.'):\n",
    "\t\t\tcontinue\n",
    "\t\tvalue_dict = {}\n",
    "\t\tfor event in my_summary_iterator(filename):\n",
    "\t\t\tfor value in event.summary.value:\n",
    "\t\t\t\ttag = value.tag\n",
    "\t\t\t\tif tag not in value_dict:\n",
    "\t\t\t\t\tvalue_dict[tag]=[]\n",
    "\t\t\t\tvalue_dict[tag].append((event.step, value.simple_value))\n",
    "\t\tresult_list.append({'event_name':filename, 'results':value_dict})\n",
    "\treturn result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for training and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(config, trainset, testset, num_epochs, batch_size, max_steps, model_dir, feature_columns, n_classes):\n",
    "\t# Create a custom estimator using model_fn to define the model\n",
    "\ttf.logging.info(\"Before classifier construction\")\n",
    "\trun_config = tf.estimator.RunConfig(\n",
    "\t\tmodel_dir=model_dir,\n",
    "\t\t#save_checkpoints_secs=EVALUATION_SECONDS, \n",
    "\t\tsave_checkpoints_steps=EVALUATION_STEPS,\n",
    "\t\t#keep_checkpoint_max=3,\n",
    "\t)\n",
    "\testimator = tf.estimator.Estimator(\n",
    "\t\tmodel_fn=build_model_fn(feature_columns, n_classes, config),\n",
    "\t\tconfig=run_config,\n",
    "\t)\n",
    "\ttf.logging.info(\"...done constructing classifier\")\n",
    "\n",
    "\t# Build train input callback\n",
    "\ttrain_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "\t\tx=trainset['x'],\n",
    "\t\ty=trainset['y'],\n",
    "\t\tnum_epochs=num_epochs,\n",
    "\t\tbatch_size=batch_size,\n",
    "\t\tshuffle=True\n",
    "\t)\n",
    "\t# Build test input callback\n",
    "\ttest_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "\t\tx=testset['x'],\n",
    "\t\ty=testset['y'],\n",
    "\t\tnum_epochs=1,\n",
    "\t\tbatch_size=batch_size,\n",
    "\t\tshuffle=False\n",
    "\t)\n",
    "\n",
    "\ttrain_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=max_steps)\n",
    "\teval_spec = tf.estimator.EvalSpec(input_fn=test_input_fn, steps=EVALUATION_STEPS, throttle_secs=0)\n",
    "\n",
    "\ttf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = get_dataframe('training_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = get_dataframe('test_groundtruth_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = encode_dataset({'train':trainset, 'test':testset})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [get_dataframe_feature_shape(trainset['x'],feature) for feature in trainset['x'].keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge dataset 1 and 2, because they have different distributions and thus we have to build new train and test sets. Before mergin we convert the datasets into datalists, this way we can easily shuffle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlist = listify_dataset(trainset)\n",
    "if USE_TEST_SET:\n",
    "\ttestlist = listify_dataset(testset)\n",
    "\tdatalist = trainlist + testlist\n",
    "else:\n",
    "\tdatalist = trainlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for plotting summary results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summary_results(summary_results):\n",
    "\tplt.clf()\n",
    "\tplt_height = len(summary_results)\n",
    "\t_, axes = plt.subplots(nrows=plt_height, sharex=True, figsize=(14,15*plt_height))\n",
    "\tfor e, (stat, value_list) in enumerate(summary_results.items()):\n",
    "\t\tax = axes[e]\n",
    "\t\t#ax.set_ylim([0, 1])\n",
    "\t\t#ax.set_yticks(value_list)\n",
    "\t\tstep_list,value_list=zip(*value_list)\n",
    "\t\tax.plot(step_list, value_list)\n",
    "\t\tax.set(xlabel='step', ylabel=stat)\n",
    "\t\tax.grid()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for cross-validating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cross_validate_model(datalist):\n",
    "\tdef get_best_stat_dict(summary_results_list):\n",
    "\t\tbest_stat_dict = {}\n",
    "\t\tfor summary_results in summary_results_list:\n",
    "\t\t\tfor stat, value_list in summary_results.items():\n",
    "\t\t\t\t_,value_list=zip(*value_list)\n",
    "\t\t\t\tif not re.search(r'(f1|precision|recall)', stat):\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tif stat not in best_stat_dict:\n",
    "\t\t\t\t\tbest_stat_dict[stat] = []\n",
    "\t\t\t\tbest_stat_dict[stat].append(max(value_list))\n",
    "\t\tfor stat,best_list in best_stat_dict.items():\n",
    "\t\t\tbest_stat_dict[stat] = {'mean':np.mean(best_list), 'std':np.std(best_list)}\n",
    "\t\treturn best_stat_dict\n",
    "\n",
    "\tdef cross_validate_model(config, reporter):\n",
    "\t\tsummary_results_list = []\n",
    "\t\tcross_validation = KFold(n_splits=config[\"N_SPLITS\"], shuffle=True, random_state=1)\n",
    "\t\tfor e, (train_index, test_index) in enumerate(cross_validation.split(datalist)):\n",
    "\t\t\tprint(f'-------- Fold {e} --------')\n",
    "\t\t\tprint(f'Train-set {e} indexes {train_index}')\n",
    "\t\t\tprint(f'Test-set {e} indexes {test_index}')\n",
    "\t\t\t# Split training and test set\n",
    "\t\t\ttrainlist = [datalist[u] for u in train_index]\n",
    "\t\t\ttrainset = dictify_datalist(trainlist)\n",
    "\t\t\t# Oversample training set (after sentences embedding)\n",
    "\t\t\tif config[\"OVERSAMPLE\"] or config[\"UNDERSAMPLE\"]:\n",
    "\t\t\t\tresample_dataset(trainset, oversample=config[\"OVERSAMPLE\"], undersample=config[\"UNDERSAMPLE\"])\n",
    "\t\t\tprint(f'Train-set {e} distribution', Counter(trainset['y']))\n",
    "\t\t\ttestlist = [datalist[u] for u in test_index]\n",
    "\t\t\ttestset = dictify_datalist(testlist)\n",
    "\t\t\tprint(f'Test-set {e} distribution', Counter(testset['y']))\n",
    "\n",
    "\t\t\tconfig_str = '_'.join(f'{key}={value}' for key,value in config.items())\n",
    "\t\t\tmodel_dir = f'{MODEL_DIR}{e}-{config_str}'\n",
    "\t\t\ttrain_and_evaluate(\n",
    "\t\t\t\tconfig=config,\n",
    "\t\t\t\ttrainset=trainset, \n",
    "\t\t\t\ttestset=testset, \n",
    "\t\t\t\tnum_epochs=TRAIN_EPOCHS, \n",
    "\t\t\t\tbatch_size=config[\"BATCH_SIZE\"], \n",
    "\t\t\t\tmax_steps=MAX_STEPS, \n",
    "\t\t\t\tmodel_dir=model_dir, \n",
    "\t\t\t\tfeature_columns=feature_columns, \n",
    "\t\t\t\tn_classes=n_classes\n",
    "\t\t\t)\n",
    "\t\t\tsummary_results = get_summary_results(f'./{model_dir}/eval')\n",
    "\t\t\tsummary_results = summary_results[-1]['results']\n",
    "\t\t\tsummary_results_list.append(summary_results)\n",
    "\t\t\tprint(f'Test-set {e} results:', summary_results)\n",
    "\t\t\tbest_stat_dict = get_best_stat_dict(summary_results_list)\n",
    "\t\t\treporter(timesteps_total=e, reward=best_stat_dict[\"f1_macro\"][\"mean\"])\n",
    "\t\t\tprint(f'Average best statistics at fold {e}: {best_stat_dict}')\n",
    "\treturn cross_validate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform automatic hyper-parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.tune as tune\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "\n",
    "ray.init(num_cpus=4)\n",
    "tune.run(build_cross_validate_model(datalist),\n",
    "    name=\"my_experiment\",\n",
    "    config={ \n",
    "        \"N_SPLITS\": tune.grid_search([3]), \n",
    "        \"OVERSAMPLE\": tune.grid_search([False]),\n",
    "        \"UNDERSAMPLE\": tune.grid_search([False]),\n",
    "        \"BATCH_SIZE\": tune.grid_search([4]),\n",
    "        'UNITS': tune.grid_search([2, 4, 8, 16, 32]),\n",
    "        'ACTIVATION_FUNCTION': tune.grid_search([tf.nn.tanh, tf.nn.relu, tf.nn.selu, tf.nn.leaky_relu, tf.nn.sigmoid]),\n",
    "        'LEARNING_RATE': tune.grid_search([0.1, 0.03, 0.01, 0.003, 0.001]),\n",
    "        'REGULARIZATION_STRENGTH': tune.grid_search([0.01, 0.003, 0.001, 0.0003]),\n",
    "    },\n",
    "    scheduler=AsyncHyperBandScheduler(reward_attr=\"reward\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
