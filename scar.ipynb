{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip3 install -r requirements.txt -U\n",
    "!python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import core lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./core')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/toor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from models.predicate_extractor import PredicateExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.tune as tune\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler, HyperBandScheduler\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tf_metrics\n",
    "from tensorflow.core.util import event_pb2\n",
    "from tensorflow.python.lib.io import tf_record\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from imblearn import over_sampling, under_sampling, combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppress tensorflow warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TEST_SET = False\n",
    "USE_PATTERN_EMBEDDING = True\n",
    "\n",
    "ZERO_CLASS = 'none'\n",
    "LABELS_TO_EXCLUDE = [\n",
    "\t#'cites',\n",
    "\t'cites_as_review',\n",
    "\t#'extends', \n",
    "\t#'uses_data_from', \n",
    "\t#'uses_method_in',\n",
    "]\n",
    "\n",
    "TRAIN_EPOCHS = None\n",
    "MAX_STEPS = 10**4\n",
    "EVALUATION_PER_TRAINING = 30\n",
    "EVALUATION_STEPS = MAX_STEPS/EVALUATION_PER_TRAINING\n",
    "MODEL_DIR = './model'\n",
    "TF_MODEL = 'USE_MLQA'\n",
    "MODEL_OPTIONS = {'tf_model':TF_MODEL, 'use_lemma':False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for converting input datasets from csv to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(dataset_file):\n",
    "\t# Load dataset\n",
    "\tdf = pd.read_csv(dataset_file, sep='\t')\n",
    "\t#print(df.dtypes)\n",
    "\n",
    "\t# Get target values list\n",
    "\tdf['citfunc'].replace(np.NaN, 'none', inplace=True)\n",
    "\tdf['citfunc'] = df['citfunc'].map(lambda x: x.strip())\n",
    "\t# Remove rows with excluded labels\n",
    "\tfor label in LABELS_TO_EXCLUDE:\n",
    "\t\tdf.loc[df.citfunc == label, 'citfunc'] = ZERO_CLASS\n",
    "\t# Remove bad rows\n",
    "\tdf['citfunc'].replace('ERROR', 'none', inplace=True)\n",
    "\tdf = df[df.citfunc != 'none']\n",
    "\t# Extract target list\n",
    "\ttarget_list = df.pop('citfunc').values.tolist()\n",
    "\n",
    "\t# Extract features from dataframe\n",
    "\tdf = df[['anchorsent','sectype']]\n",
    "\t\n",
    "\t# Remove null values\n",
    "\tdf['anchorsent'].replace(np.NaN, '', inplace=True)\n",
    "\tdf['sectype'].replace(np.NaN, 'none', inplace=True)\n",
    "\n",
    "\tdf = df[df.anchorsent != '']\n",
    "\tdf['anchorsent'] = df['anchorsent'].map(lambda x: re.sub(r'\\[\\[.*\\]\\]','',x))\n",
    "\tdf['anchorsent'] = df['anchorsent'].map(lambda x: re.sub(r'[^\\x00-\\x7F]+',' ',x))\n",
    "\tdf['anchorsent'] = df['anchorsent'].map(lambda x: re.sub(r\"^'(.*)'$\",r'\\1',x))\n",
    "\n",
    "\t# Print dataframe\n",
    "\tprint('Dataframe')\n",
    "\tprint(df)\n",
    "\t\n",
    "\t# Return dataset\n",
    "\tfeature_list = df.columns.values.tolist()\n",
    "\tx_dict = {feature: df[feature].tolist() for feature in feature_list}\n",
    "\ty_list = target_list\n",
    "\treturn {'x':x_dict, 'y':y_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for casting dataset to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpyfy_dataset(set):\n",
    "\tset['x'] = {k: np.array(v) for k,v in set['x'].items()}\n",
    "\tset['y'] = np.array(set['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for encoding a dataset, from string to numerical representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(dataset):\n",
    "\t# Embed anchor sentences into vectors\n",
    "\tfor key,value in dataset.items():\n",
    "\t\tdf = value['x']\n",
    "\t\tif USE_PATTERN_EMBEDDING:\n",
    "\t\t\tdf['main_predicate'] = df['anchorsent']\n",
    "\t\t# Embed anchor sentences\n",
    "\t\tcache_file = f'{TF_MODEL}.{key}.anchorsent.embedding_cache.pkl'\n",
    "\t\tif os.path.isfile(cache_file):\n",
    "\t\t\twith open(cache_file, 'rb') as f:\n",
    "\t\t\t\tembedded_sentences = pickle.load(f)\n",
    "\t\telse:\n",
    "\t\t\tMODEL_MANAGER = PredicateExtractor(MODEL_OPTIONS)\n",
    "\t\t\tdf['anchorsent'] = list(df['anchorsent'])\n",
    "\t\t\tembedded_sentences = MODEL_MANAGER.embed(df['anchorsent'])\n",
    "\t\t\twith open(cache_file, 'wb') as f:\n",
    "\t\t\t\tpickle.dump(embedded_sentences, f)\n",
    "\t\tdf['anchorsent'] = embedded_sentences\n",
    "\t\t# Embed extra info\n",
    "\t\tif USE_PATTERN_EMBEDDING:\n",
    "\t\t\tcache_file = f'{TF_MODEL}.{key}.extra.embedding_cache.pkl'\n",
    "\t\t\tif os.path.isfile(cache_file):\n",
    "\t\t\t\twith open(cache_file, 'rb') as f:\n",
    "\t\t\t\t\tembedded_extra = pickle.load(f)\n",
    "\t\t\telse:\n",
    "\t\t\t\tMODEL_MANAGER = PredicateExtractor(MODEL_OPTIONS)\n",
    "\t\t\t\textra_list = []\n",
    "\t\t\t\tfor text in df['main_predicate']:\n",
    "\t\t\t\t\textra = list(Counter(pattern['predicate'] for pattern in MODEL_MANAGER.get_pattern_list(text)).keys())\n",
    "\t\t\t\t\textra_list.append(extra[0] if len(extra)>0 else '')\n",
    "\t\t\t\tembedded_extra = MODEL_MANAGER.embed(extra_list)\n",
    "\t\t\t\twith open(cache_file, 'wb') as f:\n",
    "\t\t\t\t\tpickle.dump(embedded_extra, f)\n",
    "\t\t\tdf['main_predicate'] = embedded_extra\n",
    "\n",
    "\t# Encode labels\n",
    "\tlabel_encoder_target = LabelEncoder()\n",
    "\tlabel_encoder_target.fit([e for set in dataset.values() for e in set['y']])\n",
    "\tprint('Label classes:', list(label_encoder_target.classes_))\n",
    "\tfor set in dataset.values():\n",
    "\t\tset['y'] = label_encoder_target.transform(set['y'])\n",
    "\n",
    "\t# Encode sectypes\n",
    "\tall_sectypes = [e for set in dataset.values() for e in set['x']['sectype']]\n",
    "\tlabel_encoder_sectype = LabelEncoder()\n",
    "\tall_sectypes = label_encoder_sectype.fit_transform(all_sectypes)\n",
    "\tonehot_encoder_sectype = OneHotEncoder()\n",
    "\tonehot_encoder_sectype.fit(all_sectypes.reshape(-1, 1))\n",
    "\tprint('Sectype classes:', list(label_encoder_sectype.classes_))\n",
    "\tfor set in dataset.values():\n",
    "\t\tlabeled_sectypes = label_encoder_sectype.transform(set['x']['sectype'])\n",
    "\t\tset['x']['sectype'] = onehot_encoder_sectype.transform(labeled_sectypes.reshape(-1, 1)).toarray()[:,1:]\n",
    "\n",
    "\t# Input features to numpy array\n",
    "\tfor set in dataset.values():\n",
    "\t\tnumpyfy_dataset(set)\n",
    "\t# Return number of target classes\n",
    "\treturn len(label_encoder_target.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for resampling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_dataset(set, resampling_fn=None):\n",
    "\tif resampling_fn is None:\n",
    "\t\treturn\n",
    "\t#numpyfy_dataset(set)\n",
    "\tprint('Dataset size before re-sampling:', len(set['y']))\n",
    "\n",
    "\t# Build combined features\n",
    "\tcombined_features_sizes = {}\n",
    "\tcombined_features_list = []\n",
    "\tfor feature in zip(*set['x'].values()):\n",
    "\t\tcombined_features = []\n",
    "\t\tfor e,data in enumerate(feature):\n",
    "\t\t\tif type(data) in [np.ndarray,list,tuple]:\n",
    "\t\t\t\tdata_list = list(data)\n",
    "\t\t\t\tcombined_features.extend(data_list)\n",
    "\t\t\t\tcombined_features_sizes[e] = (len(data_list), type(data[0]))\n",
    "\t\t\telse:\n",
    "\t\t\t\tcombined_features.append(data)\n",
    "\t\t\t\tcombined_features_sizes[e] = (1, type(data))\n",
    "\t\tcombined_features_list.append(combined_features)\n",
    "\t#print(combined_features_list[0])\n",
    "\n",
    "\t# Re-sample data\n",
    "\tcombined_features_list = np.array(combined_features_list, dtype=np.object)\n",
    "\t#combined_features_list, set['y'] = over_sampling.RandomOverSampler(sampling_strategy='all').fit_sample(combined_features_list, set['y'])\n",
    "\tcombined_features_list, set['y'] = resampling_fn().fit_sample(combined_features_list, set['y'])\n",
    "\n",
    "\t# Separate features\n",
    "\tnew_combined_features_list = []\n",
    "\tfor combined_features in combined_features_list:\n",
    "\t\tnew_combined_features = []\n",
    "\t\tstart = 0\n",
    "\t\tfor e,(size,dtype) in combined_features_sizes.items():\n",
    "\t\t\tfeature = combined_features[start:start+size]\n",
    "\t\t\tif size > 1:\n",
    "\t\t\t\t#feature = np.array(feature, dtype=dtype)\n",
    "\t\t\t\tfeature = np.array(feature, dtype=np.float32)\n",
    "\t\t\telse:\n",
    "\t\t\t\tfeature = feature[0]\n",
    "\t\t\tnew_combined_features.append(feature)\n",
    "\t\t\tstart += size\n",
    "\t\tnew_combined_features_list.append(new_combined_features)\n",
    "\t#print(new_combined_features_list[0])\n",
    "\tseparated_features = list(zip(*new_combined_features_list))\n",
    "\n",
    "\tfor feature, value in zip(set['x'].keys(), separated_features):\n",
    "\t\tset['x'][feature] = value\n",
    "\tprint('Dataset size after re-sampling:', len(set['y']))\n",
    "\tnumpyfy_dataset(set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for getting the dataframe feature shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_feature_shape(df, feature):\n",
    "\tfirst_element = df[feature][0]\n",
    "\tshape = first_element.shape if type(first_element) is np.ndarray else ()\n",
    "\treturn tf.feature_column.numeric_column(feature, shape=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to convert a data-set into a data-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify_dataset(dataset):\n",
    "\tdataset_xs = zip(*dataset['x'].values())\n",
    "\tdataset_xs = map(lambda x: tuple((k,v) for k,v in zip(dataset['x'].keys(),x)), dataset_xs)\n",
    "\treturn list(zip(dataset_xs, dataset['y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to convert a data-set into a data-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictify_datalist(datalist):\n",
    "\txs, y = zip(*datalist)\n",
    "\ty_list = np.array(y)\n",
    "\txs = zip(*xs)\n",
    "\txs_dict = {}\n",
    "\tfor x_tuples in xs:\n",
    "\t\tfeature_names, x_tuples = zip(*x_tuples)\n",
    "\t\tfeature = feature_names[0]\n",
    "\t\txs_dict[feature] = np.array(x_tuples)\n",
    "\t\t#print(feature, len(xs_dict[feature]))\n",
    "\t#print('y', len(y_list))\n",
    "\treturn {\n",
    "\t\t'x': xs_dict,\n",
    "\t\t'y': y_list\n",
    "\t}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the DNN classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_fn(feature_columns, n_classes, config):\n",
    "\tdef model_fn(\n",
    "\t\tfeatures, # This is batch_features from input_fn\n",
    "\t\tlabels,   # This is batch_labels from input_fn\n",
    "\t\tmode):\t# And instance of tf.estimator.ModeKeys, see below\n",
    "\n",
    "\t\tif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "\t\t\ttf.logging.info(\"my_model_fn: PREDICT, {}\".format(mode))\n",
    "\t\telif mode == tf.estimator.ModeKeys.EVAL:\n",
    "\t\t\ttf.logging.info(\"my_model_fn: EVAL, {}\".format(mode))\n",
    "\t\telif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\t\t\ttf.logging.info(\"my_model_fn: TRAIN, {}\".format(mode))\n",
    "\n",
    "\t\t# Create the layer of input\n",
    "\t\tinput_layer = tf.feature_column.input_layer(features, feature_columns)\n",
    "\t\t#input_layer = tf.expand_dims(input_layer, 1)\n",
    "\n",
    "\t\tinput_layer = tf.layers.Dense(config['UNITS'], #3, padding='same',\n",
    "\t\t\tactivation=config['ACTIVATION_FUNCTION'], \n",
    "\t\t\t#kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.003)\n",
    "\t\t)(input_layer)\n",
    "\n",
    "\t\tinput_layer = tf.layers.Dropout()(input_layer)\n",
    "\t\t#input_layer = tf.layers.Flatten()(input_layer)\n",
    "\n",
    "\t\tlogits = tf.layers.Dense(n_classes, \n",
    "\t\t\t#kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.003)\n",
    "\t\t)(input_layer)\n",
    "\n",
    "\t\t# class_ids will be the model prediction for the class (Iris flower type)\n",
    "\t\t# The output node with the highest value is our prediction\n",
    "\t\tdef sample(logits, random=True):\n",
    "\t\t\tif random:\n",
    "\t\t\t\tu = tf.random_uniform(tf.shape(logits), dtype=logits.dtype)\n",
    "\t\t\t\tlogits -= tf.log(-tf.log(u))\n",
    "\t\t\treturn tf.argmax(logits, axis=1)\n",
    "\n",
    "\t\tpredictions = { 'class_ids': sample(logits, random=False) }\n",
    "\n",
    "\t\t# 1. Prediction mode\n",
    "\t\t# Return our prediction\n",
    "\t\tif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "\t\t\treturn tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "\t\t# Evaluation and Training mode\n",
    "\n",
    "\t\t# Calculate the loss\n",
    "\t\tloss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\t\tloss += tf.losses.get_regularization_loss()\n",
    "\n",
    "\t\t# Calculate the accuracy between the true labels, and our predictions\n",
    "\t\ty_true=labels\n",
    "\t\ty_pred=predictions['class_ids']\n",
    "\t\taverage_type_list = ['micro','macro','weighted']\n",
    "\t\tmetrics = {}\n",
    "\t\tfor average in average_type_list:\n",
    "\t\t\tmetrics[f'precision_{average}'] = tf_metrics.precision(y_true, y_pred, n_classes, average=average)\n",
    "\t\t\tmetrics[f'recall_{average}'] = tf_metrics.recall(y_true, y_pred, n_classes, average=average)\n",
    "\t\t\tmetrics[f'f1_{average}'] = tf_metrics.f1(y_true, y_pred, n_classes, average=average)\n",
    "\n",
    "\t\t# 2. Evaluation mode\n",
    "\t\t# Return our loss (which is used to evaluate our model)\n",
    "\t\t# Set the TensorBoard scalar my_accurace to the accuracy\n",
    "\t\t# Obs: This function only sets value during mode == ModeKeys.EVAL\n",
    "\t\t# To set values during training, see tf.summary.scalar\n",
    "\t\tif mode == tf.estimator.ModeKeys.EVAL:\n",
    "\t\t\treturn tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "\t\t# If mode is not PREDICT nor EVAL, then we must be in TRAIN\n",
    "\t\tassert mode == tf.estimator.ModeKeys.TRAIN, \"TRAIN is only ModeKey left\"\n",
    "\n",
    "\t\t# 3. Training mode\n",
    "\n",
    "\t\t# Default optimizer for DNNClassifier: Adagrad with learning rate=0.05\n",
    "\t\t# Our objective (train_op) is to minimize loss\n",
    "\t\t# Provide global step counter (used to count gradient updates)\n",
    "\t\t#optimizer = tf.train.AdagradOptimizer(0.05)\n",
    "\t\t#optimizer = tf.train.AdamOptimizer()\n",
    "\t\toptimizer = tf.train.ProximalAdagradOptimizer(learning_rate=config['LEARNING_RATE'], l2_regularization_strength=config['REGULARIZATION_STRENGTH'])\n",
    "\t\ttrain_op = optimizer.minimize(loss, global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "\t\t# For Tensorboard\n",
    "\t\tfor metric_name, metric in metrics.items():\n",
    "\t\t\ttf.summary.scalar(metric_name, metric[1])\n",
    "\n",
    "\t\t# Return training operations: loss and train_op\n",
    "\t\treturn tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\treturn model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for extracting summaries (statistics) from tensorboard events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_list(directory):\n",
    "\tdoc_list = []\n",
    "\tfor obj in os.listdir(directory):\n",
    "\t\tobj_path = os.path.join(directory, obj)\n",
    "\t\tif os.path.isfile(obj_path):\n",
    "\t\t\tdoc_list.append(obj_path)\n",
    "\t\telif os.path.isdir(obj_path):\n",
    "\t\t\tdoc_list.extend(get_document_list(obj_path))\n",
    "\treturn doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_results(summary_dir):\n",
    "\tdef my_summary_iterator(path):\n",
    "\t\tfor r in tf_record.tf_record_iterator(path):\n",
    "\t\t\tyield event_pb2.Event.FromString(r)\n",
    "\n",
    "\tresult_list = []\n",
    "\tdocument_list = get_document_list(summary_dir)\n",
    "\t#print(document_list)\n",
    "\tfor filename in document_list:\n",
    "\t\tprint(filename)\n",
    "\t\tif not os.path.basename(filename).startswith('events.'):\n",
    "\t\t\tcontinue\n",
    "\t\tvalue_dict = {}\n",
    "\t\tfor event in my_summary_iterator(filename):\n",
    "\t\t\tfor value in event.summary.value:\n",
    "\t\t\t\ttag = value.tag\n",
    "\t\t\t\tif tag not in value_dict:\n",
    "\t\t\t\t\tvalue_dict[tag]=[]\n",
    "\t\t\t\tvalue_dict[tag].append((event.step, value.simple_value))\n",
    "\t\tresult_list.append({'event_name':filename, 'results':value_dict})\n",
    "\treturn result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for training and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(config, trainset, testset, num_epochs, batch_size, max_steps, model_dir, feature_columns, n_classes):\n",
    "\t# Create a custom estimator using model_fn to define the model\n",
    "\ttf.logging.info(\"Before classifier construction\")\n",
    "\trun_config = tf.estimator.RunConfig(\n",
    "\t\tmodel_dir=model_dir,\n",
    "\t\t#save_checkpoints_secs=EVALUATION_SECONDS, \n",
    "\t\tsave_checkpoints_steps=EVALUATION_STEPS,\n",
    "\t\t#keep_checkpoint_max=3,\n",
    "\t)\n",
    "\testimator = tf.estimator.Estimator(\n",
    "\t\tmodel_fn=build_model_fn(feature_columns, n_classes, config),\n",
    "\t\tconfig=run_config,\n",
    "\t)\n",
    "\ttf.logging.info(\"...done constructing classifier\")\n",
    "\n",
    "\t# Build train input callback\n",
    "\ttrain_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "\t\tx=trainset['x'],\n",
    "\t\ty=trainset['y'],\n",
    "\t\tnum_epochs=num_epochs,\n",
    "\t\tbatch_size=batch_size,\n",
    "\t\tshuffle=True\n",
    "\t)\n",
    "\t# Build test input callback\n",
    "\ttest_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "\t\tx=testset['x'],\n",
    "\t\ty=testset['y'],\n",
    "\t\tnum_epochs=1,\n",
    "\t\tbatch_size=batch_size,\n",
    "\t\tshuffle=False\n",
    "\t)\n",
    "\n",
    "\ttrain_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=max_steps)\n",
    "\teval_spec = tf.estimator.EvalSpec(input_fn=test_input_fn, steps=EVALUATION_STEPS, start_delay_secs=0, throttle_secs=0)\n",
    "\n",
    "\ttf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for plotting summary results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summary_results(summary_results):\n",
    "\tplt.clf()\n",
    "\tplt_height = len(summary_results)\n",
    "\t_, axes = plt.subplots(nrows=plt_height, sharex=True, figsize=(14,15*plt_height))\n",
    "\tfor e, (stat, value_list) in enumerate(summary_results.items()):\n",
    "\t\tax = axes[e]\n",
    "\t\t#ax.set_ylim([0, 1])\n",
    "\t\t#ax.set_yticks(value_list)\n",
    "\t\tstep_list,value_list=zip(*value_list)\n",
    "\t\tax.plot(step_list, value_list)\n",
    "\t\tax.set(xlabel='step', ylabel=stat)\n",
    "\t\tax.grid()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for cross-validating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cross_validate_model(datalist):\n",
    "\tdef get_best_stat_dict(summary_results_list):\n",
    "\t\tbest_stat_dict = {}\n",
    "\t\tfor summary_results in summary_results_list:\n",
    "\t\t\tfor stat, value_list in summary_results.items():\n",
    "\t\t\t\t_,value_list=zip(*value_list)\n",
    "\t\t\t\tif not re.search(r'(f1|precision|recall)', stat):\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tif stat not in best_stat_dict:\n",
    "\t\t\t\t\tbest_stat_dict[stat] = []\n",
    "\t\t\t\tbest_stat_dict[stat].append(np.mean(sorted(value_list, reverse=True)[:3]))\n",
    "\t\tfor stat,best_list in best_stat_dict.items():\n",
    "\t\t\tbest_stat_dict[stat] = {'mean':np.mean(best_list), 'std':np.std(best_list)}\n",
    "\t\treturn best_stat_dict\n",
    "\n",
    "\tdef cross_validate_model(config, reporter):\n",
    "\t\t# Perform k-fold cross-validation\n",
    "\t\tsummary_results_list = []\n",
    "\t\tcross_validation = KFold(n_splits=config[\"N_SPLITS\"], shuffle=True, random_state=1)\n",
    "\t\tfor e, (train_index, test_index) in enumerate(cross_validation.split(datalist)):\n",
    "\t\t\tprint(f'-------- Fold {e} --------')\n",
    "\t\t\tprint(f'Train-set {e} indexes {train_index}')\n",
    "\t\t\tprint(f'Test-set {e} indexes {test_index}')\n",
    "\t\t\t# Split training and test set\n",
    "\t\t\ttrainlist = [datalist[u] for u in train_index]\n",
    "\t\t\ttrainset = dictify_datalist(trainlist)\n",
    "\t\t\t# Re-sample training set (after sentences embedding)\n",
    "\t\t\tresample_dataset(trainset, resampling_fn=config[\"RESAMPLING_FN\"])\n",
    "\t\t\tprint(f'Train-set {e} distribution', Counter(trainset['y']))\n",
    "\t\t\ttestlist = [datalist[u] for u in test_index]\n",
    "\t\t\ttestset = dictify_datalist(testlist)\n",
    "\t\t\tprint(f'Test-set {e} distribution', Counter(testset['y']))\n",
    "\n",
    "\t\t\t#config_str = '_'.join(f'{key}={value if not callable(value) else value.__name__}' for key,value in config.items())\n",
    "\t\t\tmodel_dir = f'{MODEL_DIR}{e}'#'-{config_str}'\n",
    "\t\t\ttrain_and_evaluate(\n",
    "\t\t\t\tconfig=config,\n",
    "\t\t\t\ttrainset=trainset, \n",
    "\t\t\t\ttestset=testset, \n",
    "\t\t\t\tnum_epochs=TRAIN_EPOCHS, \n",
    "\t\t\t\tbatch_size=config[\"BATCH_SIZE\"], \n",
    "\t\t\t\tmax_steps=MAX_STEPS, \n",
    "\t\t\t\tmodel_dir=model_dir, \n",
    "\t\t\t\tfeature_columns=feature_columns, \n",
    "\t\t\t\tn_classes=n_classes\n",
    "\t\t\t)\n",
    "\t\t\tsummary_results = get_summary_results(f'./{model_dir}/eval')\n",
    "\t\t\tsummary_results = summary_results[-1]['results']\n",
    "\t\t\tsummary_results_list.append(summary_results)\n",
    "\t\t\tprint(f'Test-set {e} results:', summary_results)\n",
    "\t\t\tbest_stat_dict = get_best_stat_dict(summary_results_list)\n",
    "\t\t\treporter(\n",
    "\t\t\t\ttimesteps_total=e, \n",
    "\t\t\t\t# F1 scores\n",
    "\t\t\t\tf1_macro_mean=best_stat_dict[\"f1_macro\"][\"mean\"],\n",
    "\t\t\t\tf1_macro_std=best_stat_dict[\"f1_macro\"][\"std\"],\n",
    "\t\t\t\tf1_micro_mean=best_stat_dict[\"f1_micro\"][\"mean\"],\n",
    "\t\t\t\tf1_micro_std=best_stat_dict[\"f1_micro\"][\"std\"],\n",
    "\t\t\t\tf1_weighted_mean=best_stat_dict[\"f1_weighted\"][\"mean\"],\n",
    "\t\t\t\tf1_weighted_std=best_stat_dict[\"f1_weighted\"][\"std\"],\n",
    "\t\t\t\t# Precision scores\n",
    "\t\t\t\tprecision_macro_mean=best_stat_dict[\"precision_macro\"][\"mean\"],\n",
    "\t\t\t\tprecision_macro_std=best_stat_dict[\"precision_macro\"][\"std\"],\n",
    "\t\t\t\tprecision_micro_mean=best_stat_dict[\"precision_micro\"][\"mean\"],\n",
    "\t\t\t\tprecision_micro_std=best_stat_dict[\"precision_micro\"][\"std\"],\n",
    "\t\t\t\tprecision_weighted_mean=best_stat_dict[\"precision_weighted\"][\"mean\"],\n",
    "\t\t\t\tprecision_weighted_std=best_stat_dict[\"precision_weighted\"][\"std\"],\n",
    "\t\t\t\t# Recall scores\n",
    "\t\t\t\trecall_macro_mean=best_stat_dict[\"recall_macro\"][\"mean\"],\n",
    "\t\t\t\trecall_macro_std=best_stat_dict[\"recall_macro\"][\"std\"],\n",
    "\t\t\t\trecall_micro_mean=best_stat_dict[\"recall_micro\"][\"mean\"],\n",
    "\t\t\t\trecall_micro_std=best_stat_dict[\"recall_micro\"][\"std\"],\n",
    "\t\t\t\trecall_weighted_mean=best_stat_dict[\"recall_weighted\"][\"mean\"],\n",
    "\t\t\t\trecall_weighted_std=best_stat_dict[\"recall_weighted\"][\"std\"],\n",
    "\t\t\t)\n",
    "\t\t\tprint(f'Average best statistics at fold {e}: {best_stat_dict}')\n",
    "\treturn cross_validate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe\n",
      "                                             anchorsent       sectype\n",
      "0     In summary, the open nature of the Internet as...  introduction\n",
      "1     Finally, a common data transformation method u...          none\n",
      "3     Hucaljuk and Rakipovic [15] included an expert...       results\n",
      "4     The first step in the modelling process is to ...       results\n",
      "5     Hucaljuk and Rakipovic [15] used a separate ex...       results\n",
      "...                                                 ...           ...\n",
      "1552  To support feature computation and combination...          none\n",
      "1553  To query the lexical databases we use the open...          data\n",
      "1554  To compute the score, we use a series of featu...          data\n",
      "1555  Using the strategy described in Section 3.3, w...          none\n",
      "1556  It is computed by comparing all patents in the...          none\n",
      "\n",
      "[1376 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "trainset = get_dataframe('training_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe\n",
      "                                            anchorsent       sectype\n",
      "0    This is common in software design when the UI ...  introduction\n",
      "1    The most related items intersect first, and th...          none\n",
      "2    In this study, participants performed a hierar...          none\n",
      "3    Card sorting software, xSort (Arroz, 2008) was...          none\n",
      "4    The concept of using PA to analyze SD evaluati...  introduction\n",
      "..                                                 ...           ...\n",
      "295  Data words are commonly studied in XML literat...          none\n",
      "296  It was shown in [7] that the language L={(ad1)...          none\n",
      "297  Originally they were defined on words over inf...          none\n",
      "298  What all of these languages (with the sole exc...  introduction\n",
      "299  Note that besides the operators in Definition ...          none\n",
      "\n",
      "[289 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "testset = get_dataframe('test_groundtruth_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label classes: ['cites', 'extends', 'uses_data_from', 'uses_method_in']\n",
      "Sectype classes: ['acknowledgements', 'background', 'conclusion', 'data', 'discussion', 'introduction', 'materials', 'methods', 'model', 'motivation', 'none', 'related work', 'results', 'scenario']\n"
     ]
    }
   ],
   "source": [
    "n_classes = encode_dataset({'train':trainset, 'test':testset})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NumericColumn(key='anchorsent', shape=(512,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='sectype', shape=(13,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='main_predicate', shape=(512,), default_value=None, dtype=tf.float32, normalizer_fn=None)]\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [get_dataframe_feature_shape(trainset['x'],feature) for feature in trainset['x'].keys()]\n",
    "print(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge dataset 1 and 2, because they have different distributions and thus we have to build new train and test sets. Before mergin we convert the datasets into datalists, this way we can easily shuffle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlist = listify_dataset(trainset)\n",
    "if USE_TEST_SET:\n",
    "\ttestlist = listify_dataset(testset)\n",
    "\tdatalist = trainlist + testlist\n",
    "else:\n",
    "\tdatalist = trainlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-17 16:44:14,448\tINFO resource_spec.py:205 -- Starting Ray with 1.9 GiB memory available for workers and up to 0.95 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.20.10.13',\n",
       " 'redis_address': '172.20.10.13:41115',\n",
       " 'object_store_address': '/tmp/ray/session_2019-11-17_16-44-14_442941_4310/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2019-11-17_16-44-14_442941_4310/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2019-11-17_16-44-14_442941_4310'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(num_cpus=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. Do not use code imported with sys.path.append inside ray distributed code: https://stackoverflow.com/questions/54338013/parallel-import-a-python-file-from-sibling-folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform automatic hyper-parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-17 16:44:17,021\tWARNING trial_runner.py:296 -- Attempting to resume experiment from /Users/toor/Documents/University/PhD/Project/SCAR/software/experiment/DL for SCAR/code/ray_results/hp_tuning. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.\n",
      "2019-11-17 16:44:23,511\tINFO trial_runner.py:170 -- Resuming trial.\n",
      "2019-11-17 16:44:23,512\tINFO trial_runner.py:241 -- TrialRunner resumed, ignoring new add_experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/1.9 GiB heap, 0.0/0.63 GiB objects\n",
      "Memory usage on this node: 5.1/8.0 GiB\n",
      "Result logdir: /Users/toor/Documents/University/PhD/Project/SCAR/software/experiment/DL for SCAR/code/ray_results/hp_tuning\n",
      "Number of trials: 120 ({'PENDING': 120})\n",
      "PENDING trials:\n",
      " - cross_validate_model_0_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=None,UNITS=4:\tPENDING\n",
      " - cross_validate_model_1_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=None,UNITS=4:\tPENDING\n",
      " - cross_validate_model_2_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=None,UNITS=4:\tPENDING\n",
      " - cross_validate_model_3_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=None,UNITS=4:\tPENDING\n",
      " - cross_validate_model_4_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_5_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_6_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_7_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_8_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._random_over_sampler.RandomOverSampler'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_9_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._random_over_sampler.RandomOverSampler'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_10_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._random_over_sampler.RandomOverSampler'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_11_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._random_over_sampler.RandomOverSampler'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_12_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._smote.SMOTE'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_13_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._smote.SMOTE'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_14_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._smote.SMOTE'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_15_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._smote.SMOTE'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_16_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_17_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_18_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_19_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_20_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_21_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_22_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_23_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_24_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=None,UNITS=6:\tPENDING\n",
      " - cross_validate_model_25_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=None,UNITS=6:\tPENDING\n",
      " - cross_validate_model_26_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=None,UNITS=6:\tPENDING\n",
      " - cross_validate_model_27_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=None,UNITS=6:\tPENDING\n",
      " - cross_validate_model_28_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=6:\tPENDING\n",
      " - cross_validate_model_29_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=6:\tPENDING\n",
      " - cross_validate_model_30_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=6:\tPENDING\n",
      " - cross_validate_model_31_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=6:\tPENDING\n",
      " - cross_validate_model_32_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._random_over_sampler.RandomOverSampler'>,UNITS=6:\tPENDING\n",
      " - cross_validate_model_33_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._random_over_sampler.RandomOverSampler'>,UNITS=6:\tPENDING\n",
      " - cross_validate_model_34_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._random_over_sampler.RandomOverSampler'>,UNITS=6:\tPENDING\n",
      " - cross_validate_model_35_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._random_over_sampler.RandomOverSampler'>,UNITS=6:\tPENDING\n",
      " - cross_validate_model_36_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._smote.SMOTE'>,UNITS=6:\tPENDING\n",
      " - cross_validate_model_37_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._smote.SMOTE'>,UNITS=6:\tPENDING\n",
      " - cross_validate_model_38_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._smote.SMOTE'>,UNITS=6:\tPENDING\n",
      " - cross_validate_model_39_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._smote.SMOTE'>,UNITS=6:\tPENDING\n",
      " - cross_validate_model_40_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=6:\tPENDING\n",
      " - cross_validate_model_41_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=6:\tPENDING\n",
      " - cross_validate_model_42_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=6:\tPENDING\n",
      " - cross_validate_model_43_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=6:\tPENDING\n",
      " - cross_validate_model_44_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=6:\tPENDING\n",
      " - cross_validate_model_45_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=6:\tPENDING\n",
      " - cross_validate_model_46_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=6:\tPENDING\n",
      " - cross_validate_model_47_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=6:\tPENDING\n",
      " - cross_validate_model_48_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=None,UNITS=8:\tPENDING\n",
      " - cross_validate_model_49_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=None,UNITS=8:\tPENDING\n",
      " - cross_validate_model_50_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=None,UNITS=8:\tPENDING\n",
      " - cross_validate_model_51_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=None,UNITS=8:\tPENDING\n",
      " - cross_validate_model_52_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=8:\tPENDING\n",
      " - cross_validate_model_53_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=8:\tPENDING\n",
      " - cross_validate_model_54_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=8:\tPENDING\n",
      " - cross_validate_model_55_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=8:\tPENDING\n",
      " - cross_validate_model_56_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._random_over_sampler.RandomOverSampler'>,UNITS=8:\tPENDING\n",
      " - cross_validate_model_57_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._random_over_sampler.RandomOverSampler'>,UNITS=8:\tPENDING\n",
      " - cross_validate_model_58_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._random_over_sampler.RandomOverSampler'>,UNITS=8:\tPENDING\n",
      " - cross_validate_model_59_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._random_over_sampler.RandomOverSampler'>,UNITS=8:\tPENDING\n",
      " - cross_validate_model_60_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._smote.SMOTE'>,UNITS=8:\tPENDING\n",
      " - cross_validate_model_61_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._smote.SMOTE'>,UNITS=8:\tPENDING\n",
      " - cross_validate_model_62_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._smote.SMOTE'>,UNITS=8:\tPENDING\n",
      " - cross_validate_model_63_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._smote.SMOTE'>,UNITS=8:\tPENDING\n",
      " - cross_validate_model_64_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=8:\tPENDING\n",
      " - cross_validate_model_65_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=8:\tPENDING\n",
      " - cross_validate_model_66_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=8:\tPENDING\n",
      " - cross_validate_model_67_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=8:\tPENDING\n",
      " - cross_validate_model_68_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=8:\tPENDING\n",
      " - cross_validate_model_69_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=8:\tPENDING\n",
      " - cross_validate_model_70_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=8:\tPENDING\n",
      " - cross_validate_model_71_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=8:\tPENDING\n",
      " - cross_validate_model_72_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=None,UNITS=10:\tPENDING\n",
      " - cross_validate_model_73_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=None,UNITS=10:\tPENDING\n",
      " - cross_validate_model_74_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=None,UNITS=10:\tPENDING\n",
      " - cross_validate_model_75_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=None,UNITS=10:\tPENDING\n",
      " - cross_validate_model_76_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=10:\tPENDING\n",
      " - cross_validate_model_77_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=10:\tPENDING\n",
      " - cross_validate_model_78_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=10:\tPENDING\n",
      " - cross_validate_model_79_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=10:\tPENDING\n",
      " - cross_validate_model_80_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._random_over_sampler.RandomOverSampler'>,UNITS=10:\tPENDING\n",
      " - cross_validate_model_81_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._random_over_sampler.RandomOverSampler'>,UNITS=10:\tPENDING\n",
      " - cross_validate_model_82_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._random_over_sampler.RandomOverSampler'>,UNITS=10:\tPENDING\n",
      " - cross_validate_model_83_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._random_over_sampler.RandomOverSampler'>,UNITS=10:\tPENDING\n",
      " - cross_validate_model_84_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._smote.SMOTE'>,UNITS=10:\tPENDING\n",
      " - cross_validate_model_85_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._smote.SMOTE'>,UNITS=10:\tPENDING\n",
      " - cross_validate_model_86_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._smote.SMOTE'>,UNITS=10:\tPENDING\n",
      " - cross_validate_model_87_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._smote.SMOTE'>,UNITS=10:\tPENDING\n",
      " - cross_validate_model_88_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=10:\tPENDING\n",
      " - cross_validate_model_89_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=10:\tPENDING\n",
      " - cross_validate_model_90_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=10:\tPENDING\n",
      " - cross_validate_model_91_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=10:\tPENDING\n",
      " - cross_validate_model_92_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=10:\tPENDING\n",
      " - cross_validate_model_93_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=10:\tPENDING\n",
      " - cross_validate_model_94_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=10:\tPENDING\n",
      " - cross_validate_model_95_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=10:\tPENDING\n",
      " - cross_validate_model_96_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=None,UNITS=12:\tPENDING\n",
      " - cross_validate_model_97_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=None,UNITS=12:\tPENDING\n",
      " - cross_validate_model_98_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=None,UNITS=12:\tPENDING\n",
      " - cross_validate_model_99_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=None,UNITS=12:\tPENDING\n",
      " - cross_validate_model_100_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_101_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_102_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_103_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_104_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._random_over_sampler.RandomOverSampler'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_105_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._random_over_sampler.RandomOverSampler'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_106_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._random_over_sampler.RandomOverSampler'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_107_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._random_over_sampler.RandomOverSampler'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_108_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._smote.SMOTE'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_109_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._smote.SMOTE'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_110_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._smote.SMOTE'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_111_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._smote.SMOTE'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_112_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_113_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_114_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_115_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_116_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_117_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_118_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_119_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=12:\tPENDING\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Resources requested: 1/4 CPUs, 0/0 GPUs, 0.0/1.9 GiB heap, 0.0/0.63 GiB objects\n",
      "Memory usage on this node: 5.1/8.0 GiB\n",
      "Result logdir: /Users/toor/Documents/University/PhD/Project/SCAR/software/experiment/DL for SCAR/code/ray_results/hp_tuning\n",
      "Number of trials: 120 ({'RUNNING': 1, 'PENDING': 119})\n",
      "PENDING trials:\n",
      " - cross_validate_model_1_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=None,UNITS=4:\tPENDING\n",
      " - cross_validate_model_2_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=None,UNITS=4:\tPENDING\n",
      " - cross_validate_model_3_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=None,UNITS=4:\tPENDING\n",
      " - cross_validate_model_4_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_5_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_6_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_7_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.combine._smote_tomek.SMOTETomek'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_8_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._random_over_sampler.RandomOverSampler'>,UNITS=4:\tPENDING\n",
      " - cross_validate_model_9_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._random_over_sampler.RandomOverSampler'>,UNITS=4:\tPENDING\n",
      "  ... 101 not shown\n",
      " - cross_validate_model_111_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._smote.SMOTE'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_112_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_113_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_114_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_115_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.over_sampling._adasyn.ADASYN'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_116_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_117_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_118_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=12:\tPENDING\n",
      " - cross_validate_model_119_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=4,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.001,RESAMPLING_FN=<class 'imblearn.under_sampling._prototype_selection._tomek_links.TomekLinks'>,UNITS=12:\tPENDING\n",
      "RUNNING trials:\n",
      " - cross_validate_model_0_ACTIVATION_FUNCTION=<function leaky_relu at 0x13a2882f0>,BATCH_SIZE=3,LEARNING_RATE=0.1,N_SPLITS=5,REGULARIZATION_STRENGTH=0.003,RESAMPLING_FN=None,UNITS=4:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m -------- Fold 0 --------\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m Train-set 0 indexes [   0    1    2 ... 1373 1374 1375]\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m Test-set 0 indexes [   3   12   19   27   37   47   48   49   51   56   58   60   65   73\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m    75   80   81   87   88   90   91   94   98  101  107  108  111  115\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   119  120  126  131  133  135  142  159  163  167  169  177  181  186\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   190  194  195  198  201  204  215  223  231  236  239  248  255  258\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   259  267  268  270  280  283  288  292  298  301  302  303  304  309\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   311  321  325  330  335  341  350  351  368  375  382  386  390  401\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   403  404  408  409  419  422  424  428  432  435  452  478  480  481\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   490  491  494  496  498  503  512  520  521  525  531  534  536  537\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   541  547  549  554  558  559  561  571  585  596  597  602  616  634\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   636  641  644  649  654  660  670  671  675  680  686  693  694  699\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   701  713  718  719  725  726  730  731  741  742  744  746  757  765\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   777  782  785  786  797  808  809  817  819  821  824  826  827  834\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   839  853  854  867  875  887  888  895  899  900  903  909  912  919\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m   922  925  926  929  942  945  959  962  979  986  989  990  992  996\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m  1007 1008 1027 1032 1035 1043 1047 1048 1053 1057 1059 1060 1065 1068\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m  1070 1073 1078 1083 1085 1087 1090 1092 1102 1108 1115 1116 1117 1125\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m  1128 1138 1141 1146 1148 1154 1155 1156 1163 1168 1170 1171 1173 1177\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m  1180 1181 1182 1191 1192 1194 1199 1204 1205 1211 1213 1219 1223 1228\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m  1229 1234 1241 1248 1251 1253 1261 1264 1268 1273 1284 1286 1294 1296\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m  1310 1322 1326 1328 1338 1342 1348 1356 1366 1369]\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m Train-set 0 distribution Counter({0: 597, 3: 308, 2: 121, 1: 74})\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m Test-set 0 distribution Counter({0: 155, 3: 79, 2: 28, 1: 14})\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m WARNING: Logging before flag parsing goes to stderr.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m W1117 16:44:28.823200 123145481334784 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m W1117 16:44:28.844712 123145481334784 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m W1117 16:44:28.846191 123145481334784 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m W1117 16:44:28.859238 123145481334784 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:205: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m W1117 16:44:28.859442 123145481334784 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:2115: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m W1117 16:44:28.859632 123145481334784 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:206: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m W1117 16:44:28.893503 123145481334784 deprecation.py:506] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m WARNING: Logging before flag parsing goes to stderr.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m W1117 16:44:29.386058 123145464324096 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m -------- Fold 0 --------\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m Train-set 0 indexes [   0    1    2 ... 1373 1374 1375]\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m Test-set 0 indexes [   3   12   19   27   37   47   48   49   51   56   58   60   65   73\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m    75   80   81   87   88   90   91   94   98  101  107  108  111  115\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   119  120  126  131  133  135  142  159  163  167  169  177  181  186\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   190  194  195  198  201  204  215  223  231  236  239  248  255  258\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   259  267  268  270  280  283  288  292  298  301  302  303  304  309\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   311  321  325  330  335  341  350  351  368  375  382  386  390  401\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   403  404  408  409  419  422  424  428  432  435  452  478  480  481\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   490  491  494  496  498  503  512  520  521  525  531  534  536  537\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   541  547  549  554  558  559  561  571  585  596  597  602  616  634\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   636  641  644  649  654  660  670  671  675  680  686  693  694  699\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   701  713  718  719  725  726  730  731  741  742  744  746  757  765\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   777  782  785  786  797  808  809  817  819  821  824  826  827  834\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   839  853  854  867  875  887  888  895  899  900  903  909  912  919\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m   922  925  926  929  942  945  959  962  979  986  989  990  992  996\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m  1007 1008 1027 1032 1035 1043 1047 1048 1053 1057 1059 1060 1065 1068\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m  1070 1073 1078 1083 1085 1087 1090 1092 1102 1108 1115 1116 1117 1125\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m  1128 1138 1141 1146 1148 1154 1155 1156 1163 1168 1170 1171 1173 1177\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m  1180 1181 1182 1191 1192 1194 1199 1204 1205 1211 1213 1219 1223 1228\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m  1229 1234 1241 1248 1251 1253 1261 1264 1268 1273 1284 1286 1294 1296\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m  1310 1322 1326 1328 1338 1342 1348 1356 1366 1369]\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m Train-set 0 distribution Counter({0: 597, 3: 308, 2: 121, 1: 74})\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m Test-set 0 distribution Counter({0: 155, 3: 79, 2: 28, 1: 14})\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m W1117 16:44:29.410149 123145464324096 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m W1117 16:44:29.412214 123145464324096 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m W1117 16:44:29.426127 123145464324096 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:205: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m W1117 16:44:29.426441 123145464324096 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:2115: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m W1117 16:44:29.426733 123145464324096 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:206: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m W1117 16:44:29.466842 123145464324096 deprecation.py:506] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m W1117 16:44:29.665950 123145481334784 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m W1117 16:44:29.790632 123145481334784 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tf_metrics/__init__.py:152: The name tf.diag_part is deprecated. Please use tf.linalg.tensor_diag_part instead.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m W1117 16:44:29.811521 123145481334784 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tf_metrics/__init__.py:140: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m WARNING: Logging before flag parsing goes to stderr.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m W1117 16:44:30.081129 123145437450240 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m -------- Fold 0 --------\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m Train-set 0 indexes [   0    1    2 ... 1373 1374 1375]\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m Test-set 0 indexes [   3   12   19   27   37   47   48   49   51   56   58   60   65   73\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m    75   80   81   87   88   90   91   94   98  101  107  108  111  115\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   119  120  126  131  133  135  142  159  163  167  169  177  181  186\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   190  194  195  198  201  204  215  223  231  236  239  248  255  258\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   259  267  268  270  280  283  288  292  298  301  302  303  304  309\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   311  321  325  330  335  341  350  351  368  375  382  386  390  401\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   403  404  408  409  419  422  424  428  432  435  452  478  480  481\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   490  491  494  496  498  503  512  520  521  525  531  534  536  537\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   541  547  549  554  558  559  561  571  585  596  597  602  616  634\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   636  641  644  649  654  660  670  671  675  680  686  693  694  699\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   701  713  718  719  725  726  730  731  741  742  744  746  757  765\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   777  782  785  786  797  808  809  817  819  821  824  826  827  834\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   839  853  854  867  875  887  888  895  899  900  903  909  912  919\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m   922  925  926  929  942  945  959  962  979  986  989  990  992  996\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m  1007 1008 1027 1032 1035 1043 1047 1048 1053 1057 1059 1060 1065 1068\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m  1070 1073 1078 1083 1085 1087 1090 1092 1102 1108 1115 1116 1117 1125\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m  1128 1138 1141 1146 1148 1154 1155 1156 1163 1168 1170 1171 1173 1177\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m  1180 1181 1182 1191 1192 1194 1199 1204 1205 1211 1213 1219 1223 1228\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m  1229 1234 1241 1248 1251 1253 1261 1264 1268 1273 1284 1286 1294 1296\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m  1310 1322 1326 1328 1338 1342 1348 1356 1366 1369]\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m Train-set 0 distribution Counter({0: 597, 3: 308, 2: 121, 1: 74})\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m Test-set 0 distribution Counter({0: 155, 3: 79, 2: 28, 1: 14})\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m W1117 16:44:30.102828 123145437450240 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m W1117 16:44:30.103783 123145437450240 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m W1117 16:44:30.114916 123145437450240 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:205: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m W1117 16:44:30.115210 123145437450240 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:2115: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m W1117 16:44:30.115617 123145437450240 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:206: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m W1117 16:44:30.151512 123145437450240 deprecation.py:506] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m W1117 16:44:30.180250 123145464324096 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m W1117 16:44:30.355269 123145464324096 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tf_metrics/__init__.py:152: The name tf.diag_part is deprecated. Please use tf.linalg.tensor_diag_part instead.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m W1117 16:44:30.368822 123145464324096 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tf_metrics/__init__.py:140: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m -------- Fold 0 --------\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m Train-set 0 indexes [   0    1    2 ... 1373 1374 1375]\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m Test-set 0 indexes [   3   12   19   27   37   47   48   49   51   56   58   60   65   73\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m    75   80   81   87   88   90   91   94   98  101  107  108  111  115\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   119  120  126  131  133  135  142  159  163  167  169  177  181  186\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   190  194  195  198  201  204  215  223  231  236  239  248  255  258\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   259  267  268  270  280  283  288  292  298  301  302  303  304  309\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   311  321  325  330  335  341  350  351  368  375  382  386  390  401\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   403  404  408  409  419  422  424  428  432  435  452  478  480  481\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   490  491  494  496  498  503  512  520  521  525  531  534  536  537\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   541  547  549  554  558  559  561  571  585  596  597  602  616  634\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   636  641  644  649  654  660  670  671  675  680  686  693  694  699\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   701  713  718  719  725  726  730  731  741  742  744  746  757  765\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   777  782  785  786  797  808  809  817  819  821  824  826  827  834\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   839  853  854  867  875  887  888  895  899  900  903  909  912  919\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m   922  925  926  929  942  945  959  962  979  986  989  990  992  996\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m  1007 1008 1027 1032 1035 1043 1047 1048 1053 1057 1059 1060 1065 1068\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m  1070 1073 1078 1083 1085 1087 1090 1092 1102 1108 1115 1116 1117 1125\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m  1128 1138 1141 1146 1148 1154 1155 1156 1163 1168 1170 1171 1173 1177\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m  1180 1181 1182 1191 1192 1194 1199 1204 1205 1211 1213 1219 1223 1228\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m  1229 1234 1241 1248 1251 1253 1261 1264 1268 1273 1284 1286 1294 1296\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m  1310 1322 1326 1328 1338 1342 1348 1356 1366 1369]\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m Train-set 0 distribution Counter({0: 597, 3: 308, 2: 121, 1: 74})\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m Test-set 0 distribution Counter({0: 155, 3: 79, 2: 28, 1: 14})\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m WARNING: Logging before flag parsing goes to stderr.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m W1117 16:44:30.661140 123145399664640 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m W1117 16:44:30.681818 123145399664640 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m W1117 16:44:30.683483 123145399664640 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m W1117 16:44:30.696483 123145399664640 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:205: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m W1117 16:44:30.696764 123145399664640 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:2115: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m W1117 16:44:30.697000 123145399664640 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:206: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m W1117 16:44:30.730165 123145399664640 deprecation.py:506] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m W1117 16:44:30.895213 123145437450240 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m W1117 16:44:31.028026 123145437450240 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tf_metrics/__init__.py:152: The name tf.diag_part is deprecated. Please use tf.linalg.tensor_diag_part instead.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m W1117 16:44:31.046041 123145437450240 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tf_metrics/__init__.py:140: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m W1117 16:44:31.405916 123145399664640 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m W1117 16:44:31.560137 123145399664640 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tf_metrics/__init__.py:152: The name tf.diag_part is deprecated. Please use tf.linalg.tensor_diag_part instead.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m W1117 16:44:31.576375 123145399664640 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tf_metrics/__init__.py:140: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m 2019-11-17 16:44:34.966265: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m W1117 16:44:35.394752 123145481334784 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:875: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4323)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m 2019-11-17 16:44:35.514277: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m W1117 16:44:35.990619 123145464324096 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:875: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m 2019-11-17 16:44:36.008845: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m 2019-11-17 16:44:36.445682: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m W1117 16:44:36.579940 123145437450240 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:875: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4324)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m W1117 16:44:36.982455 123145399664640 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:875: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m To construct input pipelines, use the `tf.data` module.\n",
      "\u001b[2m\u001b[36m(pid=4322)\u001b[0m W1117 16:44:41.264599 123145464324096 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 278 vs previous value: 278. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m W1117 16:44:41.924196 123145399664640 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 133 vs previous value: 133. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "\u001b[2m\u001b[36m(pid=4325)\u001b[0m W1117 16:44:41.952452 123145399664640 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 156 vs previous value: 156. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n"
     ]
    }
   ],
   "source": [
    "experiment_name = 'hp_tuning'\n",
    "local_dir = os.path.join('.','ray_results')\n",
    "analysis = tune.run( # https://ray.readthedocs.io/en/latest/tune-package-ref.html#ray.tune.run\n",
    "    build_cross_validate_model(datalist),\n",
    "    num_samples=1, # Number of times to sample from the hyperparameter space. Defaults to 1. If grid_search is provided as an argument, the grid will be repeated num_samples of times.\n",
    "    name=experiment_name,\n",
    "    local_dir=local_dir,\n",
    "    resume=os.path.isdir(os.path.join(local_dir,experiment_name)),\n",
    "    #global_checkpoint_period=15*60,\n",
    "    #keep_checkpoints_num=3,\n",
    "    config={ \n",
    "        \"N_SPLITS\": tune.grid_search([\n",
    "            #3,\n",
    "            #4,\n",
    "            5,\n",
    "        ]), \n",
    "        \"RESAMPLING_FN\": tune.grid_search([\n",
    "            None,\n",
    "            #combine.SMOTEENN, \n",
    "            combine.SMOTETomek, \n",
    "            over_sampling.RandomOverSampler,\n",
    "            over_sampling.SMOTE,\n",
    "            over_sampling.ADASYN,\n",
    "            #under_sampling.RandomUnderSampler,\n",
    "            #under_sampling.EditedNearestNeighbours,\n",
    "            under_sampling.TomekLinks,\n",
    "        ]),\n",
    "        \"BATCH_SIZE\": tune.grid_search([\n",
    "            #2,\n",
    "            3, \n",
    "            4,\n",
    "        ]),\n",
    "        'UNITS': tune.grid_search([\n",
    "            4, \n",
    "            6, \n",
    "            8, \n",
    "            10,\n",
    "            12,\n",
    "        ]),\n",
    "        'ACTIVATION_FUNCTION': tune.grid_search([\n",
    "            #None,\n",
    "            #tf.nn.relu,\n",
    "            tf.nn.leaky_relu,\n",
    "            #tf.nn.selu,\n",
    "            #tf.nn.tanh,\n",
    "        ]),\n",
    "        #'LEARNING_RATE': tune.sample_from(lambda spec: 0.1*3*random.random()),\n",
    "        'LEARNING_RATE': tune.grid_search([\n",
    "            #0.3,\n",
    "            0.1,\n",
    "            #0.03,\n",
    "            #0.01,\n",
    "        ]),\n",
    "        'REGULARIZATION_STRENGTH': tune.grid_search([\n",
    "            #0.01,\n",
    "            0.003,\n",
    "            0.001,\n",
    "            #0.0003,\n",
    "            #0.0001,\n",
    "        ]),\n",
    "    },\n",
    "    scheduler=AsyncHyperBandScheduler(\n",
    "        metric='f1_macro_mean',\n",
    "        mode='max',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Best config: \", analysis.get_best_config(metric='f1_macro_mean'))\n",
    "analysis_df = analysis.dataframe()\n",
    "#analysis_df['f1_macro_min'] = analysis_df['f1_macro_mean']-analysis_df['f1_macro_std']\n",
    "#analysis_df['f1_macro_max'] = analysis_df['f1_macro_mean']+analysis_df['f1_macro_std']\n",
    "analysis_df['config/RESAMPLING_FN'] = analysis_df['config/RESAMPLING_FN'].map(lambda x: x.split('.')[-1][:-2] if x is not None else x)\n",
    "best_stats = analysis_df.sort_values(['timesteps_total','f1_macro_mean'], ascending=[False,False]).filter(regex='timesteps_total|macro|config|logdir').iloc[:10]\n",
    "best_stats.style"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
