[{"string": "In summary, the open nature of the Internet as a transaction infrastructure and its global nature create uncertainty around on-line transactions, and this poses trust, credibility and risk as crucial elements of e-transaction (Hoffman et al., 1999).", "probabilities": [0.8504377007484436, 0.035428788512945175, 0.02613729238510132, 0.08799616992473602]}, {"string": "Finally, a common data transformation method used in image classification that derives a single label binary classifier for every class in the class set is called Binary Relevance (BR) (Boutell et al., 2003).", "probabilities": [0.08583805710077286, 0.009700451977550983, 0.35758885741233826, 0.546872615814209]}, {"string": "Hucaljuk and Rakipovic [15] included an expert-selected feature set in addition to their initial feature set.", "probabilities": [0.7259100079536438, 0.12494630366563797, 0.04620316997170448, 0.10294059664011002]}, {"string": "The first step in the modelling process is to select which candidate models will be used in the experimentation [34].", "probabilities": [0.3583418130874634, 0.020626842975616455, 0.15362270176410675, 0.467408686876297]}, {"string": "Hucaljuk and Rakipovic [15] used a separate expert-selected feature set against their own feature set, to investigate the value of expert opinion for feature selection.", "probabilities": [0.25044554471969604, 0.02210184559226036, 0.21101965010166168, 0.5164330005645752]}, {"string": "For example, Cao [5] used seasons up from 2005/2006 to 2009/2010 were used as the training data, and the 2010/2011 season was used as the test data, to evaluate models that predict basketball match results.", "probabilities": [0.39212366938591003, 0.02057746611535549, 0.13923980295658112, 0.4480590522289276]}, {"string": "SensorSim [12] is an extension to ns-2.", "probabilities": [0.14336439967155457, 0.02548438310623169, 0.464943528175354, 0.36620765924453735]}, {"string": "This deployment model was borrowed from the proposal presented in [11].", "probabilities": [0.6052338480949402, 0.08758723735809326, 0.058935705572366714, 0.24824319779872894]}, {"string": "Average field measurement is performed by the distributed self-clocking scheme described in [59].", "probabilities": [0.0869799256324768, 0.00978545006364584, 0.3560401201248169, 0.5471944808959961]}, {"string": "Event-to-Sink Reliable Transport (ESRT) decreases congestion near the data sink by regulating the frequency of sensors reporting [31].", "probabilities": [0.7485275864601135, 0.05328070744872093, 0.06415561586618423, 0.1340361088514328]}, {"string": "The proofs can be considered as an extension of the interactive protocols that were presented in [23].", "probabilities": [0.1439444124698639, 0.7760549187660217, 0.041353411972522736, 0.038647301495075226]}, {"string": "This paper significantly extends our preliminary work published in [32].", "probabilities": [0.09507528692483902, 0.8479922413825989, 0.02906513214111328, 0.027867350727319717]}, {"string": "If a bit error occurs in the encrypted message block during transmission in the wireless channel, this error will expand to all bits of the decrypted message with a probability of 50%; this is a result of the Strict Avalanche Criterion (SAC) of the encryption algorithms [31].", "probabilities": [0.8096238970756531, 0.03816577047109604, 0.039920881390571594, 0.1122894212603569]}, {"string": "The idea of having a cascade of mix servers was expanded in [28] which proposed onion routing mechanisms that became the de facto privacy preserving protocols.", "probabilities": [0.8169105052947998, 0.06179360672831535, 0.030566155910491943, 0.09072968363761902]}, {"string": "This paper substantially extends from [41] by adding simulation validation using ns-3 (Section 5) as well as new materials, including a theoretical analysis of the protocol s convergence, accuracy and resiliency properties (Section 4), a sensitivity analysis of the effect of trust formation on application performance (Section 6), and a discussion on applicability (Section 7).", "probabilities": [0.16292890906333923, 0.671350359916687, 0.05471981316804886, 0.1110008955001831]}, {"string": "Those reports might reveal the occurrence of abnormal services or attacks;(b)Pattern learning which executes algorithms and statistical analysis models for StM data attributes and features discovery (e.g., ARL [20]).", "probabilities": [0.7958874702453613, 0.02542414516210556, 0.034206219017505646, 0.1444820761680603]}, {"string": "We followed the recommendations as prescribed by FERA to conduct the evaluation of the two versions of RAMSES (the one published in [15] - without considering RAModel - and the evolved version described in this paper).", "probabilities": [0.08586561679840088, 0.009730195626616478, 0.35763785243034363, 0.5467664003372192]}, {"string": "This paper is an updated and improved version of the conference paper [127].", "probabilities": [0.20644530653953552, 0.07481417804956436, 0.4993138313293457, 0.2194267064332962]}, {"string": "The OpenNEbula system extends the benefits of virtualization platforms from a single physical resource to a pool of resources, decoupling the server, not only from the physical infrastructure but also from the physical location [17].", "probabilities": [0.8423405885696411, 0.03623545914888382, 0.028686143457889557, 0.0927378460764885]}, {"string": "In this paper, we adopt goal elicitation approach presented in [14].", "probabilities": [0.0857762023806572, 0.009737662971019745, 0.35780656337738037, 0.5466795563697815]}, {"string": "Fig. 1 gives an overview of the CBS development life cycle, adapted from [19].", "probabilities": [0.4968431293964386, 0.11396517604589462, 0.1944883018732071, 0.19470340013504028]}, {"string": "Altered Vista system, by Walker et al. [44], has been proposed for the recommendation of learning objects based on collaborative filtering applied in an educational setting.", "probabilities": [0.6915035843849182, 0.09411019086837769, 0.048974424600601196, 0.16541177034378052]}, {"string": "As inferred from the name  ultra large scale systems , this type of system includes those whose scale and size are far beyond today s systems [1].", "probabilities": [0.8499197959899902, 0.035526204854249954, 0.026296213269233704, 0.08825775235891342]}, {"string": "The scale in ultra large scale systems changes everything [1].", "probabilities": [0.5796343684196472, 0.13220733404159546, 0.1261339783668518, 0.16202428936958313]}, {"string": "Note that a detailed descriptions of the composite benchmark functions are available in the CEC 2005 technical report [52].", "probabilities": [0.8487513661384583, 0.035716015845537186, 0.02666575089097023, 0.08886685222387314]}, {"string": "The other test beds that we have chosen are six composite benchmark functions from a CEC 2005 special session [52].", "probabilities": [0.2024155706167221, 0.06905189156532288, 0.507961094379425, 0.22057142853736877]}, {"string": "Note that we use a similar penalty function for GWO to perform a fair comparison [63].", "probabilities": [0.33276423811912537, 0.022799527272582054, 0.1654660403728485, 0.47897017002105713]}, {"string": "Ideally it should include all the data needed for the construction instead of the data being scattered throughout numerous drawings, folders, tables, reports, documents, etc. [9].", "probabilities": [0.7342405319213867, 0.06499525159597397, 0.06467641890048981, 0.13608776032924652]}, {"string": "Biddiscombe [8] reported the use of laser scanning on an actual tunneling project for controlling as-built dimensions.", "probabilities": [0.8486669659614563, 0.03648367524147034, 0.02646520733833313, 0.08838418126106262]}, {"string": "Another approach that considerably resembles the methodology proposed in this paper is the BPEL for Semantic Web Services (BPEL4SWS) [44] language, which extends the standard WS-BPEL 2.0 specification.", "probabilities": [0.6392436623573303, 0.07548169791698456, 0.05550988391041756, 0.22976477444171906]}, {"string": "[5] develops and evaluates several new approaches to automatically generate file attributes based on context.", "probabilities": [0.08970049023628235, 0.8649702668190002, 0.02272103913128376, 0.022608231753110886]}, {"string": "Unfortunately, these traditional hierarchies often do not scale to large data collections and to the fine-grained classifications required [5].", "probabilities": [0.8415082693099976, 0.037108466029167175, 0.028918227180838585, 0.09246508777141571]}, {"string": "  Although some FM tools show some integration with related (same lifecycle stage) systems, Nelson et al. [7] report  there are still limitations in the level of communication between different subsystems of an FM system and between the helpdesk and building management systems  as well as  inadequate links between FM and decision analysis tools .", "probabilities": [0.8503002524375916, 0.03554336726665497, 0.026153407990932465, 0.08800294995307922]}, {"string": "One specific FM application is reported by [20].", "probabilities": [0.651032567024231, 0.22889389097690582, 0.037204544991254807, 0.08286897838115692]}, {"string": "According to Caldas et al. [11], GPS applications have been applied to construction practices, such as positioning of equipment and surveying.", "probabilities": [0.7221686840057373, 0.0194287970662117, 0.039254870265722275, 0.21914765238761902]}, {"string": "In this paper, Semantic Texton Forests (STFs) [37] are used.", "probabilities": [0.11220305413007736, 0.010974918492138386, 0.3235057592391968, 0.5533162951469421]}, {"string": "OAM extends CPM to include the representation of assembly products and the relationships between products components [33].", "probabilities": [0.790540874004364, 0.06009232997894287, 0.04170116409659386, 0.10766555368900299]}, {"string": "A study conducted by Golden reported significant variations in the style and content of the requirement statements as a part of his study [41].", "probabilities": [0.8412238955497742, 0.04184833541512489, 0.027583535760641098, 0.08934424072504044]}, {"string": "Having reviewed the KBE definitions, the statement of Ammar-Khodja et al. [20] is indeed appropriate: the definitions available in literature are quite similar.", "probabilities": [0.679244875907898, 0.10878792405128479, 0.05046815052628517, 0.16149911284446716]}, {"string": "Moreover, a scenario matrix was developed that extends the process matrix [43] to handle multi-model information resources.", "probabilities": [0.2036573886871338, 0.070468470454216, 0.5055131912231445, 0.2203609049320221]}, {"string": "In the last years, the research is increasingly focusing on the study of ontology-based approaches for product lifecycles interoperability in extended enterprise: Bock et al. [31] describe an example of a product modelling language to support collaborative design, combining the benefits of ontology with expanded capabilities in conventional product modelling language.", "probabilities": [0.8504490852355957, 0.035375308245420456, 0.026139449328184128, 0.08803609013557434]}, {"string": "De Medeiros and Schwabe expanded IBIS and added the semantic relations by Kuaba approach, which provides an ontological vocabulary and a set of rules described in F-logic [10] in order to support software design reuse.", "probabilities": [0.6388900876045227, 0.06466563791036606, 0.12130016833543777, 0.1751440167427063]}, {"string": "McCall [9] proposed the Procedural Hierarchy of Issues (PHI) model, which expands the range of issues in the IBIS model and improves their relationships.", "probabilities": [0.8486022353172302, 0.03576962649822235, 0.026704033836722374, 0.0889241173863411]}, {"string": "In the following section, we propose an information measure based on modified Carnap s entropy concept [26] that we call expanded tessellation entropy.", "probabilities": [0.08546348661184311, 0.009690320119261742, 0.35818567872047424, 0.5466604828834534]}, {"string": "This work has been documented in [30].", "probabilities": [0.8106122612953186, 0.03614679351449013, 0.03731774166226387, 0.11592330783605576]}, {"string": "Alejandra Segura et al. [12] revealed that search performance, such as novelty, coverage, and precision, are different according to the relations used when a query is expanded.", "probabilities": [0.8481184244155884, 0.035300154238939285, 0.02687196433544159, 0.08970952779054642]}, {"string": "This module is constructed using LIBSVM [32], computer software designed to perform supervised machine learning based on the support vector machine (SVM) model.", "probabilities": [0.08537585288286209, 0.009676551446318626, 0.35826000571250916, 0.546687662601471]}, {"string": "OWL is a vocabulary extension for the Resource Description Framework (RDF) [12].", "probabilities": [0.2544465959072113, 0.0634499192237854, 0.436021625995636, 0.2460819035768509]}, {"string": "The operator is a scoring type based on Yager s OWA operator [73], and the prioritized aggregation includes three complex calculation processes for priority analysis.", "probabilities": [0.08897659927606583, 0.010266967117786407, 0.36151906847953796, 0.5392374396324158]}, {"string": "OntoAMAS extends TextViz [33] another Prot g  plug-in dedicated to documents semantic annotation.", "probabilities": [0.23106922209262848, 0.11458051204681396, 0.44338494539260864, 0.21096527576446533]}, {"string": "Bouramoul et al. [24] improved the document ranking of current search engines (Google, Bing, and Yahoo) through re-ranking the top retrieved results based on the similarity between the expanded document vector and the query vector, where both vectors were expanded with WordNet concepts linked by semantic relations.", "probabilities": [0.7333114743232727, 0.020129183307290077, 0.038750335574150085, 0.20780892670154572]}, {"string": "AlMasri et al. [25] tackled the term mismatch problem for document ranking through modifying documents according to a given query and semantic relations between terms, and adapted a number of language models to expand a document by the query terms that have semantically-related document terms but do not appear in the document.", "probabilities": [0.762694239616394, 0.034755393862724304, 0.03979077562689781, 0.16275958716869354]}, {"string": "MMOY holds a domain-specific knowledge graph, so it can be used to expand user s query [20] so as to improve the retrieval effectiveness.", "probabilities": [0.5563346743583679, 0.09943312406539917, 0.10348941385746002, 0.24074287712574005]}, {"string": "In our case study, we used C-Support Vector Classification [7].", "probabilities": [0.0855565145611763, 0.00970421638339758, 0.35805544257164, 0.5466837882995605]}, {"string": "The use of the resources available on the Gateway has been expanded from those suggested in [33] with the introduction of context information such as region, network information and location information.", "probabilities": [0.8427431583404541, 0.03491460904479027, 0.028497928753495216, 0.09384432435035706]}, {"string": "However, design practices are being applied to an expanding scope of activities which include from digital product interaction of graphic areas, product design, engineer design, product service design, even to business strategy and social policy [3].", "probabilities": [0.8340327739715576, 0.03932621330022812, 0.03081180900335312, 0.09582921862602234]}, {"string": "In today s competitive and expanding global marketplace, competitive advantage lies with those companies that understand and respond quickly to dynamic user requirements in product development while able to bring the product to the market sooner and guaranteeing the quality, reliability and performance [6].", "probabilities": [0.7562642097473145, 0.06062709167599678, 0.055785391479730606, 0.12732325494289398]}, {"string": "The ongoing digitization of planning processes in the building sector has become a major improvement for the AEC industry and is being expanded into the infrastructure domain [10].", "probabilities": [0.7063737511634827, 0.07430076599121094, 0.07436327636241913, 0.1449621617794037]}, {"string": "Previous research extends the LoD concept used in the GIS domain towards multi-scale representations of building information models, particularly used for the modeling of shield tunnels [9].", "probabilities": [0.792171835899353, 0.026336051523685455, 0.035104017704725266, 0.14638808369636536]}, {"string": "It then expanded and enriched to co-word networks in the following researches [4].", "probabilities": [0.6829454302787781, 0.07989856600761414, 0.08627333492040634, 0.15088264644145966]}, {"string": "Adjustable backboards, foam supports and wooden lateral guides were used for comfort, support, assisted in consistent postural alignment and minimized error during readings (Gerke et al., 2011).", "probabilities": [0.7572522759437561, 0.11444869637489319, 0.036042552441358566, 0.09225647151470184]}, {"string": "The subject then walked two laps around the room followed by repositioning and measurement on the stadiometer (Stothart and McGill, 2000).", "probabilities": [0.21541772782802582, 0.017128961160779, 0.2605975270271301, 0.5068557858467102]}, {"string": "Additionally, the DSI (angle between a line drawn parallel along the posterior aspect of the proximal sacrum and the horizontal plane) (Bierma-Zeinstra et al., 2001) was recorded at the S1 spinous process.", "probabilities": [0.5209574699401855, 0.1412045955657959, 0.16151542961597443, 0.1763225942850113]}, {"string": "For an alpha level of 0.05, power of 80%, assumed spinal height increase of 1.7 mm (Magnusson et al., 1994) and estimated standard deviation (SD) of 3.0 mm from pilot testing, it was calculated that 27 subjects would be required for detecting a significant difference in spinal height following the sustained PSS with or without lumbar support.", "probabilities": [0.683077335357666, 0.17743384838104248, 0.041623253375291824, 0.09786559641361237]}, {"string": "These communication differences may be a reflection of the team members having distinct responsibilities and complex interdependencies with one another (Kozlowski and Ilgen, 2006).", "probabilities": [0.850139856338501, 0.035504136234521866, 0.026219109073281288, 0.08813682198524475]}, {"string": "For example, Kelly and McGrath (1985) found differences in the rate of occurrence of certain content categories based on cognitive demand levels when they evaluated ad hoc teams performing the same sets of tasks under two different time limits (a relaxed condition vs. a time pressure one in which participants had to complete the same task in half the time).", "probabilities": [0.8500860929489136, 0.034595388919115067, 0.02633936144411564, 0.08897910267114639]}, {"string": "Also, Serfaty et al. (1993) found that increased time pressure was associated with a higher  anticipation ratio .", "probabilities": [0.846741259098053, 0.03652745485305786, 0.027129409834742546, 0.08960194885730743]}, {"string": "Likewise, Funke and Galster (2007b) tested text only, voice only, and text & voice modes, and found that teams communicated less in the text only condition compared to the other two.", "probabilities": [0.8483942151069641, 0.03669821098446846, 0.026507211849093437, 0.08840037882328033]}, {"string": "From a common ground perspective this is not surprising since people shape their interactions according to the communication medium (Clark and Brennan, 1991).", "probabilities": [0.8504936695098877, 0.03541269153356552, 0.026121966540813446, 0.08797167241573334]}, {"string": "The communication richness, the nature of team interactions and the requirements to support team communications and activities differ according to the characteristics of the communication medium used such as copresence, visibility, audibility, sequentiality reviewability, and revisability (Priest et al., 2006).", "probabilities": [0.8144474625587463, 0.039367545396089554, 0.03726455196738243, 0.10892035067081451]}, {"string": "On a another meta-analysis, team tenure was not significantly correlated with team performance effects (Bell, 2007).", "probabilities": [0.821834921836853, 0.05185620114207268, 0.03174499049782753, 0.09456396102905273]}, {"string": "Mesmer-Magnus and DeChurch (2009) confirmed that when teams share information team performance improves.", "probabilities": [0.8502805829048157, 0.03546503558754921, 0.0261835977435112, 0.08807084709405899]}, {"string": "For instance, Nyssen and Blavier (2009) reported that teams with more experienced members talked less, however familiarity was not mentioned.", "probabilities": [0.850458562374115, 0.03542681783437729, 0.026130108162760735, 0.08798442035913467]}, {"string": "In a complementary study Wildman et al. (2012) taxonomy includes a classification of team task types along with six team-level dimensions including: task interdependence, communication structure, physical distribution, role structure, leadership structure, and team life span.", "probabilities": [0.7102020978927612, 0.06738220900297165, 0.07560122758150101, 0.1468145251274109]}, {"string": "Hollenbeck et al. (2012) proposed a three-dimensional framework based on three constructs: skill differentiation (from a team in which each team member has the same skills to a team in which each member has completely different skills), authority differentiation (from self-managing teams to those who have a  leader who has full authority ), and temporal stability (from newly formed teams to those whose members that have worked together for a long time).", "probabilities": [0.8490568399429321, 0.03606647253036499, 0.02642023377120495, 0.08845634758472443]}, {"string": "A threshold of 200 ms was used for detecting dwells based on the minimum viewing time of 150 ms required to process and understand a complex pattern (Thorpe et al., 1996).", "probabilities": [0.5358567833900452, 0.024012746289372444, 0.08744312077760696, 0.3526873290538788]}, {"string": "Fundamentally, network analysis offers methods that explore the relationship between items in a connected system (Scott, 2013).", "probabilities": [0.8504785299301147, 0.035411182790994644, 0.026126982644200325, 0.08798325061798096]}, {"string": "Graphic viewing networks were constructed to visualise the most common paths taken between the nine available information sources (Starke et al., 2017).", "probabilities": [0.20339526236057281, 0.06907516717910767, 0.506714940071106, 0.2208145558834076]}, {"string": "Associated metrics partially overlap with metrics related to transition matrices (Holmqvist et al., 2011) already established in the vision literature.", "probabilities": [0.8299225568771362, 0.05075271427631378, 0.02912415936589241, 0.09020063281059265]}, {"string": "In network analysis as applied to vision research (Starke et al., 2017), a  node  is a ROI connected to another ROI through an  edge  (gaze shift).", "probabilities": [0.8487517237663269, 0.03546105697751045, 0.02666384167969227, 0.08912342041730881]}, {"string": "The normalised Levenshtein distance was calculated by dividing the calculated Levenshtein distance by the longest string (Josephson and Holmes, 2006), where 100% means that two paths are completely different.", "probabilities": [0.085364431142807, 0.009675410576164722, 0.3582748770713806, 0.5466852784156799]}, {"string": "Scan patterns are commonly defined as the sequence in which people attend to different regions of interest (ROIs) in a given display (Holmqvist et al., 2011).", "probabilities": [0.8454766273498535, 0.03627773001790047, 0.027700968086719513, 0.09054464846849442]}, {"string": "Using cognitive modelling, we have recently shown that given a validity distribution akin to the present study and the need for eye movements, an optimum number of three to four ROIs would theoretically be attended to (Chen et al., 2017) based on an assumed Markov Decision Process and the learning and integration of the three best cues.", "probabilities": [0.6802749633789062, 0.19842885434627533, 0.036672577261924744, 0.08462360501289368]}, {"string": "The exception in Chen et al. (2017) was condition  FD , where the predicted number of attended ROIs rose to six, while in the present study the observed average number of attended ROIs for condition  FD  was even higher (eight out of nine).", "probabilities": [0.81617271900177, 0.06389141082763672, 0.030154963955283165, 0.08978088200092316]}, {"string": "This finding is in line with the established literature on approximations in human decision making, where, given the nature of decision making under uncertainty, there may not be much benefit to complex cue integration, with heuristics and short-cuts often leading to comparable task performance (Gigerenzer, 2008).", "probabilities": [0.8248631954193115, 0.028879623860120773, 0.03679036721587181, 0.1094667911529541]}, {"string": "Despite these efforts, our understanding of principles underlying visual interaction with UIs is considered under-developed (Ehmke and Wilson, 2007), and comparatively little work systematically investigates the effect of data presentation on visual scanning approaches and information foraging.", "probabilities": [0.847241222858429, 0.0337294302880764, 0.02730037458240986, 0.0917290449142456]}, {"string": "In a field study investigating visual sampling across multiple displays in a road traffic control room (Starke et al., 2017), we recently showed that search activity was idiosyncratic (with individuals adopting different patterns of search) but also preferential (with some information displays being attended to more often than others).", "probabilities": [0.6181899905204773, 0.21639811992645264, 0.05941900610923767, 0.1059928834438324]}, {"string": "Similarly, Paeye et al. (2016) showed that preferentially attended ROIs can be controlled through reinforcement learning when working with a gaze-contingent study design.", "probabilities": [0.850231945514679, 0.035482052713632584, 0.02619456872344017, 0.0880914106965065]}, {"string": "Higher similarity in scan patterns than this has previously been observed for repeated viewing and imagination of geometrical shapes, ranging from 18% to 25% (Brandt and Stark, 1997) in a task which provides a reduced action and decision space.", "probabilities": [0.8504747748374939, 0.035425398498773575, 0.026125023141503334, 0.08797474950551987]}, {"string": "In case of significance, post hoc Mann-Whitney U tests were performed for all pairwise comparisons (six per variable), corrected for multiple testing using the Bonferroni correction (Field, 2013).", "probabilities": [0.08536277711391449, 0.009675275534391403, 0.3582770824432373, 0.5466849207878113]}, {"string": "When reading alphanumerical data, a participant has to foveate (attend with central vision) a ROI with often multiple saccades (eye movements) to gather information, since in reading only a few words are extracted at a time (Rayner, 1998).", "probabilities": [0.8504275679588318, 0.035424210131168365, 0.026142451912164688, 0.08800573647022247]}, {"string": "At present, some of the open questions in scan pattern analysis concern the meaningful comparison of scan patterns, the consistency of scan patterns, as well as the relationship between scan patterns and cognitive processes (Holmqvist et al., 2011).", "probabilities": [0.8503347635269165, 0.0354439951479435, 0.02616848237812519, 0.08805269002914429]}, {"string": "In air traffic monitoring, visual scanning shows a tendency towards repeatable patterns independent of changes in the locations of points of interest within a stationary display (Ellis and Stark, 1986).", "probabilities": [0.850493848323822, 0.03541221842169762, 0.026121972128748894, 0.08797194063663483]}, {"string": "Ultimately, the similarity of scan patterns depends on the study design and may   in part   arise from the consistency of image characteristics (Josephson and Holmes, 2002).", "probabilities": [0.8504358530044556, 0.03542667627334595, 0.02613851986825466, 0.08799895644187927]}, {"string": "The same holds true for static stimuli such as dermatology slides (Vaidyanathan et al., 2014).", "probabilities": [0.8465578556060791, 0.03683901205658913, 0.027097903192043304, 0.0895051658153534]}, {"string": "Familiarity and expertise may affect the consistency of scan patterns: in dynamic tasks, scan patterns can vary in consistency between novices and experts, when judging fish locomotion (Jarodzka et al., 2010).", "probabilities": [0.8503636121749878, 0.03544739633798599, 0.026157433167099953, 0.0880315825343132]}, {"string": "When looking at websites, there is evidence for similarity in (preferred) scan patterns for some participants (Josephson and Holmes, 2002).", "probabilities": [0.8504548668861389, 0.03542419150471687, 0.02613231912255287, 0.08798848092556]}, {"string": "This paper extends [31] in different respects.", "probabilities": [0.521206796169281, 0.3257220387458801, 0.0537765733897686, 0.0992944985628128]}, {"string": "This paper extends and revises the work presented in [19].", "probabilities": [0.05698457732796669, 0.9152438640594482, 0.01462637074291706, 0.013145241886377335]}, {"string": "As put by Woolridge [27], BDI architectures are primarily focused on practical reasoning, i.e. the process of deciding, step by step, which action to perform to reach a goal.", "probabilities": [0.8408868908882141, 0.04027058184146881, 0.02814331091940403, 0.09069917351007462]}, {"string": "Algorithms and implementation details are provided in [74].", "probabilities": [0.2218598872423172, 0.07305661588907242, 0.4817947745323181, 0.22328871488571167]}, {"string": "A full account of the Dialogs features and the corresponding algorithms is available in [74].", "probabilities": [0.2919260561466217, 0.07524638622999191, 0.4020968973636627, 0.23073063790798187]}, {"string": "Using a set of  anchors  (pairs of related terms from the ontologies specified as related), Anchor-PROMPT extends these relations to analyze the non-local context by traversing the paths between those anchors, returning potential candidates for similarity, and computing cumulative similarity scores for the terms involved [53] (as contrasted to a description of the similarities and differences of the ontologies themselves, as addressed by CAIS).", "probabilities": [0.7279476523399353, 0.01994788460433483, 0.03947843611240387, 0.212626114487648]}, {"string": "Accordingly, the audience for information has expanded to include, among others, patients and policy makers [5].", "probabilities": [0.811458170413971, 0.0457986518740654, 0.03743740916252136, 0.10530570149421692]}, {"string": "  Third, to expand the acronyms and abbreviations not defined in the abbreviations section, the software for abbreviation definition recognition presented in [48] is used.", "probabilities": [0.1649133563041687, 0.023831166326999664, 0.4104447066783905, 0.40081077814102173]}, {"string": "Aronson and Rindflesch [43] use MetaMap to expand queries with UMLS Metathesaurus concepts.", "probabilities": [0.35928475856781006, 0.018284965306520462, 0.15363603830337524, 0.4687942564487457]}, {"string": "Second, medical language, despite being highly specialised, is also highly interpretive, and it is constantly expanding [12].", "probabilities": [0.8483242988586426, 0.0359521321952343, 0.026742935180664062, 0.08898060023784637]}, {"string": "Concerning works dealing only with query expansion using multiple domain knowledge sources, the work in [49] identified unique concepts issued from several terminologies such as MeSH, Entrez Gene and ADAM for expanding the original query with the synonyms, hypernyms, hyponyms, lexical variants and implicitly related concepts.", "probabilities": [0.7099488973617554, 0.019439293071627617, 0.04182843491435051, 0.22878332436084747]}, {"string": "Similarly, authors in [6] exploited several medical knowledge sources such MeSH, Entrez gene, SNOMED, UMLS, etc. for query expansion by expanding the original user query with synonyms, abbreviations and hierarchically related terms identified using PubMed.", "probabilities": [0.46362313628196716, 0.07452729344367981, 0.2421676069498062, 0.21968191862106323]}, {"string": "The ADT [25] is the GUI interface of AutoDock4.", "probabilities": [0.20241406559944153, 0.06872610002756119, 0.5079697370529175, 0.2208900898694992]}, {"string": "Omissions in terminologies are undesirable, and locating them is one of the goals of work in terminology auditing [42].", "probabilities": [0.8504098653793335, 0.03542502596974373, 0.02614578604698181, 0.08801932632923126]}, {"string": "We used Stanford CoreNLP toolkit [31] to extract word shapes.", "probabilities": [0.08536817878484726, 0.009676232002675533, 0.35827136039733887, 0.546684205532074]}, {"string": "In artificial intelligence and in the medical domain, there is a trend of using ontologies to better evaluate the difference between discrete attributes [36].", "probabilities": [0.8401182293891907, 0.03822636976838112, 0.028987063094973564, 0.09266825765371323]}, {"string": "In this work, we converted AlexNet [21] into a fully convolutional network by transforming fully connected layers into convolution layers.", "probabilities": [0.08871619403362274, 0.010256921872496605, 0.35472118854522705, 0.5463057160377502]}, {"string": "Another approach is the extension of OWL by epistemic operators for non-monotonic features [37].", "probabilities": [0.8500277996063232, 0.035438090562820435, 0.02626780793070793, 0.08826638013124466]}, {"string": "The first phase of the project   SKA1 (Tan et al., 2015)   will consist of hundreds of dishes and hundreds of thousands of antennas, enabling the monitoring and surveying of the sky in unprecedented detail and speed, with a second phase expanding these capabilities to at least an order of magnitude.", "probabilities": [0.7466908097267151, 0.05269550159573555, 0.06546053290367126, 0.13515326380729675]}, {"string": "Another work line is presented in [35].", "probabilities": [0.6585405468940735, 0.083476722240448, 0.08337279409170151, 0.17460989952087402]}, {"string": "This work is an extension of reference [28].", "probabilities": [0.06760590523481369, 0.8974489569664001, 0.01849510706961155, 0.016449956223368645]}, {"string": "Consequently, fuzzy extension to the ER algebra [10] has been sketched.", "probabilities": [0.6368432641029358, 0.06499915570020676, 0.12256772816181183, 0.17558981478214264]}, {"string": "A preliminary version of this work was originally published as a conference paper [33].", "probabilities": [0.1789698749780655, 0.740809977054596, 0.03890477493405342, 0.04131529852747917]}, {"string": "In this section we will focus on one of the possibilities, proposed in the setting of quantifier-guided aggregation[67].", "probabilities": [0.8192427158355713, 0.04463408514857292, 0.03489818051457405, 0.10122504830360413]}, {"string": "The resulting OWA operator can be used to evaluate the degree of truth of quantifier propositions (see [67] for details).", "probabilities": [0.8452035188674927, 0.03609790280461311, 0.02779133804142475, 0.09090724587440491]}, {"string": "This paper is an extension of a previous work presented in [18].", "probabilities": [0.1309845894575119, 0.7336985468864441, 0.046408116817474365, 0.08890876919031143]}, {"string": "All of these methods mentioned above are variants of graph embedding [12].", "probabilities": [0.09851210564374924, 0.010310962796211243, 0.3401828408241272, 0.5509941577911377]}, {"string": "To evaluate to the SVM classifier, we trained the data using the LibSVM tool developed by Chang and Lin [42].", "probabilities": [0.08536690473556519, 0.009675944223999977, 0.3582725524902344, 0.5466846227645874]}, {"string": "This protocol has been the subject of analysis for some probabilistic properties [31].", "probabilities": [0.8497219681739807, 0.03513661026954651, 0.026360532268881798, 0.08878086507320404]}, {"string": "Singh in [52] extends CTL logic with modalities for social commitments, beliefs, and intentions in order to formally model the interactions between interacting parties in a MAS.", "probabilities": [0.7896490693092346, 0.06704925000667572, 0.039910025894641876, 0.10339165478944778]}, {"string": "The MOP statement described in [27] is used.", "probabilities": [0.12828952074050903, 0.01408865675330162, 0.3074423372745514, 0.5501794815063477]}, {"string": "The classification effectiveness is usually measured in terms of four parameters [36], Precision (P), Recall (R), F1 (as combination of P and R using Eq. (6)), and False Positive error rate (FP).", "probabilities": [0.11566893011331558, 0.01551651768386364, 0.4096549451351166, 0.45915958285331726]}, {"string": "We use micro-averaging equations (see Eqs. (7) and (8)), because they are more precise than macro-averaging equations [36].", "probabilities": [0.08543799817562103, 0.009679609909653664, 0.3581687808036804, 0.546713650226593]}, {"string": "This work was later expanded in Lughofer et al. [48], where the authors propose new elimination techniques for rule redundancies, including merging of rules.", "probabilities": [0.8220880627632141, 0.048857685178518295, 0.03258787468075752, 0.09646637737751007]}, {"string": "To resolve this problem, the trust score induced ordered weighted operator (TS-IOWA) operator is proposed, which extends the induced ordered weighted averaging (IOWA) operator proposed by Yager [60] being  :{1,  , n} {1,  , n} a permutation such that u (i) u (i+1),  i=1,  , n 1, i.e.,  u (i), p (i)  is the 2-tuple with u (i) the ith highest value in the set {u1,  , un}.", "probabilities": [0.14250358939170837, 0.025321779772639275, 0.46588510274887085, 0.3662894368171692]}, {"string": "In 1988, Yager introduced an aggregation technique based on the order weighted averaging (OWA) scheme [58].", "probabilities": [0.7164770364761353, 0.019429733976721764, 0.04025857895612717, 0.2238345891237259]}, {"string": "In [59] being Q the Basic Unit-interval Monotone (BUM) membership function (non-decreasing Q:[0, 1] [0, 1] such that Q(0)=0, Q(1)=1) of the linguistic quantifier, S(h)= l=1hs (l),sl the importance degree of criterion l, and   the permutation used to produce the ordering of the values to be aggregated.", "probabilities": [0.7250171303749084, 0.04381411895155907, 0.07662047445774078, 0.15454822778701782]}, {"string": "For SVM, the LibSVM library provided the functionality [18], while the Neural Net Framework (NNF, http://nnf.sourceforge.net) was used for the NN model.", "probabilities": [0.5044568777084351, 0.04139645770192146, 0.09526021033525467, 0.3588864505290985]}, {"string": "For laterally expandable SOMs, one of the earliest work is the incremental grid growing proposed by Blackmore and Miikkulainen [21].", "probabilities": [0.8495352268218994, 0.03466099873185158, 0.026486050337553024, 0.08931771665811539]}, {"string": "A preliminary version of this work was presented earlier [25].", "probabilities": [0.2938520908355713, 0.6193574666976929, 0.035766761749982834, 0.051023636013269424]}, {"string": "An extension of current semantic Web languages, for instance OWL 2 [53], is used to represent such information.", "probabilities": [0.26896482706069946, 0.018763156607747078, 0.2241036742925644, 0.4881684184074402]}, {"string": "This conforms to the observation in [18].", "probabilities": [0.6974855065345764, 0.14427855610847473, 0.04687731713056564, 0.1113586351275444]}, {"string": "The SVM classifier was used based on default parameter values with a usual nonlinear kernel provided in the LIBSVM software package [25].", "probabilities": [0.08543369174003601, 0.009684190154075623, 0.35819050669670105, 0.5466915369033813]}, {"string": "Our approach extends the classification method that was developed in [11] by embedding the evolving Classifier eT2Class, which employs type-2 fuzzy sets instead of type-1 to better model uncertainty in news article.", "probabilities": [0.1433965414762497, 0.7160034775733948, 0.05119452252984047, 0.08940541744232178]}, {"string": "The rule consequent is controlled by the Chebyshev polynomial [28], expanding the degree of freedom of the Takagi Sugeno Kang (TSK) rule consequent.", "probabilities": [0.6202462315559387, 0.023409437388181686, 0.07153334468603134, 0.28481099009513855]}, {"string": "It is becoming more and more important for manufacturing partners to work together to expand their manufacturing capacity so as to quickly satisfy customer s diverse and personalized needs [2].", "probabilities": [0.7010462880134583, 0.07600050419569016, 0.07636220008134842, 0.1465909630060196]}, {"string": "The algorithms used in our experiments were developed on Python with the Scikit-Learn [29] library, except the LLR classifier.5 Multinomial Na ve Bayes (MNB): is a probabilistic classifier based on Bayes  theorem.", "probabilities": [0.08539250493049622, 0.009677484631538391, 0.3582358658313751, 0.5466942191123962]}, {"string": "Basically, the nodes with high local neighborhood density are identified first and then expand from such nodes to get the densely connected regions, i.e., network modules [93].", "probabilities": [0.3904167413711548, 0.077394999563694, 0.30423063039779663, 0.22795765101909637]}, {"string": "This paper extends our preliminary work [8] which considers single-label learning problem in CC.", "probabilities": [0.05572199448943138, 0.9169647097587585, 0.014422728680074215, 0.012890598736703396]}, {"string": "The HGP-LVM is an extension of the original Gaussian Process Latent Variable Model (GP-LVM) (Lawrence, 2004).", "probabilities": [0.09747487306594849, 0.010764678940176964, 0.34318679571151733, 0.5485736131668091]}, {"string": "Besides, the evidenced influence of the syntactic roles has been characterized for English (Wiemer-Hastings, 2004) and Spanish (this work).", "probabilities": [0.8500816822052002, 0.03554433956742287, 0.026229606941342354, 0.08814424276351929]}, {"string": "This is in accordance with recent neurolinguistic findings (Malaia & Newman, 2014) and contradicts the generative grammar theories.", "probabilities": [0.7009004950523376, 0.12489728629589081, 0.05331435799598694, 0.12088780105113983]}, {"string": "In narrative, this has been deeply studied (Goldman, Graesser, & van den Broek, 1999).", "probabilities": [0.8497827053070068, 0.0356924794614315, 0.026292284950613976, 0.08823249489068985]}, {"string": "IDA is functionally conscious (Franklin, 2003).", "probabilities": [0.8310443162918091, 0.029814409092068672, 0.03250861167907715, 0.10663264244794846]}, {"string": "It extends a prior work, published in [4], which realized a basic idea to experiment with static service composition according to the inter-relationship of services.", "probabilities": [0.05536317825317383, 0.9174385070800781, 0.014373128302395344, 0.012825126759707928]}, {"string": "Recent work [9] extends this approach to noisy data by using an adaptive threshold to cull Delaunay balls that arise due to noise.", "probabilities": [0.7298604846000671, 0.11979802697896957, 0.04589444398880005, 0.10444701462984085]}, {"string": "The radial distortion technique [30] expands the FOV in AR by distorting the whole scene into a fisheye view using a 3D-reconstructed model that even includes the off-screen area.", "probabilities": [0.10534580796957016, 0.012865603901445866, 0.35130488872528076, 0.5304837226867676]}, {"string": "In a sense, our method extends the work of Shen et al. [13], where they synthesize general textures, by segmenting a sample texture to its main components.", "probabilities": [0.6038108468055725, 0.086150161921978, 0.05950438976287842, 0.25053462386131287]}, {"string": "For example, the Bubble Cursor [93] employs a sphere-like selection tool that automatically expands to reach the object closest to its center.", "probabilities": [0.7574039101600647, 0.021978937089443207, 0.037248510867357254, 0.18336868286132812]}, {"string": "This article describes our HITPROTO haptic data visualization tool (HDV) and it extends work presented in the haptic symposium conference [2].", "probabilities": [0.06025426834821701, 0.9095783233642578, 0.015944000333547592, 0.014223463833332062]}, {"string": "The proposed approach is based on the affinity aggregation spectral clustering (AASC) [12], which extends the spectral clustering to the setting where multiple affinities matrices are available.", "probabilities": [0.08930999785661697, 0.010152284987270832, 0.35416433215141296, 0.5463734269142151]}, {"string": "Our system extends the technique proposed in [13] to represent and manipulate cloth simulation data as well.", "probabilities": [0.6253578662872314, 0.07176200300455093, 0.05575244873762131, 0.2471277266740799]}, {"string": "The approach proposed in [13] extends these latter editing approaches by preserving the fine time-varying details during the manipulation process, which increases the quality of the final result (Fig. 1).", "probabilities": [0.5153207182884216, 0.2738662660121918, 0.09015115350484848, 0.12066192179918289]}, {"string": "EventFlow [27] extends Lifelines2 with an overview display, events with duration, and more advanced queries.", "probabilities": [0.31541478633880615, 0.24083110690116882, 0.26753684878349304, 0.17621728777885437]}, {"string": "In this paper, we extend our previous work [4] with additional features of our system and examine their capabilities with several expanded examples in Section 4.2.", "probabilities": [0.1392551213502884, 0.7138944864273071, 0.049897562712430954, 0.09695274382829666]}, {"string": "To enhance the representation ability on human pose space, we train a SCAPE model [1] based on an expanded pose training data combined with multiple pose database, which is described in Section 4.2.", "probabilities": [0.08542810380458832, 0.009686517529189587, 0.35820716619491577, 0.5466782450675964]}, {"string": "The first and second databases are from CMU [11].", "probabilities": [0.20269615948200226, 0.0690513476729393, 0.5076138973236084, 0.22063864767551422]}, {"string": "In this paper, isosurfaces are defined as follows [23].", "probabilities": [0.4341660737991333, 0.0757436603307724, 0.266998827457428, 0.22309143841266632]}, {"string": "This paper represents an extended journal version of the CAe 2014 paper by Semmo and D llner [17].", "probabilities": [0.20943598449230194, 0.07595358788967133, 0.4949207603931427, 0.21968960762023926]}, {"string": "In terms of application, we want to expand the technique so that it can be used with different data sets that have a strong visual appeal, such as texture generation and dance choreography [42].", "probabilities": [0.4925119876861572, 0.026952538639307022, 0.09794887155294418, 0.3825865089893341]}, {"string": "Based on this idea, a version of this paper has been published in Graphics Interface 2015 [27].", "probabilities": [0.6784253716468811, 0.07237520068883896, 0.09418860077857971, 0.15501092374324799]}, {"string": "Further research can be found in paper [4], where they introduce assembly-based modeling which is a promising approach to broadening the accessibility of 3D modeling.", "probabilities": [0.7915258407592773, 0.04034481570124626, 0.04585440829396248, 0.1222749873995781]}, {"string": "Starting in the 1990s, it gained in interest with the spread of acquisition devices and reconstruction techniques [12].", "probabilities": [0.8500858545303345, 0.03554116189479828, 0.02622876688838005, 0.0881442129611969]}, {"string": "[38] is method for aerial images based segmentation on images descriptors and an energy minimization on a conditional random field.", "probabilities": [0.4886344373226166, 0.12148499488830566, 0.0899423286318779, 0.29993829131126404]}, {"string": "The encoder part of SegNet is initialized with the VGG16 weights [35].", "probabilities": [0.08539405465126038, 0.009677576832473278, 0.35823854804039, 0.5466898083686829]}, {"string": "On man-made terrain and buildings, we place second with a comparable score as [2] and Harris Net.", "probabilities": [0.20165494084358215, 0.066973015666008, 0.5089336037635803, 0.22243840992450714]}, {"string": "In [2], the authors use a random forest classifier trained on multi-scale 3D features taking into account both surface and context properties.", "probabilities": [0.08614041656255722, 0.00980744045227766, 0.3575100004673004, 0.5465421676635742]}, {"string": "We mainly experiment on the Semantic 3D dataset [40] (http://semantic3d.net).", "probabilities": [0.08539356291294098, 0.009679129347205162, 0.3582395017147064, 0.5466878414154053]}, {"string": "A very short (3 layers) residual network [37] is added at the end of the two SegNet.", "probabilities": [0.20233632624149323, 0.06917776167392731, 0.5079800486564636, 0.22050586342811584]}, {"string": "DeePr3SS [39] is also a multi-view approach, very similar to ours but later, with multi-stream fully convolutional networks for 2D classification of virtual views rendered by Gaussian point splattering.", "probabilities": [0.6668184399604797, 0.20297515392303467, 0.04025808721780777, 0.08994829654693604]}, {"string": "For example, in [30], a deep framework is used to compute a metric for identifying architectural style distance between to building models.", "probabilities": [0.7580597400665283, 0.04927524924278259, 0.056627314537763596, 0.13603761792182922]}, {"string": "For this purpose, [27] proposed to use local, multiscale 3D tensors around the actual 3D points.", "probabilities": [0.7311077117919922, 0.08082900196313858, 0.060569338500499725, 0.12749403715133667]}, {"string": "  TU-Berlin-Class [2] (training stages 1 3) for sketch classification comprising 250 categories of sketches, 80 per category, crowd-sourced from 1350 different non-expert participants with diverse drawing styles;", "probabilities": [0.2501259446144104, 0.07239753007888794, 0.44886013865470886, 0.2286163717508316]}, {"string": "Inspired from curriculum learning [39], we trained our model by giving it multiple learning tasks, one-by-one with increasing difficulties.", "probabilities": [0.13642163574695587, 0.7202945351600647, 0.04872109368443489, 0.09456273913383484]}, {"string": "Further gains in compactness could be explored e.g. via product quantization as [31] but such optimizations are beyond the scope of this paper.", "probabilities": [0.6992336511611938, 0.06976613402366638, 0.07856615632772446, 0.15243412554264069]}, {"string": "Next, we evaluated over Saavedra-SBIR (using mAP) and TU-Berlin-Retr (using Tb proposed in [15]).", "probabilities": [0.08536429703235626, 0.00967536773532629, 0.3582749664783478, 0.5466854572296143]}, {"string": "This early wave of SBIR systems was complemented in the late nineties by algorithms accepting line-art sketches, more closely resembling the free-hand sketches casually generated by lay users in the act of sketching a throw-away query [12].", "probabilities": [0.6524682641029358, 0.024532461538910866, 0.05473795160651207, 0.2682613730430603]}, {"string": "Qi et al. [20] implemented an alternative edge detection pre-process delivering a performance gain in cluttered scenes.", "probabilities": [0.6609587669372559, 0.09032580256462097, 0.05262618884444237, 0.1960892379283905]}, {"string": "  TU-Berlin-Retr [15] (testing) takes into account not only the category of the retrieved images but also the relative order of the relevant images.", "probabilities": [0.6891381144523621, 0.05924419313669205, 0.09373439103364944, 0.15788336098194122]}, {"string": "Saliency was first determined for each voxel, and was then enhanced by center-surround operations between voxels inspired by the standard cognitive saliency model [4].", "probabilities": [0.08775211125612259, 0.009804944507777691, 0.35485178232192993, 0.5475911498069763]}, {"string": "For instance, Lee et al. [44] introduced point and class saliency measures to quantify the color saliency of a single data point or a class of data points in a categorical map visualization.", "probabilities": [0.8504474759101868, 0.03543703258037567, 0.026131363585591316, 0.08798419684171677]}, {"string": "We compared the data of this study with eye tracking data of the memorability experiment [2] with conditions closer to natural image viewing.", "probabilities": [0.597048282623291, 0.10508884489536285, 0.11359035968780518, 0.18427255749702454]}, {"string": "For comparing task-based visual analysis to more exploratory analysis, we additionally analyzed the eye tracking data of the memorability experiment [2] (Mem-task).", "probabilities": [0.08797988295555115, 0.009879513643682003, 0.3547254502773285, 0.5474151968955994]}, {"string": "Our results show that despite having improved bottom-up saliency models for information visualization, like DVS [13], the influence of bottom-up visual saliency is drastically reduced during task-based visual analysis.", "probabilities": [0.5865975022315979, 0.05792674049735069, 0.12906724214553833, 0.22640849649906158]}, {"string": "For their eye tracking experiments, Goldberg and Helfman [54] defined AOIs for three sequential steps required to retrieve values in linear or radial graphs:  find dimension ,  find associated datapoint ,  get datapoint value .", "probabilities": [0.7184839844703674, 0.019230535253882408, 0.039841994643211365, 0.22244353592395782]}, {"string": "The participants of Borkin et al. s [2] experiment were  recruited from the local communities of Cambridge and Boston , but no information about their visualization literacy is provided.", "probabilities": [0.24427056312561035, 0.07522335648536682, 0.4542880654335022, 0.22621801495552063]}, {"string": "To test the influence of a target area s visual saliency on visual search performance, we computed the correlation between the task-dependent AOI saliency (using the model by Itti et al. [4]) and its FF for each of the three low-level analytical tasks.", "probabilities": [0.08540693670511246, 0.009677687659859657, 0.35821297764778137, 0.5467023253440857]}, {"string": "This tendency is reflected in the selected descriptions of visualization content of users in Borkin et al. s [2] memorability experiment (supplemental material).", "probabilities": [0.8491388559341431, 0.035737209022045135, 0.026514971628785133, 0.0886090099811554]}, {"string": "However, for the seminal saliency model by Itti et al. [4], the fixation-saliency similarities are equally low for the low-level analytical tasks as for the memorability task.", "probabilities": [0.8484982848167419, 0.03562023490667343, 0.02658909372985363, 0.08929236233234406]}, {"string": "Eye-tracking data of the original memorability experiment [2] can be found at http://massvis.mit.edu/.", "probabilities": [0.2021547108888626, 0.06898177415132523, 0.5083181858062744, 0.22054532170295715]}, {"string": "In order to keep the same viewing conditions as in the original memorability experiment [2], the task description that would affect participants  scanning sequence, was not displayed in this step of the experiment.", "probabilities": [0.29203417897224426, 0.01702982559800148, 0.18595390021800995, 0.504982054233551]}, {"string": "Harel et al. [14] also followed this approach.", "probabilities": [0.7995025515556335, 0.04102914780378342, 0.041653089225292206, 0.1178152859210968]}, {"string": "Goferman et al. [19] proposed saliency detection based on patches with unique low-level features, visual organization rules according to which regions close to salient pixels are also salient and high-level factors, such as human faces.", "probabilities": [0.8492039442062378, 0.03597129136323929, 0.026426689699292183, 0.08839809149503708]}, {"string": "Zhang et al. [18] proposed a Bayesian framework that incorporates top-down information dependent on the target s features with bottom-up saliency that is represented as the self-information derived from the statistics of natural images.", "probabilities": [0.8425807356834412, 0.039732277393341064, 0.027745747938752174, 0.08994116634130478]}, {"string": "The 3D shape context (Frome et al., 2004) extends the concept of the shape context to 3D point clouds and offers a solution for constructing local 3D reference frames.", "probabilities": [0.7387596368789673, 0.05248921737074852, 0.06956493854522705, 0.1391863077878952]}, {"string": "To identify the quantifier for the MCDA decision rule, we employed one of the most frequently used methods for defining a parameterized subset on the unit interval (Yager, 1996", "probabilities": [0.08550680428743362, 0.009683137759566307, 0.3580681383609772, 0.5467418432235718]}, {"string": "In this setting, function Q: I  I where I=[0,1] in such a way that Q(0)=0, Q(1)=1 and Q( x)  Q( y) if x>y, corresponds to a fuzzy subset representation of the linguistic quantifier Q. Based on the function, the OWA operator is guided by Q. As a result, the order weights are defined as follows (Yager, 1996", "probabilities": [0.08768344670534134, 0.009901026263833046, 0.35573428869247437, 0.54668128490448]}, {"string": "Yager (1988) proposed OWA as a parameterized family of combination operators.", "probabilities": [0.8504658341407776, 0.03542052581906319, 0.026128562167286873, 0.0879850909113884]}, {"string": "By changing the parameter,  , one can generate different types of quantifiers and their associated operators between the two extreme cases of the   at least one  and   all  linguistic quantifiers (Yager, 1996).", "probabilities": [0.8254993557929993, 0.039611365646123886, 0.03435633331537247, 0.10053295642137527]}, {"string": "This family uniformly extends the classical RT family [1] by associating a semiring value to the basic role definition.", "probabilities": [0.6354370713233948, 0.06737520545721054, 0.12236505001783371, 0.17482267320156097]}, {"string": "This extension is called the property-oriented concept lattice [4].", "probabilities": [0.20640693604946136, 0.06945931911468506, 0.5027755498886108, 0.22135822474956512]}, {"string": "Very shortly after that work had been submitted (in March 1918), Felix Hausdorff expanded the definition of  dimension  significantly for the evolution of the definition of fractals, allowing sets to have non-integer dimensions [8].", "probabilities": [0.8214837312698364, 0.043779194355010986, 0.03430723026394844, 0.10042990744113922]}, {"string": "Garau, on an extension of GeoPDEs to include adaptivity based on hierarchical splines [9].", "probabilities": [0.5362259745597839, 0.1820899099111557, 0.1276899129152298, 0.1539941430091858]}, {"string": "Level of personal contentment was measured by an additive scale of three items extracted from the Satisfaction with Life Scale developed by Diener, Emmons, Larsen, and Griffin (1985).", "probabilities": [0.08625347912311554, 0.00976550579071045, 0.35712379217147827, 0.5468572974205017]}, {"string": "The personality traits were measured using part of the 10-Item Personality Inventory (Gosling et al., 2003).", "probabilities": [0.085380420088768, 0.009676200337707996, 0.3582513630390167, 0.5466920733451843]}, {"string": "Therefore, it can be used as a proxy for the longer Big-Five instruments (Gosling et al., 2003).", "probabilities": [0.6906986236572266, 0.07803841680288315, 0.08071781694889069, 0.1505451649427414]}, {"string": "Socialization is one of the modes for transferring individual s knowledge and for expanding organizational knowledge (Nonaka, 1994).", "probabilities": [0.8330357074737549, 0.03998499736189842, 0.031018460169434547, 0.09596081078052521]}, {"string": "This paper is a significantly extended version of the paper which was presented at the 1st World Summit on the Knowledge Society at Athens, Greece in 2008 (Lee, Kim, & Hackney, 2008).", "probabilities": [0.20955482125282288, 0.07397646456956863, 0.4960477650165558, 0.2204209715127945]}, {"string": "We used the life satisfaction scale developed by Diener, Emmons, Larsen, and Griffin (1985), which contains items such as,  The conditions of my life today are excellent,  and  So far, I have gotten the important things I want in life.  Respondents in our sample reported a moderately high level of life satisfaction (see Fig. A4; Range 1 5, M = 3.48, SD = 0.77,   = 0.85).", "probabilities": [0.08543187379837036, 0.00968569703400135, 0.358198344707489, 0.5466840267181396]}, {"string": "Trust also involves a  calculative process  (Doney & Cannon, 1997, p. 37) related to the value people receive from their relationships.", "probabilities": [0.8504778742790222, 0.03541731461882591, 0.026126204058527946, 0.08797861635684967]}, {"string": "The scale was developed for use with an online store (Hsieh, Chiu, & Chiang, 2005).", "probabilities": [0.4571617841720581, 0.03047720529139042, 0.11377925425767899, 0.3985818028450012]}, {"string": "The items were constructed from the definition of brand use practices given by Schau et al. (2009).", "probabilities": [0.08582901209592819, 0.009728411212563515, 0.3582836985588074, 0.5461588501930237]}, {"string": "It is derived from the definition given by Schau et al. (2009).", "probabilities": [0.2084806114435196, 0.6705410480499268, 0.06206117197871208, 0.05891713500022888]}, {"string": "For example, we mentioned that the effects of value creation practices evolve over time (Schau et al., 2009); however; we do not know how these effects act over time and how they develop.", "probabilities": [0.8492807149887085, 0.035715252161026, 0.026470409706234932, 0.08853365480899811]}, {"string": "After a review of the sociology literature, Muniz and O Guinn (2001) identify three core components or markers of a community; shared consciousness of kind, shared rituals and traditions, and moral responsibility or obligations to society.", "probabilities": [0.8503524661064148, 0.03551742061972618, 0.02614266239106655, 0.0879875123500824]}, {"string": "Brand trust is  the willingness of the average consumer to rely on the ability of the brand to perform its stated function  (Chaudhuri & Holbrook, 2001, p. 82).", "probabilities": [0.8504930734634399, 0.03541307896375656, 0.02612212300300598, 0.0879717543721199]}, {"string": "The Ten Item Personality Inventory (TIPI), a brief 10-item measure of the Big Five Personality domains (Gosling, Rentfrow, & Swann, 2003), was administered to assess personality.", "probabilities": [0.41094493865966797, 0.019140155985951424, 0.1320594847202301, 0.43785545229911804]}, {"string": "For example, consider calling, texting, Facebook, e-mail, sending photos, gaming, surfing the Internet, watching videos, and all other uses driven by  apps  and software  (Lepp et al., 2013).", "probabilities": [0.7309158444404602, 0.019887041300535202, 0.038754526525735855, 0.21044260263442993]}, {"string": "That is, two focus groups of undergraduate students reviewed these questions for several validity criteria including: (1) clarity in wording, (2) relevance of the items, (3) use of standard English, (4) absence of biased words and phrases, (5) formatting of items, and (6) clarity of the instructions (Fowler, 2002).", "probabilities": [0.44512492418289185, 0.07544797658920288, 0.25790807604789734, 0.22151900827884674]}, {"string": "Because the ability to accurately read emotion in the facial expression of others is one of the most important nonverbal communication skills, we used the Faces subtests of the second edition of the Diagnostic Analysis of Nonverbal Behavior (DANVA2) (Nowicki & Carton, 1993).", "probabilities": [0.08543292433023453, 0.009679166600108147, 0.35817551612854004, 0.546712338924408]}, {"string": "The forward Digit Span (Wechsler, 2004) a subset of the Wechsler Intelligence Scale for children, was administered as a distracter task between the CASP and the DANVA2.", "probabilities": [0.11111368983983994, 0.010864870622754097, 0.3246099352836609, 0.5534115433692932]}, {"string": "For both dependent variables, we ran univariate analyses of covariance, the preferred method of analysis for this design (Dimitrov & Rumrill, 2003), using gender, ethnicity, and age, as well as a composite variable called media-use sum (i.e., sum of time spent watching television, playing video games, using cell phones, and using computers) as covariates, in order to control for demographics and prior media use.", "probabilities": [0.08541180193424225, 0.009679713286459446, 0.3582116663455963, 0.5466967821121216]}, {"string": "A 5-item personality inventory adapted from Gosling, Rentfrow, and Swann (2003)4 was used to measure the big-five personality traits, extroversion, agreeableness, conscientiousness, neuroticism, and openness to experiences.", "probabilities": [0.08717634528875351, 0.009788062423467636, 0.3557141125202179, 0.5473214983940125]}, {"string": "Turkeys (Walker et al., 2011) boxplot was used to visualise the variables in order to investigate the spread of data and highlight any outliers.", "probabilities": [0.7206776142120361, 0.01919052191078663, 0.03936377912759781, 0.2207680642604828]}, {"string": "The 10-item instrument has been found to be a reasonable substitute for the longer Big-Five inventory when research conditions means that a short measure is the only way (Gosling, Rentfrow, & Swann, 2003).", "probabilities": [0.5867099761962891, 0.04596342146396637, 0.07838784158229828, 0.2889387607574463]}, {"string": "These datasets were taken from the UCI machine learning repository (Asuncion & Newman, 2007), from which full documentation for all datasets can be obtained.", "probabilities": [0.08536925911903381, 0.009675675071775913, 0.3582679033279419, 0.546687126159668]}, {"string": "The context is represented by means of the well-known key-value model (Adomavicius, Sankaranarayanan, Sen, & Tuzhilin, 2005) using as dimensions some of the different feature spaces related to items.", "probabilities": [0.16962160170078278, 0.013100607320666313, 0.26693588495254517, 0.5503419041633606]}, {"string": "Mobile Phone Involvement Questionnaire (MPIQ): In addition to the NMP-Q, the 8-item MPIQ (Walsh et al., 2010) was administered.", "probabilities": [0.08550623059272766, 0.00968586839735508, 0.3580784499645233, 0.5467294454574585]}, {"string": "To determine how homogeneous the items in the NMP-Q were (DeVellis, 2003), internal consistency reliability was examined using Cronbach s alpha as the internal consistency reliability coefficient.", "probabilities": [0.08545061945915222, 0.009682837873697281, 0.35815879702568054, 0.5467077493667603]}, {"string": "The degree of the correlation between the scores provided evidence of similarity between the NMP-Q and MPIQ (DeVellis, 2003); and thus, served as a means of checking for construct validity of the NMP-Q.", "probabilities": [0.712868332862854, 0.1456996500492096, 0.04273789003491402, 0.09869414567947388]}, {"string": "With the proliferation of inexpensive mobile devices, we are now living in a mobile age in which mobile ICTs are vigorously and quickly adopted (Oulasvirta, Rattenbury, Ma, & Raita, 2012).", "probabilities": [0.7154474258422852, 0.07202202826738358, 0.07073821127414703, 0.14179235696792603]}, {"string": "The presence of simple structure supports the adequacy of rotation (Tabachnick & Fidell, 2013).", "probabilities": [0.8501309752464294, 0.03551346808671951, 0.02622379921376705, 0.08813163638114929]}, {"string": "For this purpose, the 8-item Mobile Phone Involvement Questionnaire (MPIQ) developed by Walsh, White, and Young (2010) was administered together with the NMP-Q. The MPIQ was just used for purposes of analysis and is not part of the NMP-Q.", "probabilities": [0.0996260866522789, 0.010426485911011696, 0.33888888359069824, 0.5510585904121399]}, {"string": "Through thematic clustering, these horizons were grouped into meaning units (Creswell, 2012).", "probabilities": [0.8354818820953369, 0.03787580877542496, 0.030970076099038124, 0.09567229449748993]}, {"string": "These transcriptions were analyzed following the phenomenological data analysis steps as described by Moustakas (1994).", "probabilities": [0.08536422997713089, 0.009675380773842335, 0.3582751750946045, 0.5466851592063904]}, {"string": "The Nomophobia Questionnaire and Mobile Phone Involvement Questionnaire (Walsh et al., 2010) were employed in the present study.", "probabilities": [0.0865667462348938, 0.009736857376992702, 0.35653162002563477, 0.5471647381782532]}, {"string": "They were all above the commonly accepted minimum value of .7 (Nunnally, 1978), suggesting that they demonstrate good internal consistency.", "probabilities": [0.8401328325271606, 0.038531139492988586, 0.028912290930747986, 0.09242375940084457]}, {"string": "As for the adequacy of sampling, the KMO index was .941, which is greater than the minimum acceptable value of .60 (Tabachnick & Fidell, 2013).", "probabilities": [0.686760425567627, 0.12285030633211136, 0.048113830387592316, 0.14227546751499176]}, {"string": " Negative Affectivity  was taken from Agho (1992).", "probabilities": [0.7257928848266602, 0.020051605999469757, 0.03975885733962059, 0.2143966406583786]}, {"string": "As expected, a two-factor structure emerged from the EFA with ML method using the GEOMIN oblique rotation (Asparouhov & Muth n, 2009).", "probabilities": [0.08946115523576736, 0.010166352614760399, 0.3532429039478302, 0.5471295714378357]}, {"string": "Therefore, the risk associated with breaches of confidentiality and the possibility that data may be accessed by a third person should be considered by the researcher and the institutional review board when planning research (Kraut et al., 2004).", "probabilities": [0.6999378204345703, 0.07618582248687744, 0.07684406638145447, 0.1470322608947754]}, {"string": "Hart-Davidson, McLeod, Klerkx and Wojcik (2010) identify 3 main challenges (1) inadequate workflow and data models for capturing the review activity, (2) lack of quality indicators and measures and (3) building a profile for the assessors.", "probabilities": [0.8504374027252197, 0.03543151170015335, 0.026136772707104683, 0.0879942923784256]}, {"string": "However, in networked hypermedia environments that particularly emphasize the multiperspectivity of a knowledge domain, namely in MHEs (cf. Lima et al., 2002), it might not be sufficient to review task-relevant content in one systematic sequence because MHEs are not primarily designed to convey isolated factual knowledge in a specific order.", "probabilities": [0.840986967086792, 0.038279447704553604, 0.028655480593442917, 0.09207811206579208]}, {"string": "According to the consultancy firm Catvertiser (2014), Facebook population of Europe is almost 300 million.", "probabilities": [0.8019850254058838, 0.04384610801935196, 0.04276129603385925, 0.11140754818916321]}, {"string": "Well-being has been evaluated using the Satisfaction With Life Scale (Diener, Emmons, Larsen, & Griffin, 1985).", "probabilities": [0.6704548597335815, 0.020423857495188713, 0.05079186335206032, 0.2583293616771698]}, {"string": "The PSS, developed by (Cohen, Kamarck, & Mermelstein, 1983), measures the perception of stress, i.e., the degree to which situations are appraised as stressful, by asking respondents to rate the frequency of their thoughts and feelings related to situations occurred in the last month (Cronbach''s alpha coefficient = .79).", "probabilities": [0.7119648456573486, 0.035154253244400024, 0.04479781910777092, 0.2080831229686737]}, {"string": "We employed the SAS-SV in this study for its strong internal consistency, for which Cronbach''s alpha coefficient was .91 (Kwon et al., 2013a).", "probabilities": [0.08538244664669037, 0.009678088128566742, 0.35825419425964355, 0.5466852784156799]}, {"string": "Furthermore, while many studies in the field used Kimberly Young''s Internet Addiction Test (Young, 1998), in our study we employed the Smartphone Addiction Test   Short Version.", "probabilities": [0.29912763833999634, 0.07225792109966278, 0.39475497603416443, 0.23385949432849884]}, {"string": "The SAS-SV, developed by (Kwon, Kim, Cho, & Yang, 2013a)), looks at smartphone usage to identify the level of risk for smartphone addiction, but does not diagnose addiction.", "probabilities": [0.8391206860542297, 0.034523267298936844, 0.029911352321505547, 0.09644471108913422]}, {"string": "Mull and Lee (2014) found that  creative projects  were one of the reasons people use Pinterest   however, only for searching for and sharing the ideas for a do-it-yourself project.", "probabilities": [0.8504127860069275, 0.03544790670275688, 0.026140734553337097, 0.08799856156110764]}, {"string": "Because of the important advantages of a short and easy to administer measurement instrument, such as the possibility to incorporate the scale into space-limited surveys, and in agreement with the findings by Lemmens et al. (2015), the ultimate aim of the present study was to develop and validate a short 9-item scale to measure SMD.", "probabilities": [0.7019282579421997, 0.12359949201345444, 0.05334818363189697, 0.12112393975257874]}, {"string": "The degree of self-esteem was measured using the six-item self-esteem scale (Rosenberg, Schooler, & Schoenbach, 1989).", "probabilities": [0.09126924723386765, 0.010004464536905289, 0.34998172521591187, 0.5487444996833801]}, {"string": "Compulsive Internet use was assessed in the first sample using the 14-item Compulsive Internet Use Scale (Meerkerk et al., 2009).", "probabilities": [0.11927640438079834, 0.011202874593436718, 0.31530192494392395, 0.5542187690734863]}, {"string": "We used structural equation modeling (SEM) with weighted least squares estimators to test these second-order factor models using CFA in MPlus (Asparouhov & Muth n, 2009).", "probabilities": [0.0853634849190712, 0.009675410576164722, 0.3582763671875, 0.546684741973877]}, {"string": "The Halo Effect can wield a powerful influence on the impressions we form of others (Smith et al., 2010).", "probabilities": [0.8445302844047546, 0.03678695112466812, 0.02784722112119198, 0.09083550423383713]}, {"string": "This is due to images being broadcasted to every follower who could, potentially, re-post the images on their own pages, thus expanding the visibility to an even wider audience (Scott, 2011).", "probabilities": [0.8495946526527405, 0.03562963008880615, 0.0263750571757555, 0.08840064704418182]}, {"string": "Studies on social media advertising and online advertising also state that consumers'' attitudes towards social media advertising is an essential determinant of its effectiveness (Chen, Fay, & Wang, 2011).", "probabilities": [0.8504545092582703, 0.03542530536651611, 0.02613213285803795, 0.0879880040884018]}, {"string": "Where the endorsed products are perceived as false and invalid, consumers develop a negative attitude towards the brand and also the celebrity endorser (Cheung, Luo, Sia, & Chen, 2009).", "probabilities": [0.8504683375358582, 0.03541925549507141, 0.02612924948334694, 0.08798320591449738]}, {"string": "Such sampling technique provides good sources of data in exploratory research (Easterby-Smith, Thorpe, & Jackson, 2015).", "probabilities": [0.8503860235214233, 0.035270314663648605, 0.02616823837161064, 0.08817548304796219]}, {"string": "PROMIS is a National Institutes of Health Roadmap initiative whose aim is to provide precise, valid, reliable, and standardized questionnaires measuring patient reported outcomes across the domains of physical, mental, and social health (Cella et al., 2010).", "probabilities": [0.8279227614402771, 0.040268465876579285, 0.03302595391869545, 0.0987827479839325]}, {"string": "We then classified a  severe  group based on both the distribution of the data and the clinical cut-off for depression recommended by the American Psychiatry Association (APA) (American Psychiatric Association, 2013).", "probabilities": [0.3324839174747467, 0.01794416457414627, 0.16571898758411407, 0.48385295271873474]}, {"string": "We assessed anxiety symptoms using the 4-item PROMIS anxiety scale (Pilkonis et al., 2011).", "probabilities": [0.08540070056915283, 0.009679680690169334, 0.3582296669483185, 0.5466899871826172]}, {"string": "We then classified a severe group with a raw score of 9 or more (out of 20), which corresponds to a T-score of 57.7 (Johnston et al., 2016).", "probabilities": [0.6266469955444336, 0.07068578153848648, 0.05557020381093025, 0.24709707498550415]}, {"string": "In other words, the whole process of implementing gamification plays a crucial role, as emphasized by Werbach (2014).", "probabilities": [0.7313281893730164, 0.06812341511249542, 0.06463344395160675, 0.13591495156288147]}, {"string": "Inspired by the intrinsic motivation inventory (cf. Ryan, Mims, & Koestner, 1983), we included scales to assess psychological need satisfaction in the areas of competence, autonomy in regard to freedom of decision, autonomy in regard to task meaningfulness, and social relatedness.", "probabilities": [0.4290933310985565, 0.029519185423851013, 0.12165075540542603, 0.41973677277565]}, {"string": "So to ensure that participants were actually aware of all game design elements relevant to the corresponding condition, we conducted a treatment check (or manipulation check, Pedhazur & Schmelkin, 1991) for each element.", "probabilities": [0.08663873374462128, 0.00975099578499794, 0.35638001561164856, 0.5472302436828613]}, {"string": "The only context excluded by definition is the use of game design elements either within the games themselves or in the game design process (Deterding, Dixon, et al., 2011).", "probabilities": [0.8503109812736511, 0.03511777147650719, 0.026208335533738136, 0.08836284279823303]}, {"string": "Badges and leaderboards assess a series of player actions and in doing so provide cumulative feedback (cf. Rigby & Ryan, 2011).", "probabilities": [0.8504924178123474, 0.035413481295108795, 0.026122208684682846, 0.08797179162502289]}, {"string": "(2)The term elements allows us to distinguish gamification from serious games (Deterding, Dixon, et al., 2011).", "probabilities": [0.8500673174858093, 0.03552938625216484, 0.026240283623337746, 0.08816296607255936]}, {"string": "To assess mediation, we used a procedure recommended by Preacher, Zyphur, and Zhang (2010) for testing mediation in a multilevel data structure.", "probabilities": [0.08541114628314972, 0.009684070013463497, 0.35822680592536926, 0.5466779470443726]}, {"string": "Third, as advocated by Bolger and Laurenceau (2013), to account for potential confounding between the within-person and the between-person levels of analysis, we included between-person averages (e.g., the average sexual desire aggregated across all 42-diary days) for all of the primary variables.", "probabilities": [0.6367555856704712, 0.0996432974934578, 0.10210583359003067, 0.16149523854255676]}, {"string": "Typical examples of benign circumscribed masses are cysts and fibroadenomas [3].", "probabilities": [0.8459147214889526, 0.03621794283390045, 0.027555294334888458, 0.09031208604574203]}, {"string": "We used the Caffe deep learning toolkit [46] in order to efficiently use the processing power of the Tesla graphics card for computation of convolutional neural network parameters.", "probabilities": [0.08539316803216934, 0.009680680930614471, 0.3582451045513153, 0.5466811060905457]}, {"string": "We ve made an experiment on the AlexNet [30].", "probabilities": [0.08548314869403839, 0.009693654254078865, 0.35814064741134644, 0.5466825366020203]}, {"string": "Given a training image, we apply Simple Linear Iterative Clustering (SLIC) [45] to the green channel of the fundus image.", "probabilities": [0.0853644609451294, 0.009675402194261551, 0.3582753539085388, 0.5466848015785217]}, {"string": "Both nets are implemented using Python and Caffe [53].", "probabilities": [0.08538583666086197, 0.009676474146544933, 0.3582434356212616, 0.546694278717041]}, {"string": "The network was originally trained outside of the NiftyNet platform as described in [26].", "probabilities": [0.5266720652580261, 0.05868095904588699, 0.08699614554643631, 0.32765084505081177]}, {"string": "This also enables automatic support for visualization of the network graph as a hierarchy at different levels of detail using the TensorBoard visualizer [37] as shown in Fig. 3.", "probabilities": [0.08882692456245422, 0.00985077116638422, 0.35329726338386536, 0.5480250120162964]}, {"string": "Wearable devices with solar-energy harvesting were presented to ease the implementation of an autonomous wireless body area network (WBAN) [31].", "probabilities": [0.81002277135849, 0.04025869071483612, 0.039077747613191605, 0.11064073443412781]}, {"string": "A model driven approach was presented to design and develop smart IoT-based system [29].", "probabilities": [0.5999839901924133, 0.0675143450498581, 0.06324691325426102, 0.2692546844482422]}, {"string": "An ontology-based public healthcare system for the IoT was proposed to overcome security and privacy challenges in a healthcare information system [10].", "probabilities": [0.8346018195152283, 0.03940318897366524, 0.03085608407855034, 0.09513887763023376]}, {"string": "Domain ontology and rules reasoning was proposed to construct a clinical decision support system for patients undergoing surgery [1].", "probabilities": [0.743930459022522, 0.05787675082683563, 0.06482428312301636, 0.13336853682994843]}, {"string": "Most of the patient data are ambiguous; therefore, classic elements of a patient ontology are fuzzified using fuzzy data types and fuzzy modifiers [13].", "probabilities": [0.7212599515914917, 0.019992249086499214, 0.04082561284303665, 0.2179221659898758]}, {"string": "The patient ontology contains five fuzzy variables:  age ,  weight ,  sex ,  height  and  disease history  in the concept of  patient profile  [47].", "probabilities": [0.20839500427246094, 0.0710327997803688, 0.49934181571006775, 0.22123032808303833]}, {"string": "(1) and (2), Jx [0, 1] is a limitation that is equal to 0 A (x, ) 1 for type-1 membership functions (T1MFs), and Jx indicates primary membership ofA , where Jx [0, 1]for x   X. If x = x  then, for each value of x, we have the following [52]", "probabilities": [0.2033252716064453, 0.06936813145875931, 0.506635844707489, 0.22067075967788696]}, {"string": "A wearable sensor-based healthcare monitoring service via the Internet of Things (IoT) is more effective for long-term medical care, and for clinical access to extract precise physiological information about patients and for disease management [7].", "probabilities": [0.7931020855903625, 0.05084392800927162, 0.04320230707526207, 0.11285175383090973]}, {"string": "However, the existing healthcare systems are inefficient at extracting accurate membership values for physical parameters of the patient''s body, because most of the healthcare systems use conventional technologies and approaches, such as risk score calculators, a classic ontology, and fuzzy logic [8].", "probabilities": [0.8345749974250793, 0.04705981910228729, 0.02846863865852356, 0.08989652246236801]}, {"string": "Dietary control and precise diabetes drug recommendations are another problem for the existing IoT-based healthcare monitoring architecture [9].", "probabilities": [0.8479065299034119, 0.03606625273823738, 0.026839790865778923, 0.0891873762011528]}, {"string": "Agreement between the systems and the experts was measured by Cohen''s Kappa coefficient equation, as follows [13] where Pr (a) denotes the actual observed agreement, and Pr (e) denotes chance agreement.", "probabilities": [0.08549882471561432, 0.009685209020972252, 0.358087956905365, 0.5467279553413391]}, {"string": "These sensors (as input variables) and the fuzzy linguistic data types and MFs are described as follows.1.Blood pressure: The pressure exerted by circulating blood on the walls of blood vessels is called blood pressure [57].", "probabilities": [0.10415060073137283, 0.011956047266721725, 0.36909937858581543, 0.5147939920425415]}, {"string": "A classic ontology with a rules-based system is presented for diabetes management to enhance accuracy [47].", "probabilities": [0.6404510140419006, 0.2376990169286728, 0.0384971909224987, 0.08335280418395996]}, {"string": "A well-known method is used to evaluate an ontology in the form of questions and answers, such as the DL query and SPARQL query [65].", "probabilities": [0.7105044722557068, 0.02170415408909321, 0.0413263738155365, 0.22646503150463104]}, {"string": "Lofti Zadeh introduced the type-1 fuzzy set in 1965 to extract vague and blurred concepts [48].", "probabilities": [0.8479933738708496, 0.03585493564605713, 0.026900872588157654, 0.08925080299377441]}, {"string": "An ontology was used to present the design of a diet assessment system [47].", "probabilities": [0.5675909519195557, 0.020411118865013123, 0.07822747528553009, 0.3337705433368683]}, {"string": "An actor-profile ontology was presented for organizational knowledge in home-care assistance [40].", "probabilities": [0.7255437970161438, 0.05739187076687813, 0.07449832558631897, 0.1425660252571106]}, {"string": "An ontological case based engineering methodology was presented for diabetes management [39].", "probabilities": [0.6372823119163513, 0.05461985617876053, 0.05466412752866745, 0.2534337043762207]}, {"string": "A semantic interoperability model for big-data in IoT (SIMB-IoT) was presented to recommend medicine for various symptoms collected from different sensors [42].", "probabilities": [0.7195234894752502, 0.019486984238028526, 0.039599597454071045, 0.22138996422290802]}, {"string": "These metrics are used to evaluate the performance of prediction methods used in the experiments [66].", "probabilities": [0.2755776345729828, 0.023707479238510132, 0.19479519128799438, 0.5059196949005127]}, {"string": "(13) and (14) (Harris Benedict) are used to calculate the BMR for males and females, respectively [63].", "probabilities": [0.604426383972168, 0.02093856781721115, 0.06775805354118347, 0.30687686800956726]}, {"string": "A fuzzy approach was used in an IoT-based healthcare monitoring system to find abnormal conditions in a patient [27].", "probabilities": [0.7175894379615784, 0.019413411617279053, 0.040012434124946594, 0.22298474609851837]}, {"string": "There are four types for range of motion (ROM), which can be used to examine the patient''s body movement: passive ROM, active-assisted ROM, active ROM, and active self assisted ROM [58].", "probabilities": [0.8060203194618225, 0.051884595304727554, 0.03626161068677902, 0.10583346337080002]}, {"string": "The Mamdani inference method is used for heart disease diagnosis [22].", "probabilities": [0.18302248418331146, 0.013957235962152481, 0.2557927668094635, 0.5472275614738464]}, {"string": "A system to capture physiological data of a person under care using IoT sensors was presented by Chiang and Liang [8].", "probabilities": [0.6268948316574097, 0.07053881883621216, 0.05550007522106171, 0.24706631898880005]}, {"string": "A fuzzy logic based method for fault-tolerant wireless sensor networks was proposed to monitor diagnosis and testing systems in a large-scale context [26].", "probabilities": [0.720913827419281, 0.019218258559703827, 0.03932950645685196, 0.22053839266300201]}, {"string": "A computational intelligence model based on fuzzy logic was proposed to handle ambiguity and vagueness in clinical knowledge and data [25].", "probabilities": [0.6295759677886963, 0.06731264293193817, 0.05522671714425087, 0.24788472056388855]}, {"string": "An analysis of heart rate variability signals is computed using fuzzy measure entropy (FuzzyMEn) [23].", "probabilities": [0.08538375794887543, 0.009676376357674599, 0.35824650526046753, 0.5466933846473694]}, {"string": "The diabetes drug and food information is gathered from the Internet and research articles, and classified manually under the supervision of domain experts [54].", "probabilities": [0.28240472078323364, 0.0747438594698906, 0.41240742802619934, 0.23044392466545105]}, {"string": "Authors in [46] proposed a method called PIN-TRUST to measure the trustworthiness of each user for a target user by employing belief propagation (BP).", "probabilities": [0.6306712627410889, 0.020295536145567894, 0.06130937114357948, 0.28772374987602234]}, {"string": "Kim and Song [7] studied the impact of two factors: the length of trust paths and aggregation functions, on the trust inference accuracy and proposed four strategies for estimating trust based on reinforcement learning.", "probabilities": [0.8084452152252197, 0.02689722552895546, 0.03311694413423538, 0.13154056668281555]}, {"string": "In our previous work on trust propagation [9], we proposed a heuristic trust propagation algorithm based on learning automata, called DLATrust, as well as a new aggregation strategy.", "probabilities": [0.19264446198940277, 0.5451403260231018, 0.09721402078866959, 0.16500124335289001]}, {"string": "Work in [3] presented a trust propagation model which includes a landmark based method with a pre-processing.", "probabilities": [0.6984588503837585, 0.03230908513069153, 0.04988892748951912, 0.2193431407213211]}, {"string": "In the context of a social web, Golbeck [11] has defined trust in a person as  a commitment to an action based on a belief that the future actions of that person will lead to a good outcome .", "probabilities": [0.8498982787132263, 0.035843681544065475, 0.02621208317577839, 0.08804596215486526]}, {"string": "Another model without trust propagation process was proposed by work in [28].", "probabilities": [0.6311333775520325, 0.07892023772001266, 0.05566798150539398, 0.23427832126617432]}, {"string": "In order to validate the effectiveness of our algorithm DyTrust, we conducted extensive experiments with the real trust network dataset, Kaitiaki and compared the results of DyTrust with those of the well-known trust propagation algorithms, as well as with those of our previous algorithms on trust propagation [9].", "probabilities": [0.7005006074905396, 0.1251617968082428, 0.05333767086267471, 0.12099999189376831]}, {"string": "For this purpose, we use the aggregation function proposed in our previous work [9], called MCFAvg, as given below.", "probabilities": [0.13266108930110931, 0.7290554046630859, 0.04713425040245056, 0.09114917367696762]}, {"string": "The value of the reward parameter a for each automaton Aj along the path is determined based on Eq. (12)[53].", "probabilities": [0.08540849387645721, 0.00968118105083704, 0.35822945833206177, 0.5466808676719666]}, {"string": "ProofThis theorem has been proved in [48] .", "probabilities": [0.8478516936302185, 0.035521119832992554, 0.026839828118681908, 0.08978747576475143]}, {"string": "Authors in [44] analyzed the vulnerabilities of EigenTrust and proposed the trust model ServiceTrust which has a better performance on some sophisticated attack models by utilizing pairwise feedback similarity weighted trust propagation into the trust model.", "probabilities": [0.5450464487075806, 0.024532359093427658, 0.09336043149232864, 0.33706074953079224]}, {"string": "Authors in [31] proposed a decision support method for estimating trust in virtual teams.", "probabilities": [0.8450853824615479, 0.036116234958171844, 0.02765864133834839, 0.09113974869251251]}, {"string": "Another famous model based on trust propagation is MoleTrust [41], which finds all shortest trust paths from a source to a target user and combines all direct trust values issued by users whose trustworthiness is more than 0.6.", "probabilities": [0.8121007680892944, 0.030648987740278244, 0.03376896306872368, 0.12348128110170364]}, {"string": "We also consider two algorithms proposed in our previous work on trust propagation [9], namely Min-MCFAvg and DLATrust, for the performance comparison.", "probabilities": [0.13196295499801636, 0.7302176356315613, 0.047051284462213516, 0.09076813608407974]}, {"string": "The VLFeat package (Vedaldi and Fulkerson, 2008) is used for efficient K-means clustering. Learning details.", "probabilities": [0.6140851974487305, 0.02016025222837925, 0.06520257145166397, 0.3005518913269043]}, {"string": "Concerning low computational cost and validity, we make the images composed of superpixels with the simple linear iterative clustering (SLIC) method in CIELAB color space (Achanta et al., 2012).", "probabilities": [0.08721236139535904, 0.009774012491106987, 0.35575878620147705, 0.5472548007965088]}, {"string": "Our aim is to facilitate the agricultural computer vision domain with the benefits of state-of-the-art machine learning, e.g. the supervised hierarchical feature representation learning and the performance increase that comes with large datasets (LeCun et al., 2015).", "probabilities": [0.8016392588615417, 0.04835888743400574, 0.04051201045513153, 0.1094898208975792]}, {"string": "Furthermore, the camera was angled 20 degrees upwards as previous research suggested this would reduce occlusions (Hemming et al., 2014).", "probabilities": [0.7174977660179138, 0.01939479261636734, 0.04002925753593445, 0.22307820618152618]}, {"string": "Without access to large datasets, domains such as agriculture previously used traditional computer vision methods using manual feature crafting (Bol n-Canedo et al., 2013) whilst capturing a limited subset of the variability.", "probabilities": [0.7217351198196411, 0.019168509170413017, 0.03913526609539986, 0.21996112167835236]}, {"string": "To model the plant and create object meshes, the commercial software PlantFactory 2015 Studio (EON Software, 2016) on OSX 10.10 was used.", "probabilities": [0.08725474029779434, 0.009770622476935387, 0.3555333614349365, 0.547441303730011]}, {"string": "Instead we looked at: (i) label set distributions, (ii) label image position distributions, (iii) part color spectra and (iv) illumination intensity distributions to gain a more quantitative insight into the similarity between sets.iWithin a dataset, label frequencies are often highly unbalanced (Caesar et al., 2015), resulting in neglected classes in some type of computer vision approaches.", "probabilities": [0.2608998119831085, 0.07414361834526062, 0.43627873063087463, 0.22867782413959503]}, {"string": "In recent years the need of robotisation in agriculture has been growing notably to keep up with the increasing demand of productivity and quality of food production whilst decreasing the pressure on resources required (Bac et al., 2014).", "probabilities": [0.7153164148330688, 0.0724041685461998, 0.07073158770799637, 0.14154785871505737]}, {"string": "Although collecting image data can be automated (van der Heijden et al., 2012), it remains required and time consuming to manually annotate.", "probabilities": [0.8500863313674927, 0.03560977429151535, 0.026210496202111244, 0.08809341490268707]}, {"string": "Specifically we used the Deeplab VGG-16 Vanilla model (Papandreou et al., 2015) with a receptive field of 128 pixels and a stride of 8 pixels.", "probabilities": [0.08537515997886658, 0.009675994515419006, 0.3582592010498047, 0.5466895699501038]}, {"string": "The resolution in depth of this ground truth is coarse, though an exact representation could be obtained if needed by exporting the scene s Z-buffer in Blender to the OpenEXR (Kainz et al., 2004) linear format.", "probabilities": [0.1867281198501587, 0.01378048025071621, 0.25534749031066895, 0.5441438555717468]}, {"string": "Therefore the synthetic image labels were post-processed in the commercial image processing software package Halcon 12 (MVTech, 2016) by removing any interpolated color pixels and replacing them with the most frequent neighbour color in a 3 3 patch If this convolution failed in case the majority of the neighbouring pixels also had been interpolated, the window was enlarged until all pixels were equal to one of the class labels.", "probabilities": [0.0932455062866211, 0.010064005851745605, 0.34716349840164185, 0.5495269298553467]}, {"string": "With the advent of state-of-the-art machine learning methods for computer vision, most notably convolutional neural networks for image classification and segmentation, the training dataset size requirement has been further increased (Najafabadi et al., 2015).", "probabilities": [0.850419819355011, 0.03541988506913185, 0.026144059374928474, 0.08801620453596115]}, {"string": "One approach is to only label images on a high level using a single class or a few keywords per image in order to classify an image globally or give a shortlist of objects in the image (Everingham et al., 2015).", "probabilities": [0.8468198776245117, 0.03604760393500328, 0.027274368330836296, 0.08985813707113266]}, {"string": "Furthermore, recently a method was created for automatic model based synthetic dataset generation for crop and weeds detection on a per-pixel level (Cicco et al., 2016), though no plant parts could be differentiated.", "probabilities": [0.6553807854652405, 0.020044010132551193, 0.05452953279018402, 0.2700456976890564]}, {"string": "A second approach is to weakly label the data with bounding boxes around objects or their parts (Papandreou et al., 2015).", "probabilities": [0.829250693321228, 0.04411614313721657, 0.03146615996956825, 0.09516703337430954]}, {"string": "The Faster R-CNN stated that the whole system can finish proposal and detection in 0.2 s using a deep VGG-16 model (Simonyan and Zisserman, 2014) to generate high-quality object proposals.", "probabilities": [0.7212978601455688, 0.019220277667045593, 0.03922363370656967, 0.2202582210302353]}, {"string": "Assessing the behavior of livestock is important as it can indicate their welfare status, well-being state and social interactions (Berckmans, 2014).", "probabilities": [0.8504831790924072, 0.03541569784283638, 0.026124833151698112, 0.08797637373209]}, {"string": "Compared to the hand design features, skill and expertise in designing feature extractors could be avoided by a deep learning procedure with the advantage of learning good features automatically (LeCun et al., 2015).", "probabilities": [0.7131543159484863, 0.07260789722204208, 0.07163522392511368, 0.1426026076078415]}, {"string": "Recent studies have shown that deep learning methods would be widely adopted by the animal behavior community (Valletta et al., 2017).", "probabilities": [0.8493739366531372, 0.03566176816821098, 0.02643289789557457, 0.08853136003017426]}, {"string": "Recent progress of object detection and recognition, which achieved a top-5 error of 4.49% (He et al., 2016), has been very close to human-level performance.", "probabilities": [0.6824398636817932, 0.18613280355930328, 0.04001055285334587, 0.09141682088375092]}, {"string": "SIFT features were computed using the VL-SIFT library version 0.9.17 [36].", "probabilities": [0.08536465466022491, 0.009675414301455021, 0.3582744598388672, 0.5466854572296143]}, {"string": "There are many superpixel generated algorithms and we used SLIC [37], a simple line iterative clustering algorithm, to generate superpixels.", "probabilities": [0.08559318631887436, 0.009692130610346794, 0.3579569458961487, 0.546757698059082]}, {"string": "This function has a fixed form defined in the  CIFAR10 Quick  example provided with the software Caffe [32].", "probabilities": [0.08641985058784485, 0.00973013136535883, 0.3567424416542053, 0.5471075773239136]}, {"string": "The algorithms are used off-the-shelf using the Python module scikit-learn [32], with parameters chosen as follows, after extensive tuning.", "probabilities": [0.08619129657745361, 0.00975650455802679, 0.3571954071521759, 0.5468567609786987]}, {"string": "In our experiments VLFeat [106] library has been used to extract SIFT keypoints.", "probabilities": [0.1339874565601349, 0.026080749928951263, 0.3104111850261688, 0.5295206308364868]}, {"string": "The four broadly tuned color tunnels are then defined as [27]", "probabilities": [0.20213253796100616, 0.06899245828390121, 0.5083499550819397, 0.22052505612373352]}, {"string": "As a baseline, we also present the results using the popular Support Vector Machines (SVMs) with radial basis function kernel [14], in which the optimal values for the parameters C and   are adopted.", "probabilities": [0.08541575074195862, 0.009683352895081043, 0.3582260310649872, 0.5466747879981995]}, {"string": "Yamamoto et al. (2012) delivered a set of experiments targeting image-processing tasks using MapReduce like creating a grayscale image from an original-colored image utilizing Ruby and octave languages for database of video shots upon Hadoop [19]. C.", "probabilities": [0.7194502949714661, 0.019620031118392944, 0.04038568586111069, 0.22054395079612732]}, {"string": "In other words, we can consider the 0.5-cut set to be the  egg , according to the  egg yolk  model (Cohn & Gotts, 1996).", "probabilities": [0.7341648936271667, 0.02015753462910652, 0.03869098052382469, 0.20698659121990204]}, {"string": "We thus define the relationships that follow Behr and Schneider (2001) to be  direct topological relationships , and the relationships based on areal approximations to be  deduced topological relationships .", "probabilities": [0.20231199264526367, 0.0689735934138298, 0.5081197619438171, 0.22059470415115356]}, {"string": "Both process scenarios were implemented with WPS instances using the 52 North WPS framework (North Geoprocessing Community, 2007) running at both locations.", "probabilities": [0.08537809550762177, 0.009676092304289341, 0.3582548499107361, 0.5466909408569336]}, {"string": "Additionally, we wrapped the schema transformation process (Step 2, Fig. 8) (Schema Transformation Service), as a Web Map Service (WMS) (based on GeoServer application server, GeoServer, 2008) by using a special GeoServer concept for wrapping resources called data stores.", "probabilities": [0.08545378595590591, 0.009680848568677902, 0.35814711451530457, 0.546718180179596]}, {"string": "As a consequence, it does not currently make the necessary allowances for a wider depiction of reality (Smith & Mark, 2001).", "probabilities": [0.8504794836044312, 0.03541918471455574, 0.026125255972146988, 0.08797618001699448]}, {"string": "For example, we could extend the geo-atom of Goodchild et al. (2007) with the addition of a URI:  x, Z, z(x), URI .", "probabilities": [0.255831778049469, 0.07589279860258102, 0.4396260678768158, 0.22864939272403717]}, {"string": "Interoperability lays out a foundation for easily integrating heterogeneous components in a plug-and-play fashion or for mashup with minor scripting of the interfaces (Bambacus et al., 2007).", "probabilities": [0.7527334690093994, 0.08412878960371017, 0.04904009774327278, 0.11409757286310196]}, {"string": "To answer complex questions, we must effectively utilize facilities, instruments, and other resources to pursue fundamental and transformational questions, unravel newly revealed mysteries, and expand our understanding of the Earth and the universe (NSF., 2009).", "probabilities": [0.2825845181941986, 0.05086062476038933, 0.36738696694374084, 0.2991679906845093]}, {"string": "Correspondingly, we adopt from (Frank, 2006) the five-step distance with five symbols: very close (VC), close (Cl), medium (Me), far (Fa), and very far (VF).", "probabilities": [0.08585673570632935, 0.009753023274242878, 0.35917574167251587, 0.5452145338058472]}, {"string": "We consider areas A 1,   , A n as a partition of a larger area A if (1) A 1       A n = A and (2) A i   A j = 0 for all i   j from (1,   , n) (Stuckenschmidt & Harmelen, 2004).", "probabilities": [0.19884426891803741, 0.06434720754623413, 0.5088101625442505, 0.227998286485672]}, {"string": "The frame rate is maintained at 25 fps and the resolution at 720   576 pixels (Schierl, Wiegand, & Kampmann, 2005).", "probabilities": [0.29282987117767334, 0.08993035554885864, 0.392209529876709, 0.22503024339675903]}, {"string": "In two workshop rounds, 12 and 6 stakeholders participated, respectively, and 15 interviews were conducted (Meyer et al., 2012).", "probabilities": [0.6924677491188049, 0.07661557197570801, 0.08080483973026276, 0.1501118391752243]}, {"string": "We refer to the SWEET ontologies using the prefix SWEET and to the MONITOR ontology by Kollarits and Wergles (2008) using the prefix MONITOR.", "probabilities": [0.08575326949357986, 0.009698011912405491, 0.3577162027359009, 0.5468325614929199]}, {"string": "Participatory means were also used to collect and review further terms and concepts as part of the RISK MAP project (Meyer et al., 2012).", "probabilities": [0.08588448911905289, 0.00970921479165554, 0.35756707191467285, 0.54683917760849]}, {"string": "The  OGC Abstract Specification Topic 2: Spatial Referencing by Coordinates  extends the classes under the SRS, such as Datum and Projection, to declare the classification (Babitski et al., 2009).", "probabilities": [0.2027762532234192, 0.06906105577945709, 0.5075079202651978, 0.22065474092960358]}, {"string": "The  OGC Simple Feature Access Part 1: Common Architecture  provides a categorization of common geospatial geometries that can be expanded into classes under the Geometry class (OGC, 2006a).", "probabilities": [0.8423990607261658, 0.036798618733882904, 0.0286899134516716, 0.09211238473653793]}, {"string": "Massive land use and land cover changes initially take place within already existing build-up areas; then they expand outwards in the adjacent suburban region and the urban fringe (Wiethoff & Baier, 2009).", "probabilities": [0.7142775058746338, 0.07228697836399078, 0.07121215760707855, 0.1422232985496521]}, {"string": "For the implementation, we have used Scikit-learn, an open-source Python module for machine learning (Pedregosa et al., 2011).", "probabilities": [0.0853772908449173, 0.009676162153482437, 0.3582558035850525, 0.5466907024383545]}, {"string": "SpanishADR [26] is the first Spanish corpus annotated with drugs and effects by two annotators expert in the field; it consists of 400 user messages collected from ForumCl nic.", "probabilities": [0.39857959747314453, 0.11312613636255264, 0.27618637681007385, 0.21210792660713196]}, {"string": "Rese and Baier [91] highlight how TRIZ is seldom used in innovation networks, as well as its exploitation has resulted in several unsuccessful experiences.", "probabilities": [0.8504343032836914, 0.035419121384620667, 0.02613803558051586, 0.08800859749317169]}, {"string": "For instance, while the indexing of Web of Science resulted too restrictive, thus excluding supposedly relevant peer-reviewed material, the employment of Google Scholar would have led to difficulties in identifying the fundamental literature due to overabundance of data [80], likely populated by a large number of divulgations characterized by loose review processes.", "probabilities": [0.8259084820747375, 0.04228660836815834, 0.03305233269929886, 0.09875254333019257]}, {"string": "The present paper extends Armando and Ponta (2010).", "probabilities": [0.6390654444694519, 0.24189722537994385, 0.037325821816921234, 0.08171152323484421]}, {"string": "Each aggregator can use two different averaging operators: an order-weighted averaging (Yager, 1988) or simple weighted averaging.", "probabilities": [0.08553821593523026, 0.009685753844678402, 0.35803139209747314, 0.5467446446418762]}, {"string": "In our implementation we will use Lib-SVM implementation (Chang and Lin, 2011) which is fast and also supports multiclass classification.", "probabilities": [0.14389601349830627, 0.013510252349078655, 0.2876828610897064, 0.5549108982086182]}, {"string": "This paper is an extension of our previous work (Bopche and Mehtre, 2016).", "probabilities": [0.09516260772943497, 0.8475682139396667, 0.029240433126688004, 0.028028782457113266]}, {"string": "A Berendsen thermostat [38 is applied to the entire domain in order to set the temperature T=300 K.", "probabilities": [0.09931420534849167, 0.010381297208368778, 0.33924543857574463, 0.5510590076446533]}, {"string": "All MD simulations in this paper (as well as all previous publications using mdFoam+) use the well-known Velocity-Verlet [24 scheme, which is illustrated in Fig. 1 and can be described algorithmically for one MD time-step, t t+ t, as follows:", "probabilities": [0.0867687240242958, 0.009790983982384205, 0.35637685656547546, 0.547063410282135]}, {"string": "Step 1 Estimate the velocity at the mid-step for all N molecules in the system: , as discussed in Section 2.6, which handles motion of molecules across faces of the mesh (and also deals with boundaries).", "probabilities": [0.08894537389278412, 0.009901231154799461, 0.3541634678840637, 0.546989917755127]}, {"string": "To compare the efficiency of the four methods, we again follow the setup of Hong (2013).", "probabilities": [0.08582711964845657, 0.00973203219473362, 0.35770636796951294, 0.5467345118522644]}, {"string": "We report the original TAE values for the DFT-CF algorithm from Hong (2013) and denote them as DFT-CF(1), along with the TAE values computed on our own machine, which are denoted as DFT-CF(2).", "probabilities": [0.13672758638858795, 0.01467838790267706, 0.29831668734550476, 0.5502774119377136]}, {"string": "In order to evaluate the efficacy of the proposed approaches, we recreate the experiments conducted in Hong (2013), which first evaluate the accuracy, and then the speed of several algorithms for calculating the Poisson binomial distribution function.", "probabilities": [0.1699439138174057, 0.01808115467429161, 0.2666969299316406, 0.545278012752533]}, {"string": "To give a ground truth distribution function for comparison, we first generate 3 independent Binomial(ni,pi) random variables, X1,X2, and X3, as is done in Hong (2013).", "probabilities": [0.0853649377822876, 0.009675652720034122, 0.3582761287689209, 0.5466833114624023]}, {"string": "For our Fourier transform implementation we use the Fastest Fourier Transform in the West (Frigo and Johnson, 2005), which is freely available as a library in C, and is easy to use.", "probabilities": [0.08536410331726074, 0.009675420820713043, 0.358275443315506, 0.5466850399971008]}, {"string": "See Hong (2013) for further discussion and details about the RNA method.", "probabilities": [0.8305032253265381, 0.029772989451885223, 0.03084244579076767, 0.10888142138719559]}, {"string": "Since the conclusion of Hong (2013) is that the DFT-CF and refined normal approximation (RNA) are generally preferred to others, we will compare our proposed methods only to them.", "probabilities": [0.6920393109321594, 0.11394459009170532, 0.05450283735990524, 0.13951325416564941]}, {"string": "Instead of the standard hyper-tangent function f(x)=tanh(x), Rectified Linear Units (ReLUs) [18]f(x)=max(0,x) is applied to the filter responses to accelerate the training procedure.", "probabilities": [0.08693283051252365, 0.009791363030672073, 0.35636720061302185, 0.5469085574150085]}, {"string": "A SVM classifier is learnt with the LIBSVM library (Chang and Lin, 2011) using the one-against-one method with a linear kernel.", "probabilities": [0.08539917320013046, 0.009678040631115437, 0.35822683572769165, 0.5466959476470947]}, {"string": "The full process is described in Ref. [28] and illustrated in Fig. 6 .", "probabilities": [0.2901138663291931, 0.12763711810112, 0.37020576000213623, 0.21204324066638947]}, {"string": "These classes have different number of images varying from 7 to 98 images each. ETH-80 [69]: This database is available in the project COGVIS.", "probabilities": [0.20216773450374603, 0.06900366395711899, 0.5082991719245911, 0.22052937746047974]}, {"string": "For all datasets, we have extracted SIFT descriptors [34] on a dense spatial grid, with the step-size corresponding to half of the patch-size, over 8 scales separated by a factor of 1.2, and the smallest patch-size set to 16 pixels.", "probabilities": [0.08538990467786789, 0.009677331894636154, 0.3582395911216736, 0.5466932058334351]}, {"string": "Currently, ImageNet [3] contains more than 12million images and 17,624 categories.", "probabilities": [0.8485867977142334, 0.035770442336797714, 0.026709506288170815, 0.08893322944641113]}, {"string": "To verify if this is still true for large number of categories, we performed an experiment on the SUN (Scene Understanding) data set [57].", "probabilities": [0.08537378162145615, 0.009676028043031693, 0.3582615852355957, 0.5466885566711426]}, {"string": "Our SVM implementation is based on the LibSVM [84] package.", "probabilities": [0.08548647165298462, 0.009696242399513721, 0.35814356803894043, 0.5466736555099487]}, {"string": "Space time interest points were extracted and HoG-HoF (histogram of gradients and histogram of motion flow) features were extracted from the local patches [57].", "probabilities": [0.08537708967924118, 0.009676302783191204, 0.35825711488723755, 0.546689510345459]}, {"string": "We use the function vl_phow provided by the vl_feat library [40] with default settings.", "probabilities": [0.08536329120397568, 0.009675378911197186, 0.3582766354084015, 0.546684741973877]}, {"string": "In this experiment we use the VLFeat library [40] that includes multiple encoding methods such as BOW, LLC, Super Vectors and Fisher Vectors.", "probabilities": [0.08539734035730362, 0.009678679518401623, 0.3582318425178528, 0.5466921329498291]}, {"string": "Furthermore, it presents similar computational delay with the classic method of Itti [12].", "probabilities": [0.6431387662887573, 0.049950990825891495, 0.05352766066789627, 0.2533825635910034]}, {"string": "We evaluate our methods on the sketch dataset with the most categories to date proposed by Eitz et al. [1], which has 250 categories and 20,000 sketches (80 sketches per category).", "probabilities": [0.12645767629146576, 0.013088478706777096, 0.3092135488986969, 0.5512402653694153]}, {"string": "HOG is computed using the VLFeat [39] implementation with each patch divided into 4 4 cells and the orientation is set to 4.", "probabilities": [0.08537009358406067, 0.009676096960902214, 0.3582678437232971, 0.5466859340667725]}, {"string": "The dataset [1] we evaluate on has as many as 250 categories.", "probabilities": [0.20221085846424103, 0.06839090585708618, 0.5081834197044373, 0.22121481597423553]}, {"string": "We use [5] to train the SVM classifier.", "probabilities": [0.08538155257701874, 0.009678761474788189, 0.35825809836387634, 0.5466816425323486]}, {"string": "SVM for both descriptors was implemented using one-vs.-all LIBSVM [6] with an RBF kernel using chi-squared distance function (RBF-X2).", "probabilities": [0.08537188917398453, 0.009675784036517143, 0.3582639992237091, 0.5466883778572083]}, {"string": "We compare also with a ConvNet-based classifier [52], trained using ImageNet 2010 metadata.", "probabilities": [0.14083468914031982, 0.023113558068871498, 0.4605124294757843, 0.37553927302360535]}, {"string": "To further explore the capability of the proposed H3DF as a local pattern descriptor, we combine the H3DF with dense sampling as used in DenseSIFT [30] with an evenly dense sampling grid at multiple scales (denseH3DF).", "probabilities": [0.08537203818559647, 0.00967610813677311, 0.358264684677124, 0.546687126159668]}, {"string": "We employ the Libsvm [60] package integrated with OpenCV for SVR-based AU intensity estimation.", "probabilities": [0.0853634923696518, 0.009675439447164536, 0.35827648639678955, 0.5466845631599426]}, {"string": "The LibSVM [34] was used to learn the SVM classifiers.", "probabilities": [0.08547616004943848, 0.009686630219221115, 0.35812944173812866, 0.546707808971405]}, {"string": "Furthermore, we collected a set of random images by sampling 20,000 images from the ImageNet data set [63] to evaluate our method on the task of self-taught image classification.", "probabilities": [0.08541916310787201, 0.009680830873548985, 0.35820338129997253, 0.5466966032981873]}, {"string": "In Section 5.3 we also investigate the use of features extracted from a CNN [67] in combination with the previous ones.", "probabilities": [0.09501867741346359, 0.011252757161855698, 0.34790170192718506, 0.5458268523216248]}, {"string": "Following [49], the output for the last convolutional layer has been taken as the image feature representation.", "probabilities": [0.16316291689872742, 0.0365753173828125, 0.4729107916355133, 0.32735100388526917]}, {"string": "To speed up the process, we compute the measurement in (8) based on superpixels obtained by the simple linear iterative clustering (SLIC) superpixel generation method [32] (with the desired number of superpixels set as 300).", "probabilities": [0.08592395484447479, 0.009706547483801842, 0.35746699571609497, 0.5469024777412415]}, {"string": "For a video, we extract a 4096-dimensional representation using a pre-trained AlexNet (Krizhevsky et al., 2012).", "probabilities": [0.08536599576473236, 0.009675554931163788, 0.3582741618156433, 0.5466842651367188]}, {"string": "We run our algorithm within the matCaffe (Caffe toolbox with the Matlab interface) framework (Jia et al., 2014).", "probabilities": [0.0853658989071846, 0.009675722569227219, 0.3582735061645508, 0.5466848611831665]}, {"string": "To extract these, the captions are processed using the Stanford CoreNLP toolkit Manning et al. (2014).", "probabilities": [0.08536261320114136, 0.009675268083810806, 0.3582773506641388, 0.546684741973877]}, {"string": "The basis kernels can be linear kernels, radial basis function (RBF) kernels and polynomial kernels, etc. In our study, we use a linear kernel for each type of features and adopt the grid search with LIBSVM (Chang and Lin, 2011) to learn the kernel weights  1,  2 and coefficients  .", "probabilities": [0.08536583930253983, 0.009675614535808563, 0.35827329754829407, 0.5466852784156799]}, {"string": "In our experiments we choose Alex net (Krizhevsky et al., 2012) as a starting point and then try to increase network s depth with a modified structure.", "probabilities": [0.09541814029216766, 0.010275368578732014, 0.34381628036499023, 0.5504902005195618]}, {"string": "We use zero-padding (referred to as Identity Projections by He et al. (2016)) to match the number of layers in order to perform element-wise addition when we have to connect layers with different number of output channels by a residual connection.", "probabilities": [0.0853758454322815, 0.009676146320998669, 0.35826078057289124, 0.5466871857643127]}, {"string": "We initialize all layers with the MSRA method for initialization as described by He et al. (2015).", "probabilities": [0.08577146381139755, 0.009717478416860104, 0.35774701833724976, 0.546764075756073]}, {"string": "Because deep CNN-based features are extracted from the network, which is trained for recognition tasks, we can regard it as a feature that expresses discriminative information of an image, in our tests, we use the Caffe implementation (Jia et al., 2014).", "probabilities": [0.08542671799659729, 0.009681248106062412, 0.35819098353385925, 0.5467011332511902]}, {"string": "We extract features from the sixth layer of the network which has the same architecture as that proposed by Krizhevsky et al. (2012) and won ILSVRC2012.", "probabilities": [0.1010020449757576, 0.010887970216572285, 0.33828991651535034, 0.5498200058937073]}, {"string": "In our approach we made use of parametric rectified linear units (He et al., 2015) (PReLU) as our activation functions.", "probabilities": [0.5113646984100342, 0.03396901115775108, 0.09181592613458633, 0.3628503084182739]}, {"string": "In this context we initialise the network parameters using MSRA (He et al., 2015) as it is an appropriate choice when employing PReLU activation units.", "probabilities": [0.08716940134763718, 0.009787499904632568, 0.3555542230606079, 0.5474889278411865]}, {"string": "We used a CNN (Krizhevsky et al., 2012) for the classification itself.", "probabilities": [0.08536612242460251, 0.009675686247646809, 0.3582729995250702, 0.5466852188110352]}, {"string": "For the head identification step, we rely on the implementation of Collin s algorithm from Stanford CoreNLP (Manning et al., 2014).", "probabilities": [0.08537402749061584, 0.009677352383732796, 0.3582654297351837, 0.546683132648468]}, {"string": "During test, the labels are obtained by classifying each bounding box using ResNet (He et al., 2016).", "probabilities": [0.08537528663873672, 0.009676030836999416, 0.3582592010498047, 0.546689510345459]}, {"string": "We train the CNN for 10,000 epochs using stochastic gradient descent (SGD) implemented in PyCaffe (Jia et al., 2014), step-decreasing the learning rate at epoch 6000, 8000 and 9000.", "probabilities": [0.08538723737001419, 0.009677017107605934, 0.3582431375980377, 0.5466926097869873]}, {"string": "MSR-VTT (Xu et al., 2016).", "probabilities": [0.1628732979297638, 0.026241203770041466, 0.4413670599460602, 0.369518518447876]}, {"string": "We also make small improvements over DeepLab-CRF (Chen et al., 2015) in the case of PASCAL-50S.", "probabilities": [0.5095981955528259, 0.06047595664858818, 0.1854993999004364, 0.24442648887634277]}, {"string": "We first utilize the simple linear iterative clustering (SLIC) algorithm (Achanta et al., 2012) to obtain superpixels on the haze image for our proposed dehazing method.", "probabilities": [0.08537162095308304, 0.009675928391516209, 0.3582648038864136, 0.546687662601471]}, {"string": "The work presented here is an extension of a preliminary version of UFOme described in [40].", "probabilities": [0.13064515590667725, 0.7370642423629761, 0.04609574005007744, 0.08619479835033417]}, {"string": "For the sake of brevity, for LibSVM we just reported the values of the most significant tuned parameters, while we omit the ones that, after tuning, are set to their standard value reported in the referenced paper [31].", "probabilities": [0.318855881690979, 0.10088435560464859, 0.3569512963294983, 0.2233085185289383]}, {"string": "Euclidean Distance is used as the similarity function for these features and the weights of the distance function are determined by either a constant weighting (CW) of each feature or by dynamically adapting the weights using the Ordered Weighting Averaging (OWA) method [52].", "probabilities": [0.08538942784070969, 0.009677180089056492, 0.3582398593425751, 0.5466935038566589]}, {"string": "The Ordered Weighted Average (OWA) method [52] allows dynamic weighting of the features by ordering the descriptor distance values.", "probabilities": [0.0869361162185669, 0.009788427501916885, 0.3563840985298157, 0.5468913912773132]}, {"string": "This paper extends [23] with the following additional contributions:1We introduce five new mapping functions i.e.,Ou^,Og^,Od^,Ot^,Oc^ for graph pattern Po in Section 3.2.", "probabilities": [0.21707683801651, 0.09178327769041061, 0.47527384757995605, 0.21586604416370392]}, {"string": "In the follow-up study reported here (which extends our initial report in [14]), we manage to assemble a much larger benchmark of 54 OntoUML models.", "probabilities": [0.13203279674053192, 0.7312187552452087, 0.04674534127116203, 0.0900031104683876]}, {"string": "This paper extends our prior research [11], where we first studied the problem of rewriting ontology-based mappings.", "probabilities": [0.06089628487825394, 0.9089648723602295, 0.01563720963895321, 0.01450168713927269]}, {"string": "We use Stanford PTBTokenizer [30] in this work.", "probabilities": [0.08536403626203537, 0.009675445966422558, 0.3582756221294403, 0.5466849207878113]}, {"string": "This paper extends our research work introduced by [11] that suggests a new fine-grained parallel/distributed ETL approach for Big Data, consisting of a set of MR-based ETL functionalities.", "probabilities": [0.05582340061664581, 0.9167294502258301, 0.0145009970292449, 0.012946044094860554]}, {"string": "Also for the character n-gram feature representation, the classification is performed by a Support Vector Machine (SVM) with a linear kernel (Chang and Lin, 2011).", "probabilities": [0.0855320394039154, 0.009687886573374271, 0.3580425977706909, 0.5467374920845032]}, {"string": "We used the LibSVM implementation (Chang and Lin, 2011) for all experiments below.", "probabilities": [0.08536410331726074, 0.00967550277709961, 0.3582756817340851, 0.546684741973877]}, {"string": "HOG/HOF: The HOG/HOF descriptors were introduced in [53] and are similar in spirit to the well known SIFT descriptor.", "probabilities": [0.5861520767211914, 0.05219095200300217, 0.06922692060470581, 0.29243001341819763]}, {"string": "In our evaluation we used the same grid parameters used in [53] as well as their online code.1", "probabilities": [0.1166086420416832, 0.018795499578118324, 0.3266860544681549, 0.5379098057746887]}, {"string": "To be able to generate the term document matrix, we preprocess the reviews by means of tokenization, stop word filtering, stemming, n-gram generation and feature selection [36.", "probabilities": [0.08958117663860321, 0.0099460044875741, 0.3524130582809448, 0.5480597615242004]}, {"string": "In this context, we concentrate on Na ve Bayes (NB) as a rather simple learning algorithm as well as Neural Network (NN) and Support Vector Machine (SVM) representing more complex learning algorithms [62.", "probabilities": [0.2059331238269806, 0.017401020973920822, 0.23690693080425262, 0.5397589206695557]}, {"string": "To investigate the three research questions, focused on examining which service aspects are expressed in online reviews and whether a reviewer''s recommendation can be explained by sentiment on core and augmented service aspects expressed in the review (i.e., RQ1a/b), whether the review''s contents have predictive power to infer the recommendation (i.e., RQ2), and whether the type of business model impacts the explanation and prediction of service recommendations (i.e., RQ3), we adapt the structured knowledge discovery process proposed by Han and Kamber [52.", "probabilities": [0.08592963218688965, 0.009760193526744843, 0.35897624492645264, 0.5453339219093323]}, {"string": "In order to extract the sentiment expressed within a review, we leverage the Harvard General Inquirer lexicon that connects syntactic, semantic, and practical information to words (e.g., delay is tagged having a negative sentiment) [55.", "probabilities": [0.08691028505563736, 0.009759986773133278, 0.3560476303100586, 0.5472820997238159]}, {"string": "We specifically take into account the word lists for positive (pos) and negative (neg) words that are used to determine the sentiment polarity expressed within the different reviews, as shown in Eq. (1) [56.", "probabilities": [0.08657293766736984, 0.009740245528519154, 0.356585294008255, 0.5471014976501465]}, {"string": "Although Na ve Bayes classifiers are rather simple and rely on potentially unrealistic assumptions, they have nevertheless been proven to generally perform well and in fact have the advantage of requiring low computational effort and thus being more time-efficient [63.", "probabilities": [0.8451991081237793, 0.037017710506916046, 0.02753162756562233, 0.09025147557258606]}, {"string": "The output neuron uses, as input, the weighted sum of outputs from neurons in the previous layer (or input variables in case of the initial input layer), and applies the activation function to the input [52.", "probabilities": [0.08798132836818695, 0.010100364685058594, 0.36322686076164246, 0.5386914014816284]}, {"string": "Furthermore, previous research has found that 10-fold stratified cross-validation performs best for evaluating models trained with real-world datasets such as the one in this study [68.", "probabilities": [0.6697222590446472, 0.10402175039052963, 0.05326123535633087, 0.1729947179555893]}, {"string": "This evaluation procedure is advantageous as it avoids overfitting based on the notion that classifier training and classifier testing are performed on separate observations [69.", "probabilities": [0.6409587860107422, 0.0316058024764061, 0.0566166453063488, 0.2708187997341156]}, {"string": "As shown by Hsu, Chang, and Lin [67, the Radial Basis Function (RBF) represents an appropriate kernel.", "probabilities": [0.5960851311683655, 0.04453371465206146, 0.08222522586584091, 0.27715596556663513]}, {"string": "As activation function, we use the most commonly adopted sigmoid function [65.", "probabilities": [0.08536577224731445, 0.009675622917711735, 0.3582741320133209, 0.5466845035552979]}, {"string": "When a neural network is trained, the weights of the different neurons are updated so that the overall neural network''s output corresponds to the actual classification [64.", "probabilities": [0.6334846615791321, 0.23509666323661804, 0.043037500232458115, 0.08838115632534027]}, {"string": "For identifying the service aspects expressed and for developing domain-specific word lists that can be applied for topic detection, we adapt the approach proposed by Loughran and McDonald [57.", "probabilities": [0.08547677844762802, 0.009689141064882278, 0.35813477635383606, 0.5466993451118469]}, {"string": "In order to evaluate the different predictive models, we perform stratified 10-fold cross-validation [68.", "probabilities": [0.0853833332657814, 0.009678443893790245, 0.3582538664340973, 0.5466842651367188]}, {"string": "We select the parameters of the kernel function by means of the grid-search heuristic proposed by [67.", "probabilities": [0.08537216484546661, 0.009676765650510788, 0.3582666516304016, 0.5466843843460083]}, {"string": "For the CF component Weka was used to instantiate a k-nearest neighbors classifier, IBk [43.", "probabilities": [0.08538351953029633, 0.009676485322415829, 0.3582472801208496, 0.5466927886009216]}, {"string": "We use the commonly accepted metrics of recall, precision, and F-measure [21 to measure accuracy across all methods.", "probabilities": [0.08544648438692093, 0.009680645540356636, 0.3581582307815552, 0.5467145442962646]}, {"string": "Following [21, the performance of CF and Cacheda et al. were contrasted with that of our method via the F-measure.", "probabilities": [0.10034699738025665, 0.012292391620576382, 0.3478790521621704, 0.5394814610481262]}, {"string": "Using external aggregate ratings have been shown to improve recommendation systems, as long as the external source is statistically representative of the population (or individual) of interest [44.", "probabilities": [0.7901884913444519, 0.0538325235247612, 0.04368150606751442, 0.11229746788740158]}, {"string": "The absolute difference in F-measure is substantial if it is >0.05, and the difference in at least one of its components (i.e., precision or recall) is statistically significant (determined using the z-test for proportions at the significance level of 0.05, two-tailed, critical value of z=1.96) [21.", "probabilities": [0.18944188952445984, 0.017377808690071106, 0.25110703706741333, 0.5420733094215393]}, {"string": "We also performed a z-test for proportions [45 for precision and recall to identify where we significantly outperformed the other methods prior to comparing F-measures.", "probabilities": [0.08550681918859482, 0.009690728038549423, 0.3580923080444336, 0.5467101335525513]}, {"string": "This method introduces a latent topic variable and assumes that both the topic-word distribution and document-topic distribution are Dirichlet distributions [50.", "probabilities": [0.08827058970928192, 0.009832399897277355, 0.3541331887245178, 0.5477637648582458]}, {"string": "Using an estimation method (e.g., Markov Chain Monte Carlo), the prior parameters of the distributions can be inferred [51.", "probabilities": [0.09005919843912125, 0.009940087795257568, 0.3516523540019989, 0.5483483672142029]}, {"string": "However, LDA also has some demonstrated drawbacks [52.", "probabilities": [0.7269346714019775, 0.0737321600317955, 0.0651770606637001, 0.13415609300136566]}, {"string": "A perplexity-based approach is applied to estimate the number of topics in all documents [57.", "probabilities": [0.21759353578090668, 0.015038054436445236, 0.2322724461555481, 0.5350959897041321]}, {"string": "Chen and Xu [56 first considered combining the domain ontology and topic model results, which enabled them to obtain constant topics from the domain ontology and new topics from customer reviews.", "probabilities": [0.8445739150047302, 0.03758712485432625, 0.027638042345643044, 0.09020086377859116]}, {"string": "In ontology-based text analytics tasks, researchers often manually construct a domain ontology before the text mining process [55.", "probabilities": [0.8217347860336304, 0.0401519276201725, 0.035671576857566833, 0.10244175791740417]}, {"string": "Finally we may consider using general regression techniques, as an alternative approach to estimate the elasticity [34].", "probabilities": [0.48165956139564514, 0.026602277532219887, 0.10170933604240417, 0.39002880454063416]}, {"string": "Temporal based preprocessing can contribute in removing artifacts [140] from the signal using linear combination of the EOG-contaminated EEG signal and the EOG signal recorded using eye movement recording electrodes.", "probabilities": [0.39571377635002136, 0.18150146305561066, 0.11058023571968079, 0.31220465898513794]}, {"string": "It records the electrical activity of neurons at the embracing area [102].", "probabilities": [0.20372000336647034, 0.06920148432254791, 0.5062357783317566, 0.22084276378154755]}, {"string": "Consciousness level monitoring via brain waves has been expanded to include not only drivers but also stayed-alone sick people as suggested in [30].", "probabilities": [0.82914799451828, 0.041644174605607986, 0.032016780227422714, 0.09719104319810867]}, {"string": "In another study [29], a virtual reality-based motion-sickness platform has been designed with a 32-channel EEG system and a joystick which is used to report the motion sickness level (MSL) in real time experiments.", "probabilities": [0.6406591534614563, 0.23793110251426697, 0.03826187923550606, 0.08314789831638336]}, {"string": "Table 1[123] gives a summary for brain acquisition methods along with their advantages and disadvantages.", "probabilities": [0.560809850692749, 0.1455368548631668, 0.1312752366065979, 0.16237802803516388]}, {"string": "EP components are labeled either exogenous or endogenous [125].", "probabilities": [0.8437744379043579, 0.03667576238512993, 0.028212571516633034, 0.09133721143007278]}, {"string": "Steady State Evoked Potentials (SSEP) [127] are evoked by a stimulus modulated at a fixed frequency and occur as an increase in EEG activity at the stimulation frequency.", "probabilities": [0.21313944458961487, 0.07068770378828049, 0.49378249049186707, 0.2223903238773346]}, {"string": "It has been found that distraction and fatigue are two main sources for driver s inattention, which is considered as a strong cause for most traffic accidents [63].", "probabilities": [0.6360782980918884, 0.10023708641529083, 0.10223034024238586, 0.16145431995391846]}, {"string": "The administration of probiotics is reported to help restore the health of H7N9 patients more quickly [164].", "probabilities": [0.7875986099243164, 0.05220818147063255, 0.04505671188235283, 0.11513656377792358]}, {"string": "Iida et al. [169] reported that optimal responses to cancer therapy require an intact commensal microbiota that mediates the therapy effects by modulating myeloid-derived cell functions in the tumor microenvironment.", "probabilities": [0.8496741056442261, 0.03577104210853577, 0.026311974972486496, 0.08824292570352554]}, {"string": "Viaud et al. [170] reported that the gut microbiota helps to shape the anticancer immune response of cyclophosphamide.", "probabilities": [0.8472393751144409, 0.0366402193903923, 0.026919854804873466, 0.08920051157474518]}, {"string": "We used the ranking of the statements outlined in Section 4 in order to capture a respondents player type as outlined in Bartle s classic work [17].", "probabilities": [0.08537119626998901, 0.009676825255155563, 0.3582684099674225, 0.5466835498809814]}, {"string": "For female players the transition from Achiever to Socialiser occurs at a lower level of agreeableness, we hypothesise this is due to the increased tendency for social emphasis over tasked emphasis [42].", "probabilities": [0.8434997797012329, 0.0376204252243042, 0.027964754030108452, 0.09091499447822571]}, {"string": "A linear regression model was created from all of the variables in the study and we followed a step-wise reduction process of removing the least significant term until all model coefficients are significant and there was no further reduction in the Akaike s Information Criteria [37].", "probabilities": [0.08541969954967499, 0.009678403846919537, 0.35819482803344727, 0.546707034111023]}, {"string": "It is worth noting that all of the characters are heroes (far from playing a  damsel in distress  trope [40]) and although there are fewer female characters in the entire hero roster, 7 out of the 12 most popular characters were female.", "probabilities": [0.6333222389221191, 0.10184919089078903, 0.10309071093797684, 0.1617378145456314]}, {"string": "Brain Hex [19] was one of the first attempts at mapping personality types to gaming motivation archetypes, this created a larger number of gaming archetypes and identified some correlations with the Mayers-Briggs psychotypes to these resulting archetypes, this was done across a number of different games and gamers were asked to consider an abstract situation to consider their gaming archetype.", "probabilities": [0.8430966734886169, 0.03279412165284157, 0.02867162600159645, 0.09543754160404205]}, {"string": "All conventional applications must expand their semantic functionalities towards YouReputation, and YouReputation, in turn, its user-friendliness and reliability towards the products on the market (see [34]).", "probabilities": [0.8037157654762268, 0.04781947657465935, 0.0398779921233654, 0.10858679562807083]}, {"string": "Upon the recommendation of the referees this article is a revised version of Portmann''s article [33] presented at the Sixth International Summer School on Aggregation Operators in Benevento, Italy.", "probabilities": [0.846643328666687, 0.036477524787187576, 0.027211686596274376, 0.0896674171090126]}, {"string": "In [6], Levesque''s logic of only knowing is introduced as an extension of autoepistemic logic to enable the modeling of expressions of the form   is all that is believed , i.e. there are no other relevant beliefs, but  . To this end, the language is expanded with a second modal operator O. For every formula  , O  reads   is all that is believed  or  only   is believed .", "probabilities": [0.7959293127059937, 0.06882661581039429, 0.036639343947172165, 0.09860478341579437]}, {"string": "The hedge  a sort of  is thus a specific hedge that in some sense has widening effect, but when applied, the given expression does not include typical examples hence absurdness of the latter example (cf. also [18]).", "probabilities": [0.8171041011810303, 0.044294390827417374, 0.03572973981499672, 0.10287179052829742]}, {"string": "The necessity to use hedges and the difference between widening and narrowing hedges has been discussed in [27].", "probabilities": [0.8452688455581665, 0.036555562168359756, 0.02769438363611698, 0.09048127382993698]}, {"string": "A revised and extended version of their work is [12].", "probabilities": [0.34170088171958923, 0.12857568264007568, 0.3186677396297455, 0.21105563640594482]}, {"string": "This paper, which extends our previous work [8], contributes to the inclusion of the risk management discipline into the Cloud computing paradigm.", "probabilities": [0.05698240548372269, 0.9148361086845398, 0.014860241673886776, 0.013321351259946823]}, {"string": "This definition extends the one presented in [23], in which d(i,ij)=1 if i and nij belong to any common domain.", "probabilities": [0.26937827467918396, 0.10905898362398148, 0.40463975071907043, 0.21692293882369995]}, {"string": "This study extends the work published in [8].", "probabilities": [0.0685335174202919, 0.8987712860107422, 0.01688366010785103, 0.015811579301953316]}, {"string": "This paper extends our previous work [8] in several ways.", "probabilities": [0.094423808157444, 0.8528866171836853, 0.02662818878889084, 0.02606138028204441]}, {"string": "These platforms aim at providing real-time, cheap, secure, and on-demand services to customers, through different types of Clouds, which also include temporary vehicular Clouds (i.e. formed by the vehicles representing the Cloud datacenters [72]) designed to expand the conventional Clouds in order to increase on-demand the whole Cloud computing, processing, and storage capabilities, by using under-utilized facilities of vehicles.", "probabilities": [0.8500311374664307, 0.03550652042031288, 0.02626047283411026, 0.08820191025733948]}, {"string": "This paper extends the work presented in [19] in the following ways: (i) a deeper contextualization of the group modeling problem with the state of the art is given, (ii) a novel motivation to the group modeling problem in our context is presented, (iii) an answer to a set of research questions that arise when dealing with the problem is given.", "probabilities": [0.3530236482620239, 0.5546187162399292, 0.03589066118001938, 0.056467048823833466]}, {"string": "CEPSim extends CloudSim [12] using a query model based on Directed Acyclic Graphs (DAGs) and introduces a simulation algorithm based on a novel abstraction called event sets.", "probabilities": [0.09708034247159958, 0.010575768537819386, 0.34300175309181213, 0.5493422150611877]}, {"string": "This article significantly extends the authors  previous work [15] by improving the discussion about CEPSim s goals and assumptions, by introducing the event set concept, by presenting detailed descriptions of all simulation algorithms and a thorough evaluation of CEPSim.", "probabilities": [0.09608731418848038, 0.8457205891609192, 0.029725873842835426, 0.028466178104281425]}, {"string": "Garg and Buyya [9] created NetworkCloudSim, which extends CloudSim with a three-tier network model and an application model that can represent communicating processes.", "probabilities": [0.5612103343009949, 0.07203619927167892, 0.16982951760292053, 0.19692398607730865]}, {"string": "This section describes a fuzzy lightweight semantic model (F-lightweight in abbreviation) which is an extension of our previous work: a context-aware lightweight similarity model (C-lightweight in abbreviation) [46].", "probabilities": [0.06526529788970947, 0.9020891189575195, 0.01677817292511463, 0.01586735248565674]}, {"string": "This paper extends our preliminary workshop publication [13] which reported on initial progress on the Cloud-e-Genome project, a collaboration between the School of Computing Science and Institute of Genetic Medicine at Newcastle University.", "probabilities": [0.05540800467133522, 0.9173592329025269, 0.014393188059329987, 0.01283960323780775]}, {"string": "Although, the concept of cloud computing is mainly popular in three flavors (1) Infrastructure-as-a-Service (IaaS), (2) Platform-as-a-Service (PaaS) and (3) Software-as-a-Service (SaaS), but in this data science age, should be equally expandable to Database-as-a-Service (DBaaS) [2].", "probabilities": [0.8480887413024902, 0.0358598493039608, 0.026864919811487198, 0.0891864076256752]}, {"string": "An alternative approach is to replace the somewhat homomorphic encryption scheme with an additively homomorphic encryption scheme, e.g. Paillier [32].", "probabilities": [0.6340653896331787, 0.0636867955327034, 0.05456230416893959, 0.2476854771375656]}, {"string": "A preliminary version of this paper appeared as [21].", "probabilities": [0.45628657937049866, 0.20189669728279114, 0.1740836650133133, 0.1677331030368805]}, {"string": "This mapping is detailed in our previous work [2].", "probabilities": [0.09158501029014587, 0.8547046184539795, 0.02630133181810379, 0.027408944442868233]}, {"string": "With the help of IoT, the Internet is expanding at an unprecedented scale connecting people all over the globe and even outside the globe (e.g. Interplanetary Web [86]).", "probabilities": [0.8504838347434998, 0.035412974655628204, 0.026125004515051842, 0.08797811716794968]}, {"string": "Specifically, C-SPARQL [12] extends the SPARQL language with window and aggregation clauses to support RDF stream processing.", "probabilities": [0.12421494722366333, 0.012045457027852535, 0.31197136640548706, 0.5517681837081909]}, {"string": "This article extends previous work [6].", "probabilities": [0.08534155040979385, 0.8677607774734497, 0.023345764726400375, 0.02355189621448517]}, {"string": "The definition of these parameters can be found in [24.", "probabilities": [0.32727527618408203, 0.1280231773853302, 0.33286184072494507, 0.21183960139751434]}, {"string": "The weighted degree of node i, kwi, is defined as [17 where ki is the degree of node i and  j=1kiwij is the sum over all the edge weight of node i. Integers   and   are the user-defined weights, showing the preferences of the user.", "probabilities": [0.2026139497756958, 0.06942708790302277, 0.5074782371520996, 0.2204807847738266]}, {"string": "Here we only discuss the case where   =   = 1 following the line of thought given in [17, treating the edge weights and the node degree equally.", "probabilities": [0.22276385128498077, 0.07443352788686752, 0.47940507531166077, 0.22339755296707153]}, {"string": "Gaining higher level evolution information about software has been regarded as a key to deal with software complexity and deterioration [26.", "probabilities": [0.8374647498130798, 0.03888328745961189, 0.02974572591483593, 0.0939062088727951]}, {"string": "The WCCNs for all the subject systems are all automatically built and analyzed by our own developed tool SNAP [25.", "probabilities": [0.08540555834770203, 0.009683000855147839, 0.3582325279712677, 0.546678900718689]}, {"string": "In addition, in the experimental cases where the visiting style was matched to appropriate content, the users demonstrated increased interest by requesting more information about the exhibits explicitly [28.", "probabilities": [0.7121890187263489, 0.07259974628686905, 0.07218683511018753, 0.14302437007427216]}, {"string": "The ultimate goal is to calculate the degrees of separation between objects and people, targeting to the maximization of the impact that can exist through the interactions of people and cultural heritage objects; the approach presented in [29 is one possible way to do this.", "probabilities": [0.725067675113678, 0.024936474859714508, 0.04540480673313141, 0.2045910507440567]}, {"string": "HIPS assumed that different visiting styles need different durations for the presentations and the empirical data support this hypothesis [27.", "probabilities": [0.8274226188659668, 0.04376029223203659, 0.03205830976366997, 0.09675882011651993]}, {"string": "HIPS was using infrared emitters to connect to the users devices (PDAs) [28.", "probabilities": [0.7090900540351868, 0.019586166366934776, 0.04188627749681473, 0.2294374406337738]}, {"string": "Typically, an RFID tag can accommodate up to 2 KBytes of data [32, so the amount of this information is essentially limited and external storage services are required to store additional data for the exhibit, including extended semantic data, multimedia files, enhanced descriptions, histories and so forth.", "probabilities": [0.8468081951141357, 0.03638093173503876, 0.027153978124260902, 0.08965689688920975]}, {"string": "This can be implemented for example by having FlashAir cards [33 installed on them, which may provide from 8 to 32 GB of storage space, plus wireless LAN communication capabilities.", "probabilities": [0.6521488428115845, 0.043985847383737564, 0.0698094293475151, 0.23405586183071136]}, {"string": "The RFID tag is sensed by RFID readers hosted in the exhibition rooms, which notify a museum-hosted server regarding the locations of the exhibits [30.", "probabilities": [0.8447709679603577, 0.036641787737607956, 0.027853703126311302, 0.0907336100935936]}, {"string": "Out of the abundance of items that they possess, museums select only a handful to put on display [2.", "probabilities": [0.8481221199035645, 0.03586592152714729, 0.026846103370189667, 0.0891658291220665]}, {"string": "In theory, the result of the meticulous work of the curator is a story that is told in a space [5.", "probabilities": [0.8491279482841492, 0.035799600183963776, 0.02650274895131588, 0.08856972306966782]}, {"string": "For most visitors, they are mere sets of exhibits, accompanied by some information such as the name of the creator or the year in which they were created [6.", "probabilities": [0.8486862778663635, 0.035732533782720566, 0.026684369891881943, 0.08889686316251755]}, {"string": "Each individual exhibit in a museum has many stories to tell: for example Goya s monumental work  The 3rd of May 1808 in Madrid  (Fig. 1) [7 can tell us about Spanish resistance to Napoleon s armies; about the horror of war; about how different people face death; about the history of Madrid; about the artist s personal style or the trends of the artistic period; and so forth.", "probabilities": [0.8504683971405029, 0.03542476147413254, 0.02612762525677681, 0.08797920495271683]}, {"string": "But personalization is not only viewed as important by experts, since museum visitors also report the need to have personalized experiences [63.", "probabilities": [0.6347692012786865, 0.10076294094324112, 0.10274725407361984, 0.16172057390213013]}, {"string": "For example, the SNOPS project provided multilevel information in a smart-city environment, where visitors could exploit object features in regards to specific contexts [50.", "probabilities": [0.7853327989578247, 0.05204497650265694, 0.04620024561882019, 0.11642206460237503]}, {"string": "Since more and more people recognize the importance of personalization in cultural heritage, due to its potential to address the needs of a very diverse audience, there are many works that attempt to apply personalization in spaces of cultural heritage [45.", "probabilities": [0.8472496867179871, 0.03629690036177635, 0.027004988864064217, 0.0894484668970108]}, {"string": "When it comes to the IoT, personalization is recognized as a critical element for the success of systems and applications [61.", "probabilities": [0.8501211404800415, 0.035513218492269516, 0.026224683970212936, 0.08814101666212082]}, {"string": "Therefore, content requesters can query the venue server regarding the acceptability of the content they received, similarly to the logic of web browsers, which query OCSP servers [41 to verify that certificates they receive have not been revoked.", "probabilities": [0.8448302745819092, 0.03718919679522514, 0.02760714665055275, 0.09037332981824875]}, {"string": "As a first measure, the wireless LAN access point that is embedded in the FlashAir cards may be set to Station Mode, in which case the FlashAir becomes a Wireless LAN client [40, and the APPSSID and APPNETWORKKEY parameters (corresponding to a WLAN s Service Set Identifier (SSID) and network key) can be set to point to a venue-operated trusted wireless network.", "probabilities": [0.6689595580101013, 0.18880236148834229, 0.044605616480112076, 0.09763238579034805]}, {"string": "This internal procedure is out of the scope of the current work and is presented thoroughly in [29.", "probabilities": [0.7289785146713257, 0.12921880185604095, 0.042854148894548416, 0.09894844889640808]}, {"string": "Therefore, exhibits that remain still can be used as reference tags, and by applying a weighted-center of gravity technique, one reader antenna can be proved sufficient for an area of 12 m by 10 m, providing a level of accuracy of about 1.07 m [36.", "probabilities": [0.6262533068656921, 0.06366132944822311, 0.05666550248861313, 0.2534198462963104]}, {"string": "According to Ting et al. [30, RFID tags can be used to geolocate objects, exploiting the signal strength captured by appropriately placed RFID reader antennas.", "probabilities": [0.817497968673706, 0.04474075511097908, 0.03593837097287178, 0.10182293504476547]}, {"string": "In this notion, we suggest that given any pair of items we can find meaningful links between them with a reasonably small number of steps that go through facts related not only to history or art, but also to popular culture and any available information about the target user s memories and interests; experiments reported in [14 provide evidence that user-specific intermediate entities can be used as elements of the path linking two nodes in a semantic network, and we plan to further explore this issue through experiments specifically targeted to the aforementioned aspects (popular culture, user s memories and interests).", "probabilities": [0.7050408124923706, 0.022445350885391235, 0.04763435572385788, 0.22487948834896088]}, {"string": "Google11Google Inc. - http://google.com.has experimented with the notion of x degrees of separation of items in museum collections based on their visual similarity [13, but we find the semantic notion of x degrees of separation far more stimulating.", "probabilities": [0.8416503667831421, 0.038193657994270325, 0.028513016179203987, 0.09164292365312576]}, {"string": "The documentary-like content also adapted to the interests of the user [25.", "probabilities": [0.20418372750282288, 0.07066764682531357, 0.5047503709793091, 0.22039827704429626]}, {"string": "Additionally, semantic information can be exploited to automatically generate documentaries as described in [19.", "probabilities": [0.2442765235900879, 0.0710555762052536, 0.4557379484176636, 0.22892999649047852]}, {"string": "Particularly  smart exhibit  has been used before in the literature to refer to exhibits that are handled by a system in a smart way [69.", "probabilities": [0.8192988038063049, 0.028271792456507683, 0.03204946219921112, 0.12037994712591171]}, {"string": "Internet linked interactive museum item labels were used to construct narratives and increase visitor engagement [58.", "probabilities": [0.10574173182249069, 0.013226022012531757, 0.39182305335998535, 0.4892091453075409]}, {"string": "A recent work developed feature detectors to match and extract various static features such as API calls, Linux system commands, and manifest s permissions [29.", "probabilities": [0.21369989216327667, 0.06980052590370178, 0.4935057759284973, 0.2229938805103302]}, {"string": "PUMA [8, is an Android malware detector which uses permission and uses-feature of an application as feature set to train machine learning algorithms.", "probabilities": [0.18259580433368683, 0.04317500442266464, 0.48197317123413086, 0.2922559976577759]}, {"string": "DroidAnalyzer is a static analysis tool which identifies potential vulnerabilities of Android apps and the presence of root exploits [21.", "probabilities": [0.6969423294067383, 0.0577799528837204, 0.0899263396859169, 0.15535135567188263]}, {"string": "Scikit-learn [45, a Python library was used for the purpose of machine learning algorithms training and testing.", "probabilities": [0.09097391366958618, 0.010130749084055424, 0.3508252203464508, 0.5480701327323914]}, {"string": "All third party applications were verified with VirusTotal [43 and we found out that 46 (7%) applications are detected as malware by minimum one antivirus engine.", "probabilities": [0.16384369134902954, 0.025133041664958, 0.27745509147644043, 0.5335682034492493]}, {"string": "In a recent work, permissions and source code analysis based feature set were used to classify apps into malicious and benign using machine learning [27.", "probabilities": [0.13380424678325653, 0.013538121245801449, 0.3013424873352051, 0.5513151288032532]}, {"string": "An Android malware classification framework to manage big app market was proposed, which used 11 different types of static features and ensemble of various learners [19.", "probabilities": [0.2379687875509262, 0.04422968998551369, 0.3150724768638611, 0.40272897481918335]}, {"string": "Apk Evaluator [32 is a permission-based classification system for the Android application.", "probabilities": [0.2319420874118805, 0.04707956686615944, 0.4261294901371002, 0.29484879970550537]}, {"string": "PIndroid [28, used permissions and intents to train ensemble learner and achieved 99.8% accuracy.", "probabilities": [0.636539101600647, 0.22535891830921173, 0.04522424563765526, 0.09287770092487335]}, {"string": "P-Spec is a formal policy specification language with context-free grammar [17, which can be utilized to formally describe the heterogeneous service s privacy policies or consumer s privacy preferences precisely.", "probabilities": [0.5724596381187439, 0.07089483737945557, 0.1624746322631836, 0.19417095184326172]}, {"string": "Changbo Ke and Zhiqiu Huang [16 utilized description logic to model privacy preferences and privacy policies, and identified the policy inconsistency conflicts between privacy preferences and privacy policies with the method of ontology rules reasoning algorithm, i.e., tableau algorithm.", "probabilities": [0.6814220547676086, 0.020847754552960396, 0.04828447476029396, 0.24944572150707245]}, {"string": "Among them, the 8 requirements covering well-defined semantics, monotonicity, credential combination, constraints on attribute value, inter-credential constraints, credential chains, local credential variables and compliance checker modes are proposed by K. Seamons [6.", "probabilities": [0.8458583950996399, 0.036216214299201965, 0.027577819302678108, 0.09034749865531921]}, {"string": "Elisa [14 proposed a service selection approach with ranking mechanism to seek out a privacy-aware service composition from service repository.", "probabilities": [0.3151058852672577, 0.07951788604259491, 0.3755910098552704, 0.22978521883487701]}, {"string": "Kagal [15 proposed a semantic-based privacy policy matching framework and a semantic-based privacy policy language which can be utilized to describe privacy policies.", "probabilities": [0.8492611646652222, 0.035829704254865646, 0.026446623727679253, 0.08846255391836166]}, {"string": "In further, according to the Ref. [19, the model of formal language can be defined as an interpretation under a specific universe of A for the formal language.", "probabilities": [0.5638463497161865, 0.08765987306833267, 0.1562080830335617, 0.1922856867313385]}, {"string": "is defined as follows [6] where MDL UL=offset of NB-IoT channel number to downlink/uplink, FDL UL_low=downlink/uplink operating band, NDL UL=downlink/uplink E-UTRA absolute radio frequency channel number (EARFCN), NoffDL UL=Minimum range of NDL UL for downlink/uplink.", "probabilities": [0.5546243786811829, 0.10832870751619339, 0.1557726263999939, 0.18127426505088806]}, {"string": "The relationship between the required data bit rate with the chirp rate and symbol rate in the LoRa modulation technique [5] is defined as follows:", "probabilities": [0.12120290845632553, 0.01945854164659977, 0.32174021005630493, 0.5375983119010925]}, {"string": "The maximum coupling loss (MCL) is the limit value of the coupling loss at which the service can be delivered, and therefore it defines the range of the service [12].", "probabilities": [0.6956794857978821, 0.08221541345119476, 0.07741241902112961, 0.14469268918037415]}, {"string": "All end-devices start and join the network as end-devices of Class A and can then decide to switch to Class B [9].", "probabilities": [0.8437594771385193, 0.03679033741354942, 0.028150230646133423, 0.09129995107650757]}, {"string": "Concept: The network slicing concept proposed by Next Generation Mobile Networks (NGMN) Alliance consists of three layers as depicted in Fig. 5, namely, (1) service instance layer, (2) network slice instance layer, and (3) resource layer [9].", "probabilities": [0.2524735629558563, 0.07300519198179245, 0.446403443813324, 0.22811783850193024]}, {"string": "Our approach extends the RankJoin operator in [8] to work with UNCQs.", "probabilities": [0.21319882571697235, 0.07055158913135529, 0.25249049067497253, 0.46375903487205505]}, {"string": "The study thus extends the work of researchers such as Pempek et al. [64] by demonstrating that the effect of social gratifications on problematic SNS use is fully mediated by the presence of arousal in the online experience.", "probabilities": [0.1050199642777443, 0.8331215977668762, 0.031248435378074646, 0.03061005286872387]}, {"string": "This article expands upon our previous workshop publication [28].", "probabilities": [0.05564265698194504, 0.9169683456420898, 0.014469524845480919, 0.012919485569000244]}, {"string": "FE extracts multiple features from the original data to generate a dataset [6].", "probabilities": [0.20238026976585388, 0.06911270320415497, 0.5079659223556519, 0.2205410748720169]}, {"string": "Work presented here extends initial results presented in [34].", "probabilities": [0.1144188791513443, 0.8196762204170227, 0.0329260379076004, 0.03297879546880722]}, {"string": "The approach presented in this paper builds on and expands the basic finding of the effectiveness of list-wise learning-to-rank, demonstrated in [27], where we first introduced ListRank, a ranking-oriented matrix factorization approach.", "probabilities": [0.13250377774238586, 0.7297530770301819, 0.047093555331230164, 0.09064958989620209]}, {"string": "This paper significantly extends our earlier work [29] and makes the following two major contributions.", "probabilities": [0.09628113359212875, 0.8454371094703674, 0.029764337465167046, 0.028517451137304306]}, {"string": "Dryad provides a large number of functionality, including generating the job graph, scheduling the processes on the available machines, handling transient failures in the cluster, collecting performance metrics, visualizing the job, invoking user-defined policies and dynamically updating the job graph in response to these policy decisions, without awareness of the semantics of the vertices [101].", "probabilities": [0.8257994055747986, 0.03396502882242203, 0.03456375375390053, 0.10567191988229752]}, {"string": "It bases on dataflow graph processing [101].", "probabilities": [0.14835171401500702, 0.02684311755001545, 0.4599684178829193, 0.3648366928100586]}, {"string": "The Apache Mahout [74] aims to provide scalable and commercial machine learning techniques for large-scale and intelligent data analysis applications.", "probabilities": [0.5887803435325623, 0.05531786382198334, 0.15359744429588318, 0.20230431854724884]}, {"string": "In the past decades, the persistent data were stored by using hard disk drives (HDDs) [87].", "probabilities": [0.7233137488365173, 0.019667264074087143, 0.03920482099056244, 0.21781422197818756]}, {"string": "Direct-attached storage (DAS), network-attached storage (NAS), and storage area network (SAN) are the enterprise storage architectures that were commonly used [99].", "probabilities": [0.6980533599853516, 0.019518908113241196, 0.04442247748374939, 0.23800517618656158]}, {"string": "Recently, a generative deep networks, called autoencoder [70], perform very well as non-linear dimensionality reduction.", "probabilities": [0.8492553234100342, 0.03577065467834473, 0.02646261267364025, 0.08851145952939987]}, {"string": "Random projection in dimensionality reduction also have been well-developed [25].", "probabilities": [0.6867669224739075, 0.05983418598771095, 0.09483347088098526, 0.1585654616355896]}, {"string": "Pentaho [4] is another software platform for Big Data.", "probabilities": [0.08596217632293701, 0.009722151793539524, 0.35788461565971375, 0.5464311242103577]}, {"string": "Karmasphere [3] is another Hadoop-based Big Data platform for business data analysis.", "probabilities": [0.2102544754743576, 0.06999819725751877, 0.4977186620235443, 0.2220286726951599]}, {"string": "Biologically inspired Computing[26] maybe provides tools to solve Big Data problems from hardware design to software design.", "probabilities": [0.8017300963401794, 0.04953070357441902, 0.04020657017827034, 0.10853255540132523]}, {"string": "In analogy to nature, bio-inspired hardware systems can be classified as three axes, phylogeny, ontogeny, and epigenesis [171].", "probabilities": [0.850429356098175, 0.03539480268955231, 0.026144253090023994, 0.08803161233663559]}, {"string": "Usually, we need to combine the distributed MapReduce and cloud computing to get an effective answer for providing petabyte-scale computing [108].", "probabilities": [0.7032260298728943, 0.07098393142223358, 0.0765330046415329, 0.14925703406333923]}, {"string": "Wang and Sun [188] proposed a bio-inspired cost minimization mechanism for data-intensive service provision.", "probabilities": [0.850452184677124, 0.03542695194482803, 0.02613273449242115, 0.08798803389072418]}, {"string": "Clustering Big Data is also developing to distributed and parallel implementation [150].", "probabilities": [0.3665491044521332, 0.07766810804605484, 0.3264409601688385, 0.22934184968471527]}, {"string": "In essence, quantum computing [107] is to harness and exploit the powerful laws of quantum mechanics to process information.", "probabilities": [0.8487733602523804, 0.035954006016254425, 0.026573145762085915, 0.08869953453540802]}, {"string": "Moreover, if the data backup process is outsourced to a third party by the CSP, risks boundary is also broadened [39].", "probabilities": [0.8058896064758301, 0.044587697833776474, 0.04052012041211128, 0.10900264978408813]}, {"string": "From the earlier version [1], we have taken over the related work discussion (Section 2); we have expanded the related work though w.r.t. to our extension.", "probabilities": [0.20660518109798431, 0.07533229142427444, 0.49878358840942383, 0.2192789614200592]}, {"string": "The current work extends our previous work about the FTS-SOCI tool [20].", "probabilities": [0.09645308554172516, 0.8451527953147888, 0.029817018657922745, 0.028577137738466263]}, {"string": "To summary, this paper extends our previous work on Preference Networks [31] with the following contributions: A novel use of  1-regularized Markov random fields for structure learning in the context of collaborative filtering.", "probabilities": [0.10327301174402237, 0.8230627775192261, 0.03087589330971241, 0.04278844967484474]}, {"string": "This paper extends our previous work of Preference Networks [31] for collaborative filtering.", "probabilities": [0.05988868325948715, 0.9105110764503479, 0.015453051775693893, 0.014147221110761166]}, {"string": "Our approach in this work is an extension of the basic one proposed in [10].", "probabilities": [0.5396193861961365, 0.17430169880390167, 0.06610067188739777, 0.21997831761837006]}, {"string": "This paper is an extension of our conference paper published in ACM BCB 2015 [23].", "probabilities": [0.058736398816108704, 0.911803126335144, 0.015513029880821705, 0.013947519473731518]}, {"string": "This work is an extension of the paper [17] and introduces the following contributions.", "probabilities": [0.05582219362258911, 0.9166860580444336, 0.014531007036566734, 0.012960754334926605]}, {"string": "We conducted our experiments using the QOS_WSDL dataset [13], which includes measurements of quality attributes  values for more than 1548 real world Web services.", "probabilities": [0.0855475515127182, 0.009707882069051266, 0.3580818772315979, 0.5466626882553101]}, {"string": "In this step, we use NbClust package written in R [10] to find the best number of clusters.", "probabilities": [0.08536561578512192, 0.009675600565969944, 0.3582734763622284, 0.5466852784156799]}, {"string": "This is the K value that represents the best number of clusters [7].", "probabilities": [0.2030090093612671, 0.06909063458442688, 0.5072014331817627, 0.22069886326789856]}, {"string": "The experiments in this step are conducted by using the R package clValid [8], which compares different clustering algorithms for the sake of identifying the best clustering algorithm.", "probabilities": [0.08554074913263321, 0.009706697426736355, 0.3580891191959381, 0.5466634631156921]}, {"string": "The gap statistics was proposed by Tibshirani et al. [28] to estimate the number of clusters in clustering analysis.", "probabilities": [0.7999582886695862, 0.04380914568901062, 0.04365111514925957, 0.11258147656917572]}, {"string": "In [3], the authors concluded that no clustering algorithm performs well in all the evaluation criteria.", "probabilities": [0.14855870604515076, 0.7817401885986328, 0.03443404659628868, 0.03526696935296059]}, {"string": "UCF denotes the traditional CF approach proposed by Resnick et al. [38], which uses the similarity between users to predict missing ratings.", "probabilities": [0.8007909059524536, 0.058993443846702576, 0.035099390894174576, 0.10511636734008789]}, {"string": "ICF denotes the traditional item-based CF approach proposed by Sarwar et al. [39].", "probabilities": [0.7328342199325562, 0.0231588464230299, 0.039574310183525085, 0.2044326215982437]}, {"string": "UCF measures the similarity between users using the Pearson correlation coefficient [8].", "probabilities": [0.08537904918193817, 0.009676523506641388, 0.358254611492157, 0.5466897487640381]}, {"string": "BCCF denotes the CF approach presented in [36] which uses a novel similarity measure based on the Bhattacharyya Coefficient.", "probabilities": [0.11043409258127213, 0.011563234962522984, 0.32731005549430847, 0.550692617893219]}, {"string": "MJDCF stands for the approach proposed by Bobadilla et al. [7] to mitigate the new user cold-start problem.", "probabilities": [0.8068416118621826, 0.0490020290017128, 0.03840760141611099, 0.10574868321418762]}, {"string": "EUCF is the entropy-based UCF approach proposed by Kaleli [27] to improve neighborhood formation in UCF.", "probabilities": [0.6904727220535278, 0.021518904715776443, 0.04596542567014694, 0.24204294383525848]}, {"string": "MFCF is a model-based CF approach which uses a basic MF model [29] to predict unknown ratings.", "probabilities": [0.5866324305534363, 0.0543825700879097, 0.06878786534070969, 0.29019707441329956]}, {"string": "HUIT, proposed by Shambour and Lu [42], is a hybrid of the user-based and item-based trust recommendation approaches.", "probabilities": [0.8402814865112305, 0.036172304302453995, 0.028668032959103584, 0.0948781967163086]}, {"string": "EICF is the entropy-based ICF approach [21] which solves the optimization problem of selecting the best neighbors of a target item based on the similarity and entropy difference between the target item and other items.", "probabilities": [0.6054986715316772, 0.167987659573555, 0.0790603831410408, 0.14745329320430756]}, {"string": "\"''\"\"For each test user, based on the \"\"\"\"Given   n\"\"\"\" experimental protocol [8], n ratings are randomly chosen as the training ratings and the remaining ratings are used as the test ratings.\"\"''\"", "probabilities": [0.08610381931066513, 0.009754475206136703, 0.35734444856643677, 0.5467972755432129]}, {"string": "According to the object-based attention theory [6], human visual processing starts with the segmentation of an image into connected regions, which are also termed as proto-objects or pre-attentive objects.", "probabilities": [0.8490824103355408, 0.03608349338173866, 0.026434853672981262, 0.08839932829141617]}, {"string": "Table 2 provides a comparative study of five other state-of-the-art segmentation algorithms developed for brain tumor segmentation, along with SGC and AGC, as applied on the BRATS database [28] and measuring accuracy in terms of DICE as well as average time of computation (in min) as evaluated on CPU-based and cluster architectures.", "probabilities": [0.3916015326976776, 0.020301852375268936, 0.13950271904468536, 0.4485938549041748]}, {"string": "Ground truth generation protocol for the BRATS database uses the FLAIR and T2 MR sequences for the process [28].", "probabilities": [0.0856035053730011, 0.009691495448350906, 0.3579385280609131, 0.546766459941864]}, {"string": "Here  E76= Vi Vj , with  .  expressing the L2-norm [21] and Vi=Li*,ai*,bi*,Vj=Lj*,aj*,bj*.", "probabilities": [0.20831899344921112, 0.07042954117059708, 0.4998171329498291, 0.22143429517745972]}, {"string": "Thus our algorithm is simultaneously employed over multiple scales, analogous to [11], for capturing the salient region(s) in the MR image at different levels of resolution.", "probabilities": [0.7003097534179688, 0.16855892539024353, 0.03994438424706459, 0.09118695557117462]}, {"string": "Re-scaling is performed to bring back the saliency maps to the original image size (M   N) using Bilinear interpolation [11].", "probabilities": [0.0854266956448555, 0.009679052047431469, 0.3581852912902832, 0.5467090010643005]}, {"string": "In order to evaluate the clusters identified by HENPC, we performed a comparison with the system HOCCLUS2 [33], which also discovers possibly overlapping and hierarchically organized clusters.", "probabilities": [0.09618627279996872, 0.011316034942865372, 0.3483622074127197, 0.5441354513168335]}, {"string": "In particular, the identified clusters are evaluated in terms of a variant of the Q-Modularity [32], which measures the quality of the clustering with respect to a random clustering.", "probabilities": [0.2111077457666397, 0.06866314262151718, 0.4933832883834839, 0.2268458753824234]}, {"string": "They are: MrSBC[6],99www.di.uniba.it/~ceci/micFiles/systems/MURENA.html.", "probabilities": [0.14373379945755005, 0.025478491559624672, 0.4644562900066376, 0.36633139848709106]}, {"string": "In order to statistically compare HENPC with HOCCLUS2, we performed the Friedman test with the Nemenyi post-hoc tests and, following the suggestions made in [10], we plot the graphs which summarize the results in Fig. 6.", "probabilities": [0.08539163321256638, 0.009680264629423618, 0.3582462966442108, 0.546681821346283]}, {"string": "As regards the parameter  , we follow the recommendations in [33], where a similar merging approach is proposed: We set  = 0.2.", "probabilities": [0.10486485064029694, 0.011494387872517109, 0.33518949151039124, 0.54845130443573]}, {"string": "which exploits the na ve Bayes classification method in the multi-relational setting. RelIBk[48], which is the multi-relational version of the k-NN algorithm and is available in RelWeka.1010kappa.arisco.pl/~adamw/home_page/rel_weka/.", "probabilities": [0.09725843369960785, 0.010907716117799282, 0.3436836302280426, 0.5481502413749695]}, {"string": "This fact enabled Enron to provide water distribution services under the Azuris Corporation brand [48], which is what causes the origin of a sudden concept drift.", "probabilities": [0.8501993417739868, 0.03538836911320686, 0.026220468804240227, 0.08819181472063065]}, {"string": "In fact, as commented in [48] both  reservoir  and  body of water  topics are directly connected because the expansion of Wessex under the administration of Enron through Argentina forced an improvement to the water supply systems (such as water towers or cisterns) to avoid failures.", "probabilities": [0.7739551663398743, 0.05769364908337593, 0.0493067130446434, 0.1190444603562355]}, {"string": "Additionally, it is also very common to use the stratified k-fold cross-validation schemes in the spam filtering domain to ensure the representativeness of each fold when compared to the whole data [25].", "probabilities": [0.5447795987129211, 0.05404642969369888, 0.17540374398231506, 0.2257702350616455]}, {"string": "Specifically related to the evaluation methodology used in the spam filtering domain is the contribution of P rez D az et al. [34].", "probabilities": [0.34373196959495544, 0.026164710521697998, 0.16008011996746063, 0.47002318501472473]}, {"string": "However, the problems associated with this situation have remained untreated until 2005 [10].", "probabilities": [0.8499089479446411, 0.03557375445961952, 0.026280170306563377, 0.0882372111082077]}, {"string": "We experiment on the following five image sets: 1) ZJU aerial image [63], which comprises of 20,946 aerial images from 10 categories.", "probabilities": [0.08540906757116318, 0.009683470241725445, 0.3582283854484558, 0.5466790795326233]}, {"string": "Most of the aerial images are collected from metropolises, such as New York, Tokyo, and Beijing; 2) USC scene [50], which consists of 375 video clips of three USC campus sites, i.e., Ahmanson Center for Biological Science (ACB), Associate and Founders Park (AnF), and Frederick D. Fagg Park (FDF); 3) Scene-15, which contains 4485 scene images from 15 categories.", "probabilities": [0.8293967247009277, 0.03592589497566223, 0.0332949161529541, 0.1013825535774231]}, {"string": "To justify the usefulness of the second component, we compare it with the active learning algorithm proposed by Zhang et al. [60].", "probabilities": [0.5974462628364563, 0.09346041083335876, 0.06026894971728325, 0.24882441759109497]}, {"string": "Further in Table 1, we present the recognition accuracy per category on the ZJU Aerial image [63].", "probabilities": [0.13374492526054382, 0.02211000956594944, 0.4462587535381317, 0.3978862464427948]}, {"string": "Compared to [60], our approach can dynamically tune the importance of different channels.", "probabilities": [0.6657170057296753, 0.1181207001209259, 0.07365996390581131, 0.1425023376941681]}, {"string": "The weighting matrix in [60] is determined by a graphlet and its spatial neighbors inside each scene image.", "probabilities": [0.20248092710971832, 0.06916148215532303, 0.5078108310699463, 0.22054676711559296]}, {"string": "al [33]: The number of generic object detectors is fixed at 200, different regularizers LR1,LRG, and LRG1 were used, and it also has three SPM levels.", "probabilities": [0.09017957746982574, 0.010363863781094551, 0.35275208950042725, 0.5467044711112976]}, {"string": "3) Overfeat-5, an architecture based on the Overfeat paper [47].", "probabilities": [0.1439206749200821, 0.026707837358117104, 0.45272183418273926, 0.3766496181488037]}, {"string": "2) Convnet-5, a modification of Krizhevsky et al.s network [31].", "probabilities": [0.23712487518787384, 0.14640629291534424, 0.4140239953994751, 0.2024448812007904]}, {"string": "Since the ground-truth segments from the LHI [57] are provided, we experiment on this data set.", "probabilities": [0.08542771637439728, 0.009685627184808254, 0.3582049608230591, 0.5466816425323486]}, {"string": "This reveals that 4-sized graphlets are highly representative for describing the local composition of aerial images in [63].", "probabilities": [0.7371199727058411, 0.052718035876750946, 0.070319764316082, 0.13984225690364838]}, {"string": "This work extends our conference papers, [41].", "probabilities": [0.08988778293132782, 0.8602331876754761, 0.02476547844707966, 0.02511357143521309]}, {"string": "A preliminary version of this paper has been published in [48].", "probabilities": [0.7083861231803894, 0.08974990248680115, 0.07002288103103638, 0.13184109330177307]}, {"string": "This paper extends [48] from the following aspects.", "probabilities": [0.12111645936965942, 0.7888868451118469, 0.0506792776286602, 0.039317306131124496]}, {"string": "As described in Section 4.4, one of the recommendation approaches presented in this paper, the global recommendation, is an extension of a recommendation approach developed previously (Hopfgartner et al., 2008).", "probabilities": [0.6364346146583557, 0.24442821741104126, 0.037450291216373444, 0.08168686181306839]}, {"string": "In the experiments, the following two datasets are used:   EIAO dataset: This is a real-world dataset from the European Internet Accessibility Observatory (EIAO) project [3] which developed a tool for performing automatic evaluation of accessibility of web sites.", "probabilities": [0.08541645854711533, 0.009681143797934055, 0.35820868611335754, 0.5466936826705933]}, {"string": "In particular, our formalization of md-schemata is an extension of the one used in [18].", "probabilities": [0.5630689263343811, 0.11171064525842667, 0.06714658439159393, 0.25807371735572815]}, {"string": "This paper extends the work presented in [44].", "probabilities": [0.061494965106248856, 0.9092632532119751, 0.015254493802785873, 0.013987281359732151]}, {"string": "We obtained the data trace from a simulated hospital environment by adapting the simulator in [25].", "probabilities": [0.08583218604326248, 0.009757323190569878, 0.357778936624527, 0.5466315150260925]}, {"string": "Table 5 shows an example triple taken from the LUBM [14] dataset.", "probabilities": [0.21446946263313293, 0.08744580298662186, 0.4810943305492401, 0.2169903665781021]}, {"string": "Values greater than 0.3 are rare in real data [45].", "probabilities": [0.8287468552589417, 0.04915226995944977, 0.030084479600191116, 0.09201644361019135]}, {"string": "Hanley and Hajian-Tilaki [36]", "probabilities": [0.806790292263031, 0.04208209738135338, 0.03693598136305809, 0.11419161409139633]}, {"string": "The steady-state solutions are found using a fully consistent Newton method, applied directly to the Eqs. (10), with the linear systems solved using a direct sparse solver [25].", "probabilities": [0.08666857331991196, 0.009771188721060753, 0.3564744293689728, 0.5470857620239258]}, {"string": "The scheme (10) is implemented in a straight-forward way, and we use Roe s method for the numerical fluxes (5)[33].", "probabilities": [0.08546723425388336, 0.009689508937299252, 0.3581530749797821, 0.5466901063919067]}, {"string": "The node positions xijk are given by some curved mesh generation procedure [21] This allows us to easily compute G(X) at any point X, which will involve the derivatives  i ( ) of the shape functions.", "probabilities": [0.1393127292394638, 0.01239448506385088, 0.3046093285083771, 0.5436835289001465]}, {"string": "For the stabilized scheme (bottom two tables), we obtain a somewhat higher order for the q variables, which could be used as part of a postprocessing step to further increase the order of convergence for the solution u[16].", "probabilities": [0.41691261529922485, 0.02496076002717018, 0.12585200369358063, 0.43227460980415344]}, {"string": "In particular, we use the following L-stable, three-stage, third-order accurate method [26] with s=3 and the coefficients given by the Runge Kutta tableaux below.We also use implicit time-stepping for computing steady-state solutions, by a sequence of increasing timesteps  t and a final step without the time derivatives.", "probabilities": [0.08559565246105194, 0.009698883630335331, 0.3579746186733246, 0.5467307567596436]}, {"string": "We also use this form for implicit time-stepping using Diagonally Implicit Runge Kutta (DIRK) schemes [26].", "probabilities": [0.08537698537111282, 0.009676383808255196, 0.358257532119751, 0.5466891527175903]}, {"string": "In our examples, we solve these equations using a standard sparse direct solver [25].", "probabilities": [0.08764169365167618, 0.009802536107599735, 0.35501742362976074, 0.5475383400917053]}, {"string": "The linear systems are solved using a preconditioned GMRES method [31], with the low-cost sparse block-Jacobi preconditioner A  described above.", "probabilities": [0.08579810708761215, 0.009727747179567814, 0.35773932933807373, 0.5467348694801331]}, {"string": "In our implementation, we store all matrices in a general purpose compressed column storage format [25].", "probabilities": [0.1284751147031784, 0.011901859194040298, 0.3054868280887604, 0.5541361570358276]}, {"string": "When solving the linear systems involving the preconditioning matrix (I- tA ), we use a sparse direct LU-factorization with fill-reducing ordering for each block [25].", "probabilities": [0.08542772382497787, 0.009679224342107773, 0.3581841289997101, 0.546708881855011]}, {"string": "Fig. 3 shows how the research study describes the first research, First step; Physical pulse is defined pulse (Lalita Achanuphab, 2014) that represents the heartbeat rhythm is defined as the number of beats per minute.", "probabilities": [0.47295644879341125, 0.07513339072465897, 0.23514989018440247, 0.21676026284694672]}, {"string": "Pulse (Wannachart Kataichan, 2014) can be explained other words, the shock waves of blood flow caused by the compression of the left ventricular wall of the artery is expanded into a rhythm.", "probabilities": [0.8453572988510132, 0.036393340677022934, 0.027710795402526855, 0.0905386283993721]}, {"string": "In order to know the major problems (health status) with regard to the use of the available furniture, ergonomic assessment (health survey) for students who have been at colleges for longer period of time was performed with the help of designed questionnaires as it was also suggested by [20].", "probabilities": [0.542046308517456, 0.10354195535182953, 0.07426810264587402, 0.280143678188324]}, {"string": "Designing of standard furniture needs directly involvement of anthropometric measurements [33].", "probabilities": [0.8445192575454712, 0.037055108696222305, 0.027750490233302116, 0.09067519009113312]}, {"string": "With the help of literature survey, Ref. [21] encourages designers to adapt  designing for an Adjustable Range , this means that classrooms furniture are required to be adjustable.", "probabilities": [0.8042678833007812, 0.0554581880569458, 0.037377458065748215, 0.10289651900529861]}, {"string": "The presence of less survey regarding anthropometric data has been due to that majority of colleges or universities administration s procure ready-made furniture which mostly fit few users (students) [11].", "probabilities": [0.849972128868103, 0.03556189686059952, 0.026261314749717712, 0.08820465952157974]}, {"string": "  Generating the mapping document according to Kimball s standards (Kimball and Caserta, 2004).", "probabilities": [0.11416088789701462, 0.010994737036526203, 0.32109951972961426, 0.5537447929382324]}, {"string": "  Data warehousing tool (ETL) (Staudt et al., 1999): includes a transformation process where the correspondence between the sources data and the target DW data is defined.", "probabilities": [0.8482811450958252, 0.03577194735407829, 0.026815034449100494, 0.08913195133209229]}, {"string": "The execution time and consumed memory are also calculated in this way by Bergmann et al. (2010).", "probabilities": [0.44537603855133057, 0.027667123824357986, 0.11754462867975235, 0.4094122052192688]}, {"string": "We therefore use Granger causality analysis in a similar fashion to [17]; we are not testing actual causation but whether one time series has predictive information about the other or not.", "probabilities": [0.08548230677843094, 0.009685053490102291, 0.3581145107746124, 0.5467181205749512]}, {"string": "To better address these non-linear effects and assess the contribution that public mood assessments can make in predictive models of DJIA values, we compare the performance of a Self-organizing Fuzzy Neural Network (SOFNN) model [28] that predicts DJIA values on the basis of two sets of inputs: (1) the past 3 days of DJIA values, and (2) the same combined with various permutations of our mood time series (explained below).", "probabilities": [0.15689954161643982, 0.02615351416170597, 0.4633454382419586, 0.3536015748977661]}, {"string": "This paper extends our prior work [22] with a broader point of view in terms of the semantic gap in virtual CPU management.", "probabilities": [0.05586192011833191, 0.9167216420173645, 0.014467590488493443, 0.012948920950293541]}, {"string": "There are two direct observations described in [13] concerning recursive decomposition: (1) data localities extend exponentially as the recursive decomposition of data space goes deeper; and (2) data localities expand exponentially as the dimensionality of data space goes higher.", "probabilities": [0.5928481221199036, 0.24942591786384583, 0.05646468326449394, 0.1012612134218216]}, {"string": "This paper extends the one in [27] and reports on a more in-depth analysis of the proposed cooperative composition, summarized as follows.", "probabilities": [0.059637971222400665, 0.9114792346954346, 0.015155693516135216, 0.013727021403610706]}, {"string": "This paper expands on the work first published as a conference paper at IPDPS [8].", "probabilities": [0.05542314052581787, 0.9173495173454285, 0.014388609677553177, 0.01283881813287735]}, {"string": "We run the experiments on a real world dataset: the Enron Email Dataset [11], which are a total of 200, 399 messages belonging to 158 users.", "probabilities": [0.0858723595738411, 0.00974893569946289, 0.3576869070529938, 0.5466918349266052]}, {"string": "This paper is an extension of our earlier published COMPSAC 2012 conference paper (Guannan Si et al., 2012).", "probabilities": [0.057049624621868134, 0.9146862626075745, 0.014891710132360458, 0.013372421264648438]}, {"string": "This work significantly extends its preliminary version (Jiang and Chan, 2013): (1) It generalizes the family of LBS techniques by presenting five more new techniques and evaluates the family against more existing techniques for benchmarking.", "probabilities": [0.09625684469938278, 0.841867208480835, 0.027045220136642456, 0.034830737859010696]}, {"string": "This paper extends and improves our previous work (Mandreoli et al., 2009) in almost every respect.", "probabilities": [0.09631001949310303, 0.8453871607780457, 0.02977457456290722, 0.028528211638331413]}, {"string": "In the case of provisional applications such as scientific calculations or MapReduce (Dean and Ghemawat, 2008) jobs, high latency also extends the execution time and accordingly increases resource costs.", "probabilities": [0.8502655625343323, 0.03484461456537247, 0.02625386416912079, 0.08863604068756104]}, {"string": "This journal paper is an extended version of our conference paper published in Shatnawi et al. (2015b).", "probabilities": [0.2384263277053833, 0.4172419607639313, 0.2141328752040863, 0.13019880652427673]}, {"string": "Specifically, in the context of Fig. 5 the literature is generally classified into approaches for i) operational ii) evolution specific and iii) development related issues of robotics.2.Thematic Classification extends the generic classification by adding details based on the primary focus of research in a collection of related studies to identify and represent the recurring research themes using thematic analysis (Boyatzis, 1998).", "probabilities": [0.8503611087799072, 0.035160306841135025, 0.026192646473646164, 0.08828598260879517]}, {"string": "The use of FAST as part of the technology foundation (FI-WARE) behind the European public-private partnership on the Internet of the Future (FI-PPP, 2012) extends the life cycle to R&D projects covering areas as far apart as the transportation of goods and people, energy efficiency or bank management in the framework of smart cities.", "probabilities": [0.8451780676841736, 0.03530435636639595, 0.027717221528291702, 0.09180029481649399]}, {"string": "This paper is an extension of a previous communication (Kallel et al., 2015) at ECSA (the European Conference on Software Architecture) 2015.", "probabilities": [0.05537540465593338, 0.9174171090126038, 0.014376685954630375, 0.012830895371735096]}, {"string": "The information that are used by SearchDOMDep (Algorithm 1) is determined by the semantic of the API according to DOM specification (Document Object Model Core, 2017), e.g., input.value is a DNRead instruction performed on DOM element input, and it is unnecessary to search the subtree of input to find dependences.", "probabilities": [0.36770883202552795, 0.018433401361107826, 0.14970701932907104, 0.46415066719055176]}, {"string": "For Rule 2 in Section 3.4, we modify the instrumentation module of Jalangi (Sen et al., 2013) to capture the enter point and the exit point of each branch for the corresponding path condition, we further judge whether there are writing instructions between these two points.", "probabilities": [0.08539026975631714, 0.009680099785327911, 0.3582479953765869, 0.5466816425323486]}, {"string": "To trace dependences dynamically, we incorporate the idea of shadow execution (Sen et al., 2013), in which the analysis can update and access the shadow value of a variable v. The shadow value records the value of def(v).", "probabilities": [0.08563388139009476, 0.009710047394037247, 0.35851824283599854, 0.5461378693580627]}, {"string": "We instrument JavaScript code using Jalangi (Sen et al., 2013) to capture all executed JavaScript (including DOM) instructions.", "probabilities": [0.08536317944526672, 0.00967533327639103, 0.3582766354084015, 0.5466848015785217]}, {"string": "We adopted a similar record-replay mechanism to Mugshot (Mickens et al., 2010).", "probabilities": [0.08546750992536545, 0.009689210914075375, 0.35815179347991943, 0.5466915369033813]}, {"string": "This article extends a preliminary study published as a research paper in a conference (Yang et al., 2016).", "probabilities": [0.0560607947409153, 0.9163395762443542, 0.014583987183868885, 0.01301555335521698]}, {"string": "The main goal of an SLR is to identify, evaluate and interpret the research results related to questions, topic area, or phenomenon and gather evidence to base conclusions (Kitchenham and Charters, 2007).", "probabilities": [0.8435863256454468, 0.03742336481809616, 0.02798641286790371, 0.09100396186113358]}, {"string": "A software tool, named StArt (State of the Art through Systematic Reviews) (LAPES, 2015), was used to support the SLR execution.", "probabilities": [0.27951452136039734, 0.01652269996702671, 0.1926298439502716, 0.5113329291343689]}, {"string": "Our snowballing process is inspired by Wohlin''s guidelines (Wohlin, 2014).", "probabilities": [0.43643999099731445, 0.030014293268322945, 0.11848320066928864, 0.4150625169277191]}, {"string": "After selection and quality analysis, we performed data extraction of selected papers based on Kitchenham and Charters (2007).", "probabilities": [0.08537784218788147, 0.00967799685895443, 0.35826143622398376, 0.5466827750205994]}, {"string": "Considering the way new concepts are proposed, an extension can be developed using a light-weight or heavy-weight (Miles and Hamilton, 2006) strategy.", "probabilities": [0.7159801125526428, 0.07178498804569244, 0.07057993859052658, 0.14165492355823517]}, {"string": "A manual search was realised in seven editions of the International iStar workshop (2008, 2010, 2011, 2013, 2014, 2015 and 2016)44http://www.cin.ufpe.br/ istar08/site/.http://sunsite.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-586/http://www.cin.ufpe.br/ istar11/arquivos/iStar11-proceedings.pdfhttp://ceur-ws.org/Vol-978/http://ceur-ws.org/Vol-1157/http://ceur-ws.org/Vol-1674/ and in the book Social Modelling for Requirements Engineering (OMG Unified Modelling Language 2011).", "probabilities": [0.815811038017273, 0.03552207723259926, 0.038179103285074234, 0.11048778891563416]}, {"string": "Extending a modelling language is to add new constructs (Brambilla et al., 2012).", "probabilities": [0.5614380240440369, 0.12867054343223572, 0.13688509166240692, 0.1730063408613205]}, {"string": "The research questions RQ3, RQ6 and RQ8 were inspired on the notational system, as defined in semiotic clarity (Goodman, 1968) (See Section 2.3).", "probabilities": [0.8482227921485901, 0.034509964287281036, 0.0268870759755373, 0.09038017690181732]}, {"string": "The search period starts in 1990 when iStar was proposed by Eric Yu in his thesis (Yu, 1997).", "probabilities": [0.8386660814285278, 0.04053739458322525, 0.028961267322301865, 0.09183530509471893]}, {"string": "Next, we identified the duplicated papers with the help of StArt (LAPES December 2015), this tool detects the similarity level between the papers and it facilitates the repeated identification.", "probabilities": [0.13177639245986938, 0.02084299549460411, 0.43833568692207336, 0.40904492139816284]}, {"string": "The joint use of iStar with other existing modelling frameworks is discussed in Franch et al. (2011).", "probabilities": [0.5518536567687988, 0.026599420234560966, 0.08271270990371704, 0.338834285736084]}, {"string": "StArt has been empirically evaluated, and it was demonstrated that such tool had positive results in the execution of SLRs (Hernandes et al., 2012).", "probabilities": [0.637662947177887, 0.09928695857524872, 0.10173779726028442, 0.1613122820854187]}, {"string": "The coordination engine supports the behavioral coordination of heterogeneous models, based on coordination patterns defined using BCOoL (Vara Larsen et al., 2015).", "probabilities": [0.08559489250183105, 0.009692150168120861, 0.35795432329177856, 0.5467585921287537]}, {"string": "The xMOF engine supports the execution of operational semantics defined with xMOF (Mayerhofer et al., 2013).", "probabilities": [0.10023462027311325, 0.010813967324793339, 0.3496149480342865, 0.5393364429473877]}, {"string": "This framework was previously presented in detail in Bousse et al. (2016), and we summarize thereafter its main aspects and features.", "probabilities": [0.7571222186088562, 0.02272188849747181, 0.03749291971325874, 0.18266311287879944]}, {"string": "The present article extends the work of Anjos et al. [12] by identifying discriminant motion cues for the detection of both photo and video attacks under various spoofing scenarios.", "probabilities": [0.21338269114494324, 0.7189332842826843, 0.029323959723114967, 0.03836001828312874]}, {"string": "In this section we present the global activity recognition model, which extends our research conducted in [8].", "probabilities": [0.09572435170412064, 0.8646154403686523, 0.019520621746778488, 0.02013954147696495]}, {"string": "This paper extends our previous work [21].", "probabilities": [0.08527635037899017, 0.8678586483001709, 0.023331154137849808, 0.02353380061686039]}, {"string": "Finally, we presented the first application of a 3D fully connected CRF on medical data, employed as a post-processing step to refine the network s output, a method that has also been shown promising for processing 2D natural images (Chen et al., 2014).", "probabilities": [0.4535383880138397, 0.030458534136414528, 0.11225330084562302, 0.4037497639656067]}, {"string": "To better preserve the signal in the initial training stage we adopt a scheme recently derived for ReLu-based networks by He et al. (2015) and initialize the kernel weights of our system by sampling from the normal distribution N(0,2/nlin).", "probabilities": [0.09088321775197983, 0.010089591145515442, 0.3508378267288208, 0.5481893420219421]}, {"string": "A phenomenon of similar nature that hinders the network s performance is the  internal covariate shift  (Ioffe and Szegedy, 2015).", "probabilities": [0.8468273282051086, 0.03765051066875458, 0.026798633858561516, 0.08872352540493011]}, {"string": "They all use the PReLu non-linearity (He et al., 2015).", "probabilities": [0.08589062839746475, 0.009743039496243, 0.35877296328544617, 0.5455934405326843]}, {"string": "We participated in the 2015 Ischemic Stroke Lesion Segmentation (ISLES) challenge, where our system achieved the best results among all participants on sub-acute ischemic stroke lesions (Maier et al., 2017).", "probabilities": [0.16280928254127502, 0.036028262227773666, 0.48488447070121765, 0.31627798080444336]}, {"string": "For countering it, we adopt the recently proposed Batch Normalisation (BN) technique to all hidden layers (Ioffe and Szegedy, 2015), which allows normalization of the FM activations at every optimization step in order to better preserve the signal.", "probabilities": [0.08570004254579544, 0.009693251922726631, 0.35778290033340454, 0.5468237996101379]}, {"string": "Its size,  l, increases at each subsequent layer l and is given by the 3-dimensional vector: ), which is unwanted behaviour for accurate segmentation.", "probabilities": [0.31110432744026184, 0.15731436014175415, 0.3285462260246277, 0.20303504168987274]}, {"string": "Brain masks were obtained using the ROBEX tool (Iglesias et al., 2011).", "probabilities": [0.08537500351667404, 0.009675930254161358, 0.35825926065444946, 0.5466898083686829]}, {"string": "Kr henb hl and Koltun (2011) observed that if the penalty function is defined as a linear combination of Gaussian kernels, k(fi,fj)= m=1Mw(m)k(m)(fi,fj), the model lends itself for very efficient inference with mean field approximation, after expressing message passing as convolutions with the Gaussian kernels in the space of the feature vectors fi, fj.", "probabilities": [0.8377561569213867, 0.038859933614730835, 0.029681947082281113, 0.09370193630456924]}, {"string": "We employ a fully connected CRF (Kr henb hl and Koltun, 2011) as a post-processing step to achieve more structured predictions.", "probabilities": [0.08536586910486221, 0.009675472974777222, 0.3582727015018463, 0.5466859340667725]}, {"string": "For brain tumours, we evaluate our system on the data from the 2015 Brain Tumour Segmentation Challenge (BRATS) (Menze et al., 2015).", "probabilities": [0.09488851577043533, 0.010655797086656094, 0.3463817238807678, 0.5480740070343018]}, {"string": "Deeper models (and the  Shallow+  model in Section 3.3) use the weight initialisation scheme of He et al. (2015).", "probabilities": [0.42670899629592896, 0.02341984026134014, 0.12530377507209778, 0.42456740140914917]}, {"string": "As a comparison, we improved over the reported performance of the pipeline in Rao et al. (2014).", "probabilities": [0.5328322052955627, 0.25788235664367676, 0.08133343607187271, 0.12795193493366241]}, {"string": "This paper significantly extends our previous workshop paper (Wu et al., 2016) in the following aspects.", "probabilities": [0.07745031267404556, 0.8796817064285278, 0.022158727049827576, 0.020709192380309105]}, {"string": "This paper significantly extends our previous workshop paper (Wu et al., 2016) in aspects of both technique and application.", "probabilities": [0.08702187240123749, 0.8634330034255981, 0.025226525962352753, 0.024318695068359375]}, {"string": "Most recently, Cooray et al. [17] proposed an interactive and multi-level framework for generating personalized summaries by giving users the flexibility to select a summarization criterion and specify the summary length.", "probabilities": [0.8417680859565735, 0.037621792405843735, 0.028618542477488518, 0.09199156612157822]}, {"string": "The hierarchical sparse coding is utilized in another research [116] to learn features for images in an unsupervised fashion.", "probabilities": [0.5900903940200806, 0.16465148329734802, 0.09697034955024719, 0.148287832736969]}, {"string": "The autoencoder is a special type of artificial neural network used for learning efficient encodings [75].", "probabilities": [0.1688775271177292, 0.014428384602069855, 0.2766959071159363, 0.5399981737136841]}, {"string": "A quite well-known variant of the sparse autoencoder is a nine-layer locally connected sparse autoencoder with pooling and local contrast normalization [84].", "probabilities": [0.20473575592041016, 0.06945345550775528, 0.5048370361328125, 0.22097375988960266]}, {"string": "Weight decay works by adding an extra term to the cost function to penalize the parameters, preventing them from exactly modeling the training data and therefore helping to generalize to new examples [6].", "probabilities": [0.7078670263290405, 0.07510612905025482, 0.07336918264627457, 0.14365766942501068]}, {"string": "When training the network, a DBM would jointly train all layers of a specific unsupervised model, and instead of maximizing the likelihood directly, the DBM uses a stochastic maximum likelihood (SML) [161] based algorithm to maximize the lower bound on the likelihood, i.e. performing only one or a few updates using a Markov chain Monte Carlo (MCMC) method between each parameter update.", "probabilities": [0.6666094660758972, 0.021512100473046303, 0.05117419734597206, 0.26070427894592285]}, {"string": "The well-known AlexNet [6] employed two distinct forms of data augmentation: the first form of data augmentation consists of generating image translations and horizontal reflections, and the second form consists of altering the intensities of the RGB channels in training images.", "probabilities": [0.5229050517082214, 0.04136306419968605, 0.0891447588801384, 0.3465871512889862]}, {"string": "The purpose of sparse coding is to learn an over-complete set of basic functions to describe the input data [94].", "probabilities": [0.21624425053596497, 0.07393356412649155, 0.48798972368240356, 0.22183246910572052]}, {"string": "One intuitive idea is to improve the performance of CNNs by increasing their size, which includes increasing the depth (the number of levels) and the width (the number of units at each level) [20].", "probabilities": [0.6992584466934204, 0.05779222398996353, 0.08851657062768936, 0.1544327288866043]}, {"string": "Gao et al. [114] further raised a Hyper-graph Laplacian Sparse Coding (HLSC) method, which extends the LSC to the case where the similarity among the instances is defined by a hyper graph.", "probabilities": [0.6835557222366333, 0.04319300875067711, 0.049625106155872345, 0.22362622618675232]}, {"string": "Weight tying allows models to learn good representations of the input data by reducing the number of parameters in Convolutional Neural Networks [160].", "probabilities": [0.8287453651428223, 0.03181428834795952, 0.032875627279281616, 0.10656470060348511]}, {"string": "The network was trained on ImageNet and integrated various regularization techniques, such as data augmentation, dropout, etc. AlexNet won the ILSVRC2012 competition [189], and set the tone for the surge of interest in deep convolutional neural network architectures.", "probabilities": [0.16348788142204285, 0.016540924087166786, 0.2726909816265106, 0.5472801923751831]}, {"string": "A popular algorithm for sparse coding inference is the Iterative Shrinkage-Thresholding Algorithm (ISTA) [107], which takes a gradient step to optimize the reconstruction term, followed by a sparsity term which has a closed form shrinkage operation.", "probabilities": [0.08755531907081604, 0.009808285161852837, 0.3554249405860901, 0.5472114682197571]}, {"string": "However, we cannot conclude that the representational performance of a CNN rivals that of the brain [123].", "probabilities": [0.8004257678985596, 0.04979650303721428, 0.04062779247760773, 0.10914992541074753]}, {"string": "To handle multiple instances of the same object in the image, DeepMultiBox [147] also showed a saliency-inspired neural network model.", "probabilities": [0.0954606831073761, 0.010861159302294254, 0.3637247681617737, 0.5299534201622009]}, {"string": "Because sizes of images from this dataset are different from each, we extract bag-of-words [42], local binary patterns, edge direction histogram as 3 views.", "probabilities": [0.20208331942558289, 0.06876961141824722, 0.5084295868873596, 0.22071750462055206]}, {"string": "We train BING on the PASCAL VOC dataset [46].", "probabilities": [0.08536949753761292, 0.00967646948993206, 0.3582700788974762, 0.5466840267181396]}, {"string": "INRIA pedestrian dataset [4] is among the most popular and oldest datasets for pedestrian detection.", "probabilities": [0.22118651866912842, 0.09709567576646805, 0.4668401777744293, 0.2148776650428772]}, {"string": "Collected from a vehicle driving through streets in an urban region, the Caltech dataset [44] consists of nearly ten hours of videos.", "probabilities": [0.2553110420703888, 0.09831411391496658, 0.42706185579299927, 0.21931292116641998]}, {"string": "The HTM implementation and documentation are available as open-source.1 In Section 3 we review the Numenta Anomaly Benchmark (NAB) [2], a rigorous benchmark dataset and scoring methodology we created for evaluating real-time anomaly detection algorithms.", "probabilities": [0.09156114608049393, 0.010669915936887264, 0.36939576268196106, 0.5283732414245605]}, {"string": "In our implementation we use the standard HTM system [49] and a standard set of parameters (See Supplementary Section S3 for the complete list).", "probabilities": [0.08587783575057983, 0.009725863113999367, 0.35760271549224854, 0.546793520450592]}, {"string": "Data-intensive science simultaneously derives from and creates the need for large quantities of data [1].", "probabilities": [0.8449124097824097, 0.036501623690128326, 0.027835827320814133, 0.09075014293193817]}, {"string": "This paper complements and extends our previous work described in [10].", "probabilities": [0.09793975949287415, 0.8428653478622437, 0.030066274106502533, 0.029128646478056908]}, {"string": "This paper is an extended version of our previous work in [21].", "probabilities": [0.05585687234997749, 0.9166458249092102, 0.01451538410037756, 0.012981901876628399]}, {"string": "This paper complements and extends previous work presented in [5] mainly in the following aspects: (i) a thorough elaboration is provided in support of the formulation of the optimization problem, choice of the kernel, the setting of the parameters and implementation details, (ii) additional qualitative examples are provided from training to evaluation and (iii) the proposed approach is effectively employed to improve the performance of non-rigid 3D shape retrieval in several standard benchmarks.", "probabilities": [0.14862926304340363, 0.7020301818847656, 0.04984527453780174, 0.09949515759944916]}, {"string": "We collected symbol occurrence and co-occurrence rates from the Infty project corpus [15] as well as from the sources of University of Waterloo course notes for introductory algebra and calculus courses.", "probabilities": [0.08543291687965393, 0.009687448851764202, 0.3582674562931061, 0.5466122031211853]}, {"string": "The UCF50 dataset was introduced in [32], consists of 50 sport action categories and all the videos denoting the actions were collected from YouTube.", "probabilities": [0.20311129093170166, 0.06569013744592667, 0.5027539134025574, 0.2284446358680725]}, {"string": "Following [34], we conduct experiments on this dataset using a four-fold cross validation protocol.", "probabilities": [0.08556091040372849, 0.009710422717034817, 0.3580683469772339, 0.546660304069519]}, {"string": "We also experiment with 32-bit data-driven attributes (DDA) learned from the training data by [37].", "probabilities": [0.08540771156549454, 0.009681693278253078, 0.3582247793674469, 0.5466858148574829]}, {"string": "Following [51], we conduct experiments on this dataset using a four-fold cross validation protocol.", "probabilities": [0.08553466945886612, 0.00970564316958189, 0.35809558629989624, 0.5466641187667847]}, {"string": "Dataset1 for makeups and non-makeups: To qualitatively evaluate our method, we use the dataset collected in [1].", "probabilities": [0.0853658989071846, 0.009675937704741955, 0.3582741618156433, 0.5466840267181396]}, {"string": "The MSRA10K dataset [53] is composed of 10,000 images randomly selected from the MSRA dataset.", "probabilities": [0.0853802040219307, 0.009678157977759838, 0.35829463601112366, 0.5466470122337341]}, {"string": "The DUT-OMRON dataset [17] contains 5168 high quality images manually selected from more than 140,000 images.", "probabilities": [0.2033061385154724, 0.06920851022005081, 0.5067522525787354, 0.22073306143283844]}, {"string": "The ECSSD dataset [28] consists of 1000 images along with pixel-wise ground truth.", "probabilities": [0.15126070380210876, 0.02988334931433201, 0.4718814492225647, 0.34697452187538147]}, {"string": "SingleV1, SingleV2: We run spectral clustering [52] on the two views under the condition that all views have complete data examples.", "probabilities": [0.2021539956331253, 0.06899731606245041, 0.5083163976669312, 0.22053232789039612]}, {"string": "CIFAR10 is a subset of the 80M tiny images [76] which consists of 60,000 32   32 color images from 10 classes (e.g., airplane, automobile, bird, ship, etc.).", "probabilities": [0.20236074924468994, 0.06902098655700684, 0.5080488324165344, 0.2205694317817688]}, {"string": "In our experiments, the bootstrap set is formed by the frontal images from 12 identities under 20 lighting conditions from session one in MultiPIE [2] database.", "probabilities": [0.085657998919487, 0.009701043367385864, 0.35791489481925964, 0.5467260479927063]}, {"string": "Some state-of-the-art metric learning based methods are introduced as following:1.RDC[33] is a typical metric learning based method in early researches for person re-identification problem.", "probabilities": [0.7139137387275696, 0.019978215917944908, 0.04080574959516525, 0.22530221939086914]}, {"string": "This method learning a Mahalanobis distance for person re-identification problem based on a distribution hypothesis which assumes the difference vector of positive sample pairs and positive sample pairs features are following two different Gaussian distribution.4.LMNN[39] is a classic metric learning based method which learns a Mahalanobis distance for classification problems.", "probabilities": [0.1890576034784317, 0.05516238138079643, 0.2608131170272827, 0.4949668347835541]}, {"string": "Li et al. [27] proposed a filter pairing neural network for person re-identification, while the parameter learning of deep network needs large sample size, Li et al. built the largest dataset for person re-identification in their work.", "probabilities": [0.7024604082107544, 0.1180569976568222, 0.06059793010354042, 0.11888475716114044]}, {"string": "Dong et al. [21] designed a novel appearance model based on pictorial structures information of human body.", "probabilities": [0.7676642537117004, 0.06099984422326088, 0.052103281021118164, 0.11923269182443619]}, {"string": "Liao et al. [7] proposed the LOMO feature which combined the color histogram features with the texture histogram features of different scales.", "probabilities": [0.7923691272735596, 0.05034380778670311, 0.04474375769495964, 0.11254329234361649]}, {"string": "Li et al. [17] proposed the Locally Adaptive Decision Functions (LADF) approach, which could be viewed as a joint model of a distance metric and a locally adapted thresholding rule.", "probabilities": [0.8450206518173218, 0.038088109344244, 0.027280069887638092, 0.08961114287376404]}, {"string": "The Cross-view Quadratic Discriminant Analysis (XQDA) [12] metric algorithm was an extension of KISSME by learning a dimension reduction projection and distance metric simultaneously.", "probabilities": [0.16799643635749817, 0.03816406801342964, 0.4866427779197693, 0.3071966767311096]}, {"string": "For example, the Probabilistic Relative Distance Comparison (PRDC) model was presented in [13] to maximize the probability of a pair of true match having a smaller distance than that of a wrong matched pair.", "probabilities": [0.7122051119804382, 0.019605571404099464, 0.04118558391928673, 0.22700372338294983]}, {"string": "Koestinger et al. [16] formulated the KISSME algorithm by computing the difference between the within-class and between-class covariance metrics.", "probabilities": [0.4830075204372406, 0.021455038338899612, 0.10514223575592041, 0.39039522409439087]}, {"string": "In [41] multiple local metrics are learned and then combined with a global metric to avoid overfitting.", "probabilities": [0.2046821564435959, 0.07075276225805283, 0.5040807127952576, 0.22048434615135193]}, {"string": "Zhang et al. [42] proposed to learn a classifier specifically for each pedestrian such that the matching model is highly tuned to the individual appearance.", "probabilities": [0.8171386122703552, 0.061518847942352295, 0.0305680800229311, 0.09077440947294235]}, {"string": "For example, the Ensemble of Localized Features (ELF) [5] was proposed to against viewpoint variants.", "probabilities": [0.793247401714325, 0.04458145797252655, 0.04624123498797417, 0.11592992395162582]}, {"string": "The CUHK01 dataset [62] is a multi-shot person re-identification dataset recorded by two different cameras in a campus environment.", "probabilities": [0.2026963084936142, 0.06906912475824356, 0.5076016783714294, 0.22063292562961578]}, {"string": "Generally, the person re-identification system [4] can be divided into four steps: pedestrian detection, feature extraction, feature transform and similarity estimation, which can be seen in Fig. 1.", "probabilities": [0.30091413855552673, 0.06018747761845589, 0.3441685140132904, 0.2947298288345337]}, {"string": "According to the experimental settings in [62], we normalize all the images to 160   60 pixels and randomly select 485 image pairs for training and take the left for testing.", "probabilities": [0.20296920835971832, 0.06886732578277588, 0.5070692300796509, 0.22109422087669373]}, {"string": "Specifically, for the LOMO feature, we extract the HSV joint histogram and the SILTP [51] histogram with three scales to describe the pedestrian appearance.", "probabilities": [0.08541101217269897, 0.009681988507509232, 0.35822030901908875, 0.5466866493225098]}, {"string": "While for the GOG feature, we follow the process in [32].", "probabilities": [0.08537130057811737, 0.0096761304885149, 0.3582737147808075, 0.5466787815093994]}, {"string": "The standard Cumulative Match Characteristic (CMC) [45] curve is employed to compare the performance of the proposed algorithm with state-of-the-art methods.", "probabilities": [0.09621133655309677, 0.010349761694669724, 0.34362924098968506, 0.5498096346855164]}, {"string": "The VIPeR dataset [45] contains 632 pedestrian image pairs captured from two disjoint camera views in outdoor scenarios.", "probabilities": [0.20877815783023834, 0.06956890225410461, 0.49972590804100037, 0.22192703187465668]}, {"string": "Zheng et al. [30] utilized the Bag-of-Words(BOW) technique to aggregate the 11-dim color names descriptor for each local patch.", "probabilities": [0.38695111870765686, 0.019941305741667747, 0.1414424180984497, 0.45166510343551636]}, {"string": "Based on individual body configuration, Farenzena et al. [6] formulated the Symmetry-Driven Accumulation of Local Features (SDALF) descriptor, where the weighted HSV histogram, the Maximally Stable Color Regions (MSCR) and the Recurrent Highly Structured Patches (RHSP) were combined to provide complementary information.", "probabilities": [0.08701305091381073, 0.009795492514967918, 0.3567737340927124, 0.546417772769928]}, {"string": "In [24], multiple linear projections were learned to transform the partitioned samples into different feature subspaces according to the feature similarity.", "probabilities": [0.08728916943073273, 0.009987703524529934, 0.36148151755332947, 0.5412415862083435]}, {"string": "Motivated by the appearance structure of person images, Matsukawa et al. [32] proposed the Gaussian of Gaussian (GOG) descriptor to describe color and texture cues.", "probabilities": [0.7037853598594666, 0.020287467166781425, 0.04333117976784706, 0.23259595036506653]}, {"string": "The PRID 450S dataset [47] contains 450 single-shot image pairs depicting walking humans recorded from two spatially disjoint camera views.", "probabilities": [0.20649392902851105, 0.06946177035570145, 0.5026584267616272, 0.22138579189777374]}, {"string": "The experiments were performed using a dataset of 804 images selected from the EDRA atlas [5].", "probabilities": [0.08537781238555908, 0.00967741385102272, 0.35825955867767334, 0.5466852784156799]}, {"string": "All images come from the tiny image set, Holidays image set and Flickr [63].", "probabilities": [0.20187906920909882, 0.06819074600934982, 0.5087951421737671, 0.22113503515720367]}, {"string": "SIFT: The dataset is composed of three image sets: a learning set with 100K 128-dimenional SIFT descriptors extracted from Flickr images, a base set and a query set respectively containing 1M descriptors and 10K descriptors from the INRIA Holidays images [63].", "probabilities": [0.08536725491285324, 0.009675987996160984, 0.3582761287689209, 0.5466806292533875]}, {"string": "Two different neighborhood sizes (k and kmax) are required in [38], separating different parts of the ranked retrieval list with different distance measures.", "probabilities": [0.20235516130924225, 0.06914792209863663, 0.5079755187034607, 0.22052133083343506]}, {"string": "Due to the small number of images per class, the UKBench [50] dataset is very challenger for unsupervised manifold learning and post-processing methods.", "probabilities": [0.27670395374298096, 0.07351992279291153, 0.41844305396080017, 0.23133310675621033]}, {"string": "The results are described as following: Shape retrieval: The experiments for shape retrieval considering the MPEG-7 [45] dataset and six different descriptors are presented in Table 3.", "probabilities": [0.08538490533828735, 0.00967862643301487, 0.3582540452480316, 0.5466823577880859]}, {"string": "Table 10 shows the MAP scores obtained on the Holidays [49] dataset, in comparison with various recent image retrieval approaches.", "probabilities": [0.15444347262382507, 0.029101165011525154, 0.46979543566703796, 0.34665995836257935]}, {"string": "Mainly due to this difficulties, the research focus was gradually shifted from designing low-level features to aspects related to higher level aspects [9].", "probabilities": [0.8429569005966187, 0.03918597847223282, 0.02772790752351284, 0.09012916684150696]}, {"string": "Table 11 presents the results obtained by the proposed manifold learning algorithm results on the UKBench [50] dataset.", "probabilities": [0.08806946128606796, 0.010160996578633785, 0.35556089878082275, 0.5462086796760559]}, {"string": "Each point represent one image of MPEG-7 [45] dataset.", "probabilities": [0.20216424763202667, 0.06902741640806198, 0.5082885026931763, 0.22051987051963806]}, {"string": "All images are considered as query images for most of datasets, except for Holidays [49], which uses 500 queries for comparison purposes.", "probabilities": [0.08766557276248932, 0.009809629991650581, 0.3555067181587219, 0.5470180511474609]}, {"string": "The experiments conducted on the Holidays [49] dataset considered analogous conditions.", "probabilities": [0.6072983145713806, 0.06883824616670609, 0.06125238910317421, 0.26261106133461]}, {"string": "Table 6 presents the results obtained by the proposed method on UKBench [50] dataset.", "probabilities": [0.0862998366355896, 0.009844471700489521, 0.35730037093162537, 0.5465553402900696]}, {"string": "Table 3 presents the experimental results, where positive gains were obtained for all color descriptors, ranging from +8.25% to +20.71%. Texture retrieval:Table 3 also presents the effectiveness results obtained for three different texture descriptors on the Brodatz [47] dataset.", "probabilities": [0.2014455795288086, 0.06671450287103653, 0.5081883668899536, 0.22365158796310425]}, {"string": "Experiments conducted for object retrieval tasks considered four color descriptors on the ETH-80 [48] dataset.", "probabilities": [0.09181053191423416, 0.010612131096422672, 0.35321828722953796, 0.5443590879440308]}, {"string": "Table 9 presents the results on the MPEG-7 [45] dataset in comparison with several state-of-the-art post-processing methods.", "probabilities": [0.1871459037065506, 0.6352401375770569, 0.09459850192070007, 0.08301547914743423]}, {"string": "Table 7 presents the results on the Holidays [49] dataset.", "probabilities": [0.2081204205751419, 0.07717811316251755, 0.49573367834091187, 0.21896781027317047]}, {"string": "The UKBench [50] dataset is often used as benchmark for both general retrieval approaches and unsupervised post-processing methods.", "probabilities": [0.09450691193342209, 0.010434475727379322, 0.34638211131095886, 0.5486765503883362]}, {"string": "The Graph Fusion [23], which is a recent and relevant unsupervised related method, is used as baseline.", "probabilities": [0.08586271107196808, 0.009730764664709568, 0.357644259929657, 0.5467623472213745]}, {"string": "Instead of local descriptors, we used other CNN feature: Overfeat [4].", "probabilities": [0.08536599576473236, 0.009675693698227406, 0.3582732677459717, 0.5466850996017456]}, {"string": "And to our best knowledge, there is the only work [28] with regard to cross modal similarity metric learning leveraging active sampling technique.", "probabilities": [0.7826706767082214, 0.0898427814245224, 0.034536391496658325, 0.09295011311769485]}, {"string": "Zhen et al. [28] present a probabilistic nonparametric Bayesian framework with weak supervision for learning cross modal similarities while we learn cross modal similarity exploiting both the inter-modal and intra-modal similarities in a shared subspace using pairwise correlation.", "probabilities": [0.7240228056907654, 0.07110414654016495, 0.06853226572275162, 0.13634081184864044]}, {"string": "The method presented in [24] analyzed several active sample selection strategies in terms of exploration and exploitation trade-offs.", "probabilities": [0.6134949922561646, 0.06830094009637833, 0.05942974239587784, 0.2587743103504181]}, {"string": "In [27], an information-theoretic criterion was presented to select the relative constraints which lead to the highest expected gain in information about the classes of examples.", "probabilities": [0.6044762134552002, 0.1828072965145111, 0.08326873183250427, 0.12944777309894562]}, {"string": "Note that the key difference between the above framework and the method in [34] are that we additionally consider the intra-modal similarity and their method does not utilize active sampling technique to reduce the labeling cost.", "probabilities": [0.6262968182563782, 0.07051697373390198, 0.05565895885229111, 0.2475273311138153]}, {"string": "Under this setting, Kang et al. [34] proposed a heterogeneous similarity learning algorithm by fitting the observed constraints with a nuclear norm penalty, where the similarity between two heterogeneous objects xi and zj is defined as SM(xi,zj)=xiTMzj.", "probabilities": [0.20311082899570465, 0.06032115966081619, 0.49462392926216125, 0.2419440746307373]}, {"string": "With the definition above, we follow the method in [34] where the nuclear norm ||M||* as the regularization term tries to discover the structures between two modalities, and wij is the weight of pair (oi, oj), which balances the effect of positive and negative pairs.", "probabilities": [0.10958300530910492, 0.011844445951282978, 0.32871919870376587, 0.5498533844947815]}, {"string": "Active learning is a leading approach to reduce the labeling cost by actively querying the most important supervised information from the oracle [18].", "probabilities": [0.7875179648399353, 0.04810292273759842, 0.04724741727113724, 0.1171317994594574]}, {"string": "For instance, Liong et al. [29] proposed a new deep coupled metric learning method for cross modal matching, which develops two sets of hierarchical nonlinear transformations by feedforward neural networks.", "probabilities": [0.8348124623298645, 0.03060067631304264, 0.03039172850549221, 0.10419513285160065]}, {"string": "Two models applicable to cross modal retrieval were proposed in [13] based on low-level correlations and semantic abstraction matching.", "probabilities": [0.34595146775245667, 0.01822211593389511, 0.15944349765777588, 0.47638294100761414]}, {"string": "In [33], the correlation between multi-modal data were captured in terms of their sharing hidden variables and discriminative ranking functions.", "probabilities": [0.2456686943769455, 0.16317708790302277, 0.3927712142467499, 0.19838300347328186]}, {"string": "In [30], the distance metric is learned for addressing the text line segmentation problem.", "probabilities": [0.26932212710380554, 0.17477849125862122, 0.35927850008010864, 0.19662092626094818]}, {"string": "In [31], an adaptive Semi-supervised Clustering Kernel Method based on Metric learning (SCKMM) is proposed to solve some problems in semi-supervised clustering, e.g. violation problem of pairwise constraints.", "probabilities": [0.6189337372779846, 0.07354360073804855, 0.1274181306362152, 0.18010447919368744]}, {"string": "For instance, Yang et al. [28] propose a boosting framework for preserving both visual and semantic similarities for image retrieval systems.", "probabilities": [0.8497575521469116, 0.03562192618846893, 0.026318637654185295, 0.08830197900533676]}, {"string": "Yeung and Chang [29] propose an extension of RCA using both positive and negative equivalent constraints.", "probabilities": [0.8477134704589844, 0.033915530890226364, 0.027147047221660614, 0.0912238284945488]}, {"string": "In [32], the distance metric is learned based on the commute time.", "probabilities": [0.2070464938879013, 0.07220645993947983, 0.5002471804618835, 0.2204999029636383]}, {"string": "Zhao et al. [19] explore the possibility of using a geometric space embedding to learn the random-walk distances, including: 1) hitting time; 2) commute time; 3) personalized PageRank.", "probabilities": [0.20853754878044128, 0.06824561208486557, 0.5002426505088806, 0.22297415137290955]}, {"string": "More recently, Tao et al. [26] propose the Manifold Ranking-Based Matrix Factorization (MRMF) model focusing on the feature information to solve the saliency detection problem.", "probabilities": [0.8092741370201111, 0.026996450498700142, 0.03304570913314819, 0.13068373501300812]}, {"string": "Another work Dual-Regularized KISS (DR-KISS) [27] improves the accuracy of person re-identification problem by developing a more discriminative distance metric learning.", "probabilities": [0.6411899924278259, 0.06705702841281891, 0.11891528964042664, 0.17283761501312256]}, {"string": "Another research work that takes into account the metric learning is the distance dynamics [13], which regards the given graph as a dynamic system where the interaction strength of the nodes changes gradually, leading to more discriminative metric.", "probabilities": [0.7118114829063416, 0.07624123990535736, 0.07389682531356812, 0.13805048167705536]}, {"string": "As the rapid development of the web technology, in particular in the social network, there emerge some multi-view graphs [14].", "probabilities": [0.8445457816123962, 0.036757443100214005, 0.027852807193994522, 0.09084396809339523]}, {"string": "It is possible for a user to behave slightly differently in various views and the social behaviours in different views may influence each other, which means that the inter-view coupling should be taken into account for discovering the relationship among nodes in the same view [15].", "probabilities": [0.8129444718360901, 0.0454762801527977, 0.03696603700518608, 0.10461325198411942]}, {"string": "Before performing the update of the relation metric, some initializations are required to transform the original graph into the initial metric space based on the Jaccard similarity [33].", "probabilities": [0.09575818479061127, 0.010190501809120178, 0.34380829334259033, 0.5502430200576782]}, {"string": "In [42], Vapnik further verified that local learning based approaches usually lead to lower empirical errors than global ones.", "probabilities": [0.6606600880622864, 0.09747768938541412, 0.08969293534755707, 0.15216925740242004]}, {"string": "For instance, Bronstein et al. proposed a cross-modal similar sensitive hashing (CMSSH) [5] algorithm based on a standard boosting framework.", "probabilities": [0.8348833322525024, 0.03776850923895836, 0.031179239973425865, 0.09616894274950027]}, {"string": "This paper is an extension of our previous work [20].", "probabilities": [0.0988994613289833, 0.8402777314186096, 0.030421191826462746, 0.030401527881622314]}, {"string": "Porway et al. [37] combined color and edge features with object-level features in a hierarchical contextual model for geospatial image annotation.", "probabilities": [0.8138377070426941, 0.04247919097542763, 0.038190267980098724, 0.10549283027648926]}, {"string": "Kembhavi et al. [40] used multi-scale HOG features computed to annotate vehicles in San Francisco images from Google Earth, and showed HOG to outperform SIFT in complex city environments.", "probabilities": [0.6540654897689819, 0.02260870486497879, 0.054565828293561935, 0.2687600553035736]}, {"string": "Grabner et al. [23] used boosting methods based on LBP and HOG to detect vehicles.", "probabilities": [0.6245604753494263, 0.07183155417442322, 0.05594022199511528, 0.2476678192615509]}, {"string": "Markususe et al. [38] applied AdaBoost classifications based on Haar, and Textons features for semantic labelling on the ISPRS benchmark.", "probabilities": [0.46763578057289124, 0.08110622316598892, 0.10509051382541656, 0.3461673855781555]}, {"string": "Chen et al. [41] relied on stacked autoencoders, trained to reconstruct PCA-compressed hyperspectral signals.", "probabilities": [0.6488891839981079, 0.0812017172574997, 0.056618090718984604, 0.2132910043001175]}, {"string": "LBP has been found improving the detection performance considerably when it is combined with some other local image gradient based descriptors [49].", "probabilities": [0.543044924736023, 0.030646491795778275, 0.08371809870004654, 0.3425905406475067]}, {"string": "Sherrah et al. [45] using a deep FCN with no downsampling to annotate high-resolution aerial imagery on a Vaihingen challenge data set, eliminating the need for either deconvolution, or interpolation.", "probabilities": [0.3712274432182312, 0.019261261448264122, 0.14819540083408356, 0.46131595969200134]}, {"string": "Paisitkriangkrai et al. [44] proposed a system based on CNNs trained on the Vaihingen challenge data set to perform semantic labeling.", "probabilities": [0.6334635615348816, 0.1779041737318039, 0.06930669397115707, 0.11932551860809326]}, {"string": "As illustrated in Fig. 2, for an input image I, we first use the linear iterative clustering (SLIC) algorithm [46] to segment I into a set of superpixels S. For each superpixel Si S, we utilize a shallow modality channel and a deep modality channel to respectively extract shallow features VS and deep features VD.", "probabilities": [0.08586370199918747, 0.009744956158101559, 0.35768836736679077, 0.5467029809951782]}, {"string": "In our implementation, after obtaining the 256 dimensional initial vector, we adopt the principal components analysis algorithm (PCA) [50] to reduce the initial 256 dimensional-vector to a more compact 80-dimensional feature vector.", "probabilities": [0.08570913225412369, 0.009708697907626629, 0.3578219413757324, 0.5467602610588074]}, {"string": "Similar with [48], we compare each pixel of the input image to each of its 8 neighbors along a clockwise circle and generate a 8-digit binary number, and then use this 8-digit binary number (corresponding to a decimal number within 0 255) as the low-level feature of each pixel.", "probabilities": [0.08570156991481781, 0.009700302965939045, 0.3579677641391754, 0.5466303825378418]}, {"string": "VS is simple to execute and has a potential to enhance the discrimination between similar geospatial objects [29].", "probabilities": [0.844385027885437, 0.037114426493644714, 0.02778676152229309, 0.09071380645036697]}, {"string": "A similar argument was made for APR [60] for which a negative bag mislabeled as positive, would lead to a high FPR.", "probabilities": [0.6822759509086609, 0.19550970196723938, 0.036860402673482895, 0.08535398542881012]}, {"string": "In that case, MIL can be viewed as a special kind of semi-supervised problem [67] where the labeled portion of the data belongs to only one class and the instance are structured in sets with label constraints.", "probabilities": [0.8410098552703857, 0.041908569633960724, 0.02766093611717224, 0.08942056447267532]}, {"string": "For example, in the drug activity prediction problem [3], the objective is to predict if a molecule induces a given effect.", "probabilities": [0.845321774482727, 0.03693454712629318, 0.02751295641064644, 0.09023076295852661]}, {"string": "Thus, observing nature segments might help to decide if the image contains a cocktail or a bear [92].", "probabilities": [0.8220004439353943, 0.0425383597612381, 0.03444647789001465, 0.10101477056741714]}, {"string": "Alternatively, in web mining tasks [77] where websites are bags and pages linked by the websites are instances, there exists a semantic relation between two bags representing websites linked together.", "probabilities": [0.8396393060684204, 0.03734644129872322, 0.02955780178308487, 0.09345641732215881]}, {"string": "Recently, the differences between these two scenarios were rigorously investigated [20].", "probabilities": [0.8421996831893921, 0.03692520037293434, 0.028726743534207344, 0.0921483039855957]}, {"string": "Alpaydin et al. [21] compared instance-space and bag-space classifiers on synthetic and real-world data.", "probabilities": [0.8488677740097046, 0.03652023896574974, 0.02638801373541355, 0.08822405338287354]}, {"string": "In [23] the similarities between MIL benchmark data sets were studied.", "probabilities": [0.2480352222919464, 0.5158707499504089, 0.13489247858524323, 0.10120157897472382]}, {"string": "Vocabulary-based methods [70] are particularly well adapted for this situation.", "probabilities": [0.6783706545829773, 0.025167236104607582, 0.04829493537545204, 0.2481672465801239]}, {"string": "For example, [186] uses instance selection algorithms inspired by the immune system to reduce the size of the data set before using MIL algorithms.", "probabilities": [0.7394542694091797, 0.020973481237888336, 0.0387633852660656, 0.20080892741680145]}, {"string": "A similar approach is used in [78] except bits are associated a pool of subgraphs patterns mined from the data set.", "probabilities": [0.6268267631530762, 0.07039091736078262, 0.055533748120069504, 0.247248575091362]}, {"string": "In some cases, clustering is performed in bag space using standard algorithms and set-based distance measures (e.g. k-Medoids and the Hausdorff distance [48]).", "probabilities": [0.23056010901927948, 0.015098820440471172, 0.22142373025417328, 0.5329173803329468]}, {"string": "The methods are grouped based on the representation space following a taxonomy similar to [15].", "probabilities": [0.08540388196706772, 0.009678595699369907, 0.3582213222980499, 0.5466962456703186]}, {"string": "All experiments have been conducted using Matlab and some implementations from the MIL toolbox [164] and the LAMDA website.11http://lamda.nju.edu.cn/.", "probabilities": [0.08538830280303955, 0.009676898829638958, 0.3582407832145691, 0.5466939806938171]}, {"string": "MI-SVM and mi-SVM[6]: These algorithms are transductive SVMs.", "probabilities": [0.17625631392002106, 0.04475671425461769, 0.49699196219444275, 0.2819949984550476]}, {"string": "CKMIL [107] locates the most positive instance in each bag based on its proximity to a single positive cluster center.", "probabilities": [0.701346218585968, 0.058506496250629425, 0.08716343343257904, 0.15298378467559814]}, {"string": "Alternatively, it has been proposed to use gradient descent with logistic regressions in a MILES like algorithm [184].", "probabilities": [0.5162910223007202, 0.020248763263225555, 0.09398243576288223, 0.369477778673172]}, {"string": "Methods computing distance between bags also become impractical as the data set size increases [15].", "probabilities": [0.7930386066436768, 0.045387234538793564, 0.04604705423116684, 0.11552708595991135]}, {"string": "From these few data sets, to our knowledge, there is only one (Birds [12]) that supplies instance labels and is non-artificial.", "probabilities": [0.7513089776039124, 0.12486172467470169, 0.0347478948533535, 0.08908137679100037]}, {"string": "This is the rationale behind Citation-kNN-ROI [66] which, however, does not perform well in practice (see Section 6.4).", "probabilities": [0.8071922063827515, 0.07144753634929657, 0.031128335744142532, 0.09023194015026093]}, {"string": "Also, methods modeling instance distributions in bags such as vocabulary-based [70] methods naturally deal with data sets containing multiple concepts/modes.", "probabilities": [0.7169721722602844, 0.020479140803217888, 0.04003377631306648, 0.22251489758491516]}, {"string": "Instance-space SVM-based methods like mi-SVM [6] can deal with disjoint regions of positive instances using a kernel.", "probabilities": [0.6438616514205933, 0.0229719839990139, 0.0570683479309082, 0.2760981023311615]}, {"string": "All methods in this experiment use an SVM which is known to be vulnerable to label noise [172].", "probabilities": [0.7144802808761597, 0.021323565393686295, 0.040485840290784836, 0.2237103134393692]}, {"string": "No further details are given on these features, except that this representation is sub-optimal and should be further investigated [6].", "probabilities": [0.7039569616317749, 0.0750354751944542, 0.07530348002910614, 0.14570406079292297]}, {"string": "Carbonneau et al. [53] studied the ability to identify witnesses of several MIL methods.", "probabilities": [0.709321916103363, 0.05331728234887123, 0.04785708710551262, 0.18950366973876953]}, {"string": "In [18], the task consists of identifying texts that contain a passage which links a protein to a particular component, process or function.", "probabilities": [0.547410786151886, 0.2847120761871338, 0.0646313801407814, 0.10324577242136002]}, {"string": "MILES[4]: In Multiple Instance Learning via Embedded instance Selection (MILES) an SVM classifies bags represented by feature vectors containing maximal similarities to selected prototypes.", "probabilities": [0.09737325459718704, 0.011029398068785667, 0.34390467405319214, 0.547692596912384]}, {"string": "MInD[93]: With this method, each bag is encoded by a vector whose fields are dissimilarities to the other bags in the training data set.", "probabilities": [0.0859413743019104, 0.009713697247207165, 0.3574622869491577, 0.546882688999176]}, {"string": "CCE[36]: This algorithm is based on clustering and classifier ensembles.", "probabilities": [0.08605417609214783, 0.009795251302421093, 0.3576074242591858, 0.5465431809425354]}, {"string": "The method is essentially the same as gradient boosting [165] except that the loss function is based on bag classification error.", "probabilities": [0.6346961855888367, 0.0744553729891777, 0.055064596235752106, 0.23578381538391113]}, {"string": "In [99], a method learning relational structure in data predicts which movies will be nominated for an award.", "probabilities": [0.2998156249523163, 0.04791349172592163, 0.28640714287757874, 0.36586377024650574]}, {"string": "miGraph[10]: This method represents each bag by a graph in which instances correspond to nodes.", "probabilities": [0.0868074893951416, 0.009807935915887356, 0.3563680946826935, 0.547016441822052]}, {"string": "The algorithm was adapted in [66] to perform instance classification.", "probabilities": [0.551112711429596, 0.06218084320425987, 0.07873620837926865, 0.3079702854156494]}, {"string": "Birds[12]: The bags of this data set correspond to 10 s recordings of bird songs from one or more species.", "probabilities": [0.21528618037700653, 0.07320398837327957, 0.48961323499679565, 0.22189663350582123]}, {"string": "The experiments were conducted using a nested cross-fold validation protocol [170].", "probabilities": [0.08540739119052887, 0.00968245044350624, 0.3582276999950409, 0.5466824769973755]}, {"string": "HEPMASS[168]: The instances of this data set come from the HEPMASS Data Set (http://archive.ics.uci.edu/ml/datasets/HEPMASS).", "probabilities": [0.20249953866004944, 0.06922370195388794, 0.5077480673789978, 0.2205287665128708]}, {"string": "Letters[169]: This semi-synthetic MIL data set uses instances from the Letter Recognition data set (https://archive.ics.uci.edu/ml/datasets/Letter+Recognition).", "probabilities": [0.08598480373620987, 0.009759895503520966, 0.3587726056575775, 0.5454826951026917]}, {"string": "In miDoc [26], a graph represents the entire MIL problem, where bags are compared based on the connecting edges.", "probabilities": [0.36780691146850586, 0.20768329501152039, 0.2421945482492447, 0.18231528997421265]}, {"string": "CCE [36] performs a clustering of the instance space.", "probabilities": [0.23801109194755554, 0.07323155552148819, 0.4627259075641632, 0.22603140771389008]}, {"string": "When texts are represented by word frequency features (e.g. BoW) the data is very sparse and high-dimensional [6].", "probabilities": [0.8313779234886169, 0.03872367739677429, 0.03231435641646385, 0.097584068775177]}, {"string": "In [155], the objective is to automatically determine the genre of musical excerpts.", "probabilities": [0.3410070538520813, 0.18540963530540466, 0.28151458501815796, 0.1920686811208725]}, {"string": "This is achieved with BoW-SVM by performing k-means clustering on all the training instances [15].", "probabilities": [0.08542045205831528, 0.009678780101239681, 0.3581945598125458, 0.5467061400413513]}, {"string": "This problem formulation is also used in [137] to recognize scenes that are inappropriate for children.", "probabilities": [0.6295239925384521, 0.07622797787189484, 0.05576302856206894, 0.23848500847816467]}, {"string": "MILD [60] discovers a set of true positive instances.", "probabilities": [0.8482714295387268, 0.03637859970331192, 0.02663159742951393, 0.08871831744909286]}, {"string": "This type of problem is well studied in single instance learning [188], but requires more exploration in the MIL context.", "probabilities": [0.8493397235870361, 0.0357990488409996, 0.02641724981367588, 0.08844395726919174]}, {"string": "There exist many methods to deal with imbalanced data [192].", "probabilities": [0.7262218594551086, 0.02325599081814289, 0.04022600129246712, 0.2102961391210556]}, {"string": "For example, in [144], a MIL classifier trained only with X-ray images labeled as healthy or as containing tuberculosis, outperforms its supervised version, trained on outlines of tuberculosis lesions.", "probabilities": [0.6326819658279419, 0.24813251197338104, 0.03762226924300194, 0.08156329393386841]}, {"string": "For example, the chronic lung disease COPD has 4 different stages, but [145] treats them all as the positive class.", "probabilities": [0.7544006705284119, 0.05275764688849449, 0.061198364943265915, 0.1316433995962143]}, {"string": "There are different levels of generality for multiple concept assumptions of this type [32].", "probabilities": [0.7828274965286255, 0.09135232865810394, 0.03400762006640434, 0.09181254357099533]}, {"string": "The BoW framework has been used in a similar fashion in [159], however, in that case instances are cepstrum feature vectors representing 1 s-long audio segments.", "probabilities": [0.6266158819198608, 0.0707627683877945, 0.0555415153503418, 0.24707981944084167]}, {"string": "Metric learning has been a hot research topic, influenced by the pioneering work of Xing et al. [36].", "probabilities": [0.6590177416801453, 0.09369709342718124, 0.09216807037591934, 0.15511715412139893]}, {"string": "One of the most widely used Mahalanobis-distance learning methods is Large-Margin Nearest Neighbours (LMNN), introduced by Weinberger et al. [37], which defines the constraints in a local way and has many extensions.", "probabilities": [0.7113438844680786, 0.019318653270602226, 0.041411351412534714, 0.2279261201620102]}, {"string": "Mirror-Descent Metric Learning (MDML), proposed by Kunapuli et al. [40], is a general online Mahalanobis-distance learning framework.", "probabilities": [0.2455829381942749, 0.07016201317310333, 0.4508454501628876, 0.2334095984697342]}, {"string": "Qi et al. [39] exploited the sparse nature of the high-dimensional feature space and considered the case of high-dimensional data together with few training samples, for which they used LogDet divergence and L1 regularization together.", "probabilities": [0.7896513938903809, 0.04958251118659973, 0.04620132967829704, 0.11456477642059326]}, {"string": "database [43], the CACD (http://bcsiriuschen.github.io/CARC/).", "probabilities": [0.1468307226896286, 0.025600139051675797, 0.46058526635169983, 0.3669838011264801]}, {"string": "Furthermore, we have tried different network structures, and the experimental results show that our method is also applicable in deeper network; while the work [1] have not demonstrated accuracy gains with increased depth.", "probabilities": [0.5666602253913879, 0.03278747946023941, 0.07598681002855301, 0.32456547021865845]}, {"string": "Finally, we evaluate the generalization ability of our approach on more databases than [1].", "probabilities": [0.09023398905992508, 0.010131272487342358, 0.3521478772163391, 0.5474868416786194]}, {"string": "However considering the sufficient pairs in a mini-batch is crucial to our model and deeper network also requires more memory, we use the Alexnet [46] to train a new deeper model to investigate the performance improvement bring by deeper network.", "probabilities": [0.12910598516464233, 0.01214559469372034, 0.30410319566726685, 0.5546451807022095]}, {"string": "However considering the sufficient pairs in a mini-batch is crucial to our model and deeper network also requires more memory, we use the Alexnet [46] to train a new deeper model to investigate the performance improvement bring by deeper network.", "probabilities": [0.12822692096233368, 0.012103758752346039, 0.30505234003067017, 0.5546169281005859]}, {"string": "However considering the sufficient pairs in a mini-batch is crucial to our model and deeper network also requires more memory, we use the Alexnet [46] to train a new deeper model to investigate the performance improvement bring by deeper network.", "probabilities": [0.12910598516464233, 0.01214559469372034, 0.30410319566726685, 0.5546451807022095]}, {"string": "However considering the sufficient pairs in a mini-batch is crucial to our model and deeper network also requires more memory, we use the Alexnet [46] to train a new deeper model to investigate the performance improvement bring by deeper network.", "probabilities": [0.12822692096233368, 0.012103758752346039, 0.30505234003067017, 0.5546169281005859]}, {"string": "Using the identity information as the supervised signal, we first learn the feature with softmax loss, and then adopt LMNN [37] to learn Mahalanobis distance.", "probabilities": [0.08536303043365479, 0.009675392881035805, 0.3582771122455597, 0.5466845035552979]}, {"string": "The experimental setting is the same as that used in [47].", "probabilities": [0.6265481114387512, 0.07083069533109665, 0.0555511973798275, 0.24706996977329254]}, {"string": "The challenges include large intra-subject variation and great inter-subject similarity [1].", "probabilities": [0.46921244263648987, 0.0756789818406105, 0.23755256831645966, 0.21755605936050415]}, {"string": "From birth to adulthood, the greatest change is craniofacial growth, which involves a change in shape; while from adulthood to old age, the most perceptible change turns skin aging, which involves texture change [2].", "probabilities": [0.848846971988678, 0.035730548202991486, 0.026625122874975204, 0.08879733085632324]}, {"string": "Different from our work, paper [1] models the similarity limited to the thresholds.", "probabilities": [0.33190345764160156, 0.5067626237869263, 0.07140007615089417, 0.089933842420578]}, {"string": "We first detect the faces using the method proposed in paper [44] and resize the faces to a uniform size.", "probabilities": [0.0854313001036644, 0.009680691175162792, 0.3581831455230713, 0.546704888343811]}, {"string": "The experimental setting is the same as in [30], and we divide the rest of the images into three folders: images taken in 2004 2006, in 2007 2009 and in 2010 2012.", "probabilities": [0.08658042550086975, 0.009857576340436935, 0.36000049114227295, 0.5435614585876465]}, {"string": "The experimental setting is the same as in [30], and we divide the rest of the images into three folders: images taken in 2004 2006, in 2007 2009 and in 2010 2012.", "probabilities": [0.08658725023269653, 0.009858093224465847, 0.3599911034107208, 0.5435635447502136]}, {"string": "The experimental setting is the same as in [30], and we divide the rest of the images into three folders: images taken in 2004 2006, in 2007 2009 and in 2010 2012.", "probabilities": [0.08669598400592804, 0.009876638650894165, 0.36023473739624023, 0.5431926846504211]}, {"string": "The experimental setting is the same as in [30], and we divide the rest of the images into three folders: images taken in 2004 2006, in 2007 2009 and in 2010 2012.", "probabilities": [0.08670281618833542, 0.009877155534923077, 0.3602253794670105, 0.5431946516036987]}, {"string": "Note that we train our model only on the MORPH dataset for fair comparison, while the newest deep model on age invariant face recognition proposed in [35] is pretrained on several other face datasets.", "probabilities": [0.6100175380706787, 0.08404901623725891, 0.05815262719988823, 0.24778084456920624]}, {"string": "The experimental setting is the same as in [30], and we divide the rest of the images into three folders: images taken in 2004 2006, in 2007 2009 and in 2010 2012.", "probabilities": [0.08658042550086975, 0.009857576340436935, 0.36000049114227295, 0.5435614585876465]}, {"string": "The experimental setting is the same as in [30], and we divide the rest of the images into three folders: images taken in 2004 2006, in 2007 2009 and in 2010 2012.", "probabilities": [0.08658725023269653, 0.009858093224465847, 0.3599911034107208, 0.5435635447502136]}, {"string": "The experimental setting is the same as in [30], and we divide the rest of the images into three folders: images taken in 2004 2006, in 2007 2009 and in 2010 2012.", "probabilities": [0.08669598400592804, 0.009876638650894165, 0.36023473739624023, 0.5431926846504211]}, {"string": "The experimental setting is the same as in [30], and we divide the rest of the images into three folders: images taken in 2004 2006, in 2007 2009 and in 2010 2012.", "probabilities": [0.08670281618833542, 0.009877155534923077, 0.3602253794670105, 0.5431946516036987]}, {"string": "Based on the CACD database, Chen et al. [31] developed a verification subset called CACD-VS, which contains 2000 positive pairs and 2000 negative pairs.", "probabilities": [0.1196647509932518, 0.01683645136654377, 0.3673895001411438, 0.49610936641693115]}, {"string": "We use the mean average precision (MAP) to measure the performance (see [31] for the MAP calculation).", "probabilities": [0.08536572754383087, 0.009675445966422558, 0.35827288031578064, 0.5466859340667725]}, {"string": "However considering the sufficient pairs in a mini-batch is crucial to our model and deeper network also requires more memory, we use the Alexnet [46] to train a new deeper model to investigate the performance improvement bring by deeper network.", "probabilities": [0.12910598516464233, 0.01214559469372034, 0.30410319566726685, 0.5546451807022095]}, {"string": "However considering the sufficient pairs in a mini-batch is crucial to our model and deeper network also requires more memory, we use the Alexnet [46] to train a new deeper model to investigate the performance improvement bring by deeper network.", "probabilities": [0.12822692096233368, 0.012103758752346039, 0.30505234003067017, 0.5546169281005859]}, {"string": "However considering the sufficient pairs in a mini-batch is crucial to our model and deeper network also requires more memory, we use the Alexnet [46] to train a new deeper model to investigate the performance improvement bring by deeper network.", "probabilities": [0.12910598516464233, 0.01214559469372034, 0.30410319566726685, 0.5546451807022095]}, {"string": "However considering the sufficient pairs in a mini-batch is crucial to our model and deeper network also requires more memory, we use the Alexnet [46] to train a new deeper model to investigate the performance improvement bring by deeper network.", "probabilities": [0.12822692096233368, 0.012103758752346039, 0.30505234003067017, 0.5546169281005859]}, {"string": "Li et al. [33] proposed a two-level hierarchical-learning model with a new feature descriptor called local pattern selection (LPS) to address this problem.", "probabilities": [0.7961307764053345, 0.04084291309118271, 0.0441812239587307, 0.11884515732526779]}, {"string": "Bouchaffra [32] introduced a novel formalism that performed dimensionality reduction and captured topological features to conduct pattern classification.", "probabilities": [0.7979493737220764, 0.0468311570584774, 0.043499600142240524, 0.11171980947256088]}, {"string": "Sungatullina et al. [26] also used the multi-feature descriptor and further proposed a multi-view discriminative-learning (MDL) method.", "probabilities": [0.5074963569641113, 0.03040255606174469, 0.0957322046160698, 0.3663688600063324]}, {"string": "Li et al. [25] proposed a multi-feature discriminant-analysis (MFDA) method, in which each face was represented by patch-based SIFT and LBP descriptors and faces were recognized using a variation of random-subspace LDA.", "probabilities": [0.45952415466308594, 0.021236976608633995, 0.11324481666088104, 0.4059939980506897]}, {"string": "Gong et al. [28] proposed a hidden-factor analysis (HFA) method, in which they supposed that there are two hidden factors, age and identity, that influence facial appearance.", "probabilities": [0.7225735783576965, 0.01960749365389347, 0.03927522152662277, 0.21854367852210999]}, {"string": "It does make sense because we only train our model on CACD, while the best result presented in [47] is 38.2%, which is trained on 672 K identities, much more face data than our training samples.", "probabilities": [0.6265812516212463, 0.07080245018005371, 0.05554594099521637, 0.24707040190696716]}, {"string": "Autoencoder loss is the Euclidean loss between input and output tensors given by  x x o 2 whereas  jyjlog(eoj keok) is the softmax loss from the predictor [56].", "probabilities": [0.20220375061035156, 0.06903814524412155, 0.508232593536377, 0.22052550315856934]}, {"string": "As an additional step we obtained the facial pose information by using active appearance models and generating facial landmarks [57].", "probabilities": [0.08546873927116394, 0.009681891649961472, 0.35812607407569885, 0.5467233657836914]}, {"string": "The autoencoder topology is inspired by ImageNet [15] and comprises of convolutional layers gradually reducing data dimensionality until we reach a fully connected layer.", "probabilities": [0.438907265663147, 0.03065168485045433, 0.11758662760257721, 0.41285446286201477]}, {"string": "MMI which originally contained only posed facial expressions, was recently extended to include natural versions of happiness, disgust and surprise [60].", "probabilities": [0.8504596948623657, 0.03542125225067139, 0.026131806895136833, 0.08798724412918091]}, {"string": "We developed specialized video recording and annotation tools to collect and label facial expressions (first presented in [54]).", "probabilities": [0.08550779521465302, 0.009682860225439072, 0.35806646943092346, 0.546742856502533]}, {"string": "The Cohn Kanade dataset [58] is one of the oldest and well known dataset containing facial expression video clips.", "probabilities": [0.2047991156578064, 0.06867393851280212, 0.5048729181289673, 0.221654012799263]}, {"string": "MMI facial expression dataset [59] involves an ongoing effort for representing both enacted and induced facial expressions.", "probabilities": [0.2042694240808487, 0.06939031928777695, 0.5054511427879333, 0.22088906168937683]}, {"string": "Our neural network was implemented using the Caffe framework [62] and trained using NVIDIA Tesla K40 GPUs.", "probabilities": [0.08541560173034668, 0.009679816663265228, 0.3582058250904083, 0.5466987490653992]}, {"string": "In fact, studies have shown that non-verbal communication accounts for more than half of all societal interactions [1].", "probabilities": [0.850303053855896, 0.03546103462576866, 0.02617586962878704, 0.08806013315916061]}, {"string": "Modeling and parameterizing human faces is one of the most fundamental problems in computer graphics [7].", "probabilities": [0.8369051218032837, 0.03774145618081093, 0.03048178181052208, 0.09487171471118927]}, {"string": "We utilize sources downloaded from Visual data transforming and taking in Resources[63] as a reference to contrast with our strategies.", "probabilities": [0.085548996925354, 0.009706317447125912, 0.35824668407440186, 0.5464979410171509]}, {"string": "Both autoencoder and predictor network topologies are implemented as Caffe prototxt files [62] and they will be made available for public usage.", "probabilities": [0.08544263243675232, 0.009687806479632854, 0.35842272639274597, 0.5464468002319336]}, {"string": "While we cannot compare against methods such as [7] because of absence of publicly available code our method still wins on MMI dataset.", "probabilities": [0.6587962508201599, 0.08185624331235886, 0.057390060275793076, 0.20195744931697845]}, {"string": "Stacked autoencoders can be used to convert high dimensional data into lower dimensional space which can be useful for classification, visualization or retrieval [55].", "probabilities": [0.8236789107322693, 0.02883872203528881, 0.03159957006573677, 0.11588273197412491]}, {"string": "In [54], the author considered velocity changes in videos as well as a semi-supervised learning approach.", "probabilities": [0.3789452612400055, 0.2795429825782776, 0.18594597280025482, 0.15556581318378448]}, {"string": "A way around this is to use autoencoders for feature extraction or weights initialization [44], followed by fine tuning over a smaller labeled dataset.", "probabilities": [0.5913299918174744, 0.05692874640226364, 0.06736759096384048, 0.2843736708164215]}, {"string": "Similar to the autoencoder [25], some variants of CILR can be obtained by applying frequently-used constraint, such as regularization constraint and sparsity constraint.", "probabilities": [0.4376094937324524, 0.06363166868686676, 0.25430387258529663, 0.244455024600029]}, {"string": "Inspired by principal component analysis, Valpola et al. [44] proposed a semi-supervised method to train deep neural networks by including the lateral shortcut connections from the encoder part to decoder part at each layer.", "probabilities": [0.4285188615322113, 0.031188061460852623, 0.1212434247136116, 0.41904962062835693]}, {"string": "Furthermore, Pezeshki et al. [45] adjusted connections between individual parts of deep models to enhance performance by learning their relationships.", "probabilities": [0.8462883234024048, 0.0367538146674633, 0.02723071537911892, 0.08972720801830292]}, {"string": "The sparsity can be utilized to discover interesting structure in data and to learn the more robust feature representations of data points [43].", "probabilities": [0.8442966341972351, 0.036790426820516586, 0.027944928035140038, 0.09096795320510864]}, {"string": "Inspired by Ng [39], we develop an variant of CILR termed sparse CILR which attempts to learn sparse feature representations by adding a sparse penalty term in loss function.", "probabilities": [0.4363851547241211, 0.03940651938319206, 0.11720795184373856, 0.40700042247772217]}, {"string": "Theoretically, in the supervised learning tasks, reducing the structural error can improve the generalization capability of models [46].", "probabilities": [0.8400554656982422, 0.038125406950712204, 0.02902045287191868, 0.09279869496822357]}, {"string": "Empirically, Krizhevsky et al. [1] have found that the weight decay utilized in the AlexNet is not merely a regularizer, it can reduce the training error of models.", "probabilities": [0.5760489702224731, 0.026709720492362976, 0.07733075320720673, 0.3199106454849243]}, {"string": "Similar to the expectation maximization algorithm [47], the Theorem 1 does not quite imply that the proposed method converges to a global optimal value, because it is indeterminate whether the result of optimizing  kd(ft(xk),yk)+d(yk,ft(T (xk))) is the global minimum at each iteration.", "probabilities": [0.825466513633728, 0.029125917702913284, 0.03364335000514984, 0.11176417768001556]}, {"string": "Different from adding regularization to loss function, a denoising function will be learned by reconstructing the original clean data from the corrupted input data [40].", "probabilities": [0.20144614577293396, 0.061410337686538696, 0.4871494472026825, 0.24999405443668365]}, {"string": "We conduct experiments on the ILSVRC2012 1K dataset [53] to evaluate the performance of the proposed method.", "probabilities": [0.08558713644742966, 0.009715455584228039, 0.3580418527126312, 0.5466554760932922]}, {"string": "Specifically, the VGG-16 and VGG-19 networks proposed in [73] are employed as the based models.", "probabilities": [0.085458904504776, 0.00968377385288477, 0.35814833641052246, 0.5467089414596558]}, {"string": "The convolutional neural network is an extension of the LeNet-5 [49], but it has more filters in convolutional layers.", "probabilities": [0.24162593483924866, 0.10605588555335999, 0.43723446130752563, 0.2150837630033493]}, {"string": "A straightforward way to train excellent DNNs is increasing the number of labeled training data [21].", "probabilities": [0.8293953537940979, 0.04096430540084839, 0.03229556232690811, 0.09734481573104858]}, {"string": "Since training DNNs can be considered as an optimal problem with a group of parameters [22], it is reasonable to learn deep models by pre-training these models with a mass of unlabeled data and fine-tuning the models for the specific classification task with labeled data.", "probabilities": [0.7765519618988037, 0.051326557993888855, 0.04733143374323845, 0.12479003518819809]}, {"string": "Thirdly, although the siamese networks [59] consider pair-wise samples as CILR to pre-train deep models, the contribution of the siamese networks is limited.", "probabilities": [0.8362206816673279, 0.03959287703037262, 0.030103089287877083, 0.09408339858055115]}, {"string": "There are 5000 training samples, 8000 test samples and 100000 unlabeled samples.ILSVRC2012 1K[53]: ILSVRC2012 1K image dataset consists of 1000 categories.", "probabilities": [0.20216216146945953, 0.06901028007268906, 0.508301854133606, 0.22052572667598724]}, {"string": "Except it has 100 classes, and only 500 training images, 100 testing images per class.STL-10[52]: The STL-10 dataset consists of 96   96 color images and relatively less labeled data.", "probabilities": [0.31606176495552063, 0.07834871113300323, 0.3753189742565155, 0.23027057945728302]}, {"string": "The deep learning library Keras [54] is employed to actualize deep models.", "probabilities": [0.08544553816318512, 0.009680826216936111, 0.358163446187973, 0.5467101335525513]}, {"string": "Inspired by Ioffe and Szegedy [48], we combat the overfitting by normalizing the inputs of each layer.", "probabilities": [0.4376847743988037, 0.031000519171357155, 0.12005211412906647, 0.411262571811676]}, {"string": "There are 50000 training images and 10000 test images.CIFAR-100[51]: This dataset is just like the CIFAR-10.", "probabilities": [0.20475919544696808, 0.06994350999593735, 0.5044907331466675, 0.2208065241575241]}, {"string": "It contains 73257 digit samples for training, 26032 digit samples for testing.CIFAR-10[51]: A dataset of 32   32 colour images in 10 classes, with 5000 examples each.", "probabilities": [0.20246149599552155, 0.06918877363204956, 0.5077623724937439, 0.22058740258216858]}, {"string": "For each deep neural network, the Gaussian initialization strategy described in [56] is used to initialize the parameters.", "probabilities": [0.1004340872168541, 0.011911430396139622, 0.34062671661376953, 0.5470277667045593]}, {"string": "The RMSProp optimizer [55] is utilized to optimize objective functions in this paper.", "probabilities": [0.08673039078712463, 0.009840495884418488, 0.3565969467163086, 0.5468322038650513]}, {"string": "Following the protocols in [4], the resnet-32 network is used in the experiments.", "probabilities": [0.08543161302804947, 0.009687370620667934, 0.3582042157649994, 0.5466768145561218]}, {"string": "To pre-train more deeper CNNs (e.g., residual networks [4]), an end-to-end approach is desiderated.", "probabilities": [0.7011827230453491, 0.0751359760761261, 0.07692974805831909, 0.14675158262252808]}, {"string": "Inspired by the sparse coding algorithm, Ranzato et al. [43] presented an unsupervised method to learn a group of sparse feature detectors that are invariant to small shifts and distortions.", "probabilities": [0.48900744318962097, 0.031036831438541412, 0.10021212697029114, 0.3797436058521271]}, {"string": "The fully connected network ( plain network  described in [69]) is employed to model deeper networks to investigate the capacities of different pre-training methods.", "probabilities": [0.10016420483589172, 0.010507741011679173, 0.3383908271789551, 0.5509372353553772]}, {"string": "For example, principle component analysis can minimize the mean squared error of training data, but the performance for image classification is worse than the autoencoder with larger mean squared error [37].", "probabilities": [0.7335731983184814, 0.13863258063793182, 0.03703934699296951, 0.0907549113035202]}, {"string": "In experiments, the K-means [80] method is employed as the postprocessing to cluster images.", "probabilities": [0.08560008555650711, 0.009692799299955368, 0.35794785618782043, 0.5467592477798462]}, {"string": "Similar to the sparse autoencoder, the contractive autoencoder [38] attempts to learn the invariant feature representations of the inputs by introducing the Frobenius norm to encoding layers.", "probabilities": [0.38443389534950256, 0.07660320401191711, 0.3105108141899109, 0.22845210134983063]}, {"string": "Furthermore, by imposing the sparse constraints to encoding layers, more robust feature representations can be learned in the sparse autoencoder [39].", "probabilities": [0.6947797536849976, 0.05927503481507301, 0.09021882712841034, 0.155726358294487]}, {"string": "However, a universal method for training very deep networks is under the way of exploring [4].", "probabilities": [0.7458330392837524, 0.022066865116357803, 0.039155393838882446, 0.1929447501897812]}, {"string": "In addition, the deconvolutional network [41] pre-trains CNNs based on the decomposition under a sparse constraint with unlabeled data only.", "probabilities": [0.16313892602920532, 0.036598898470401764, 0.39689019322395325, 0.40337198972702026]}, {"string": "By applying the up-pooling operator, the convolutional autoencoder [29] has been developed to pre-train CNNs by reconstructing input images.", "probabilities": [0.095391184091568, 0.010569044388830662, 0.34546735882759094, 0.5485724210739136]}, {"string": "The denoising autoencoder [40] successfully learns a denoising function by reconstructing the original clean data from the corrupted inputs.", "probabilities": [0.20252111554145813, 0.06927399337291718, 0.5076897740364075, 0.22051513195037842]}, {"string": "For data preparation, we follow the previous work as described in [42].", "probabilities": [0.14936017990112305, 0.6758900880813599, 0.05995815619826317, 0.11479157954454422]}, {"string": "We carry out CILR and its variants with a VGG-style network [73] as illustrated in Table 7.", "probabilities": [0.08541448414325714, 0.00968095287680626, 0.358211487531662, 0.5466930866241455]}, {"string": "We also perform experiment to study the performance of CILR on the residual networks [4].", "probabilities": [0.08576752990484238, 0.009717375040054321, 0.35775887966156006, 0.5467562079429626]}, {"string": "In [1] with assuming g(x)=|x|.", "probabilities": [0.14981666207313538, 0.027039598673582077, 0.4581298232078552, 0.3650139272212982]}, {"string": "In [1], four-color labeling is implemented as coloring the regions one by one.", "probabilities": [0.2026074081659317, 0.06912990659475327, 0.5076571106910706, 0.22060558199882507]}, {"string": "Specifically, in grouping step, we define a similarity matrix S={Si,j},i,j Rn on the initial local region set  ={ri},i=1,2, ,Rn, and then apply Affinity Propagation (AP) clustering [42] with S. AP clustering only takes the similarity between pairs of data as input, and there is no need to specify the initial centers or the number of them beforehand.", "probabilities": [0.14237399399280548, 0.025318576022982597, 0.4660514295101166, 0.3662559688091278]}, {"string": "As suggested by Frey and Dueck [42], the minimum similarity or the median similarity of similarity matrix S will be good initial choice for the preference.", "probabilities": [0.6346604228019714, 0.09524160623550415, 0.10445642471313477, 0.16564153134822845]}, {"string": "As to GBIS [22], it only uses local similarity criterion, but our method utilizes MLG for global approximate solution and global grouping based heuristic four-color labeling strategy for smooth coloring.", "probabilities": [0.305634081363678, 0.43988993763923645, 0.07207703590393066, 0.18239887058734894]}, {"string": "In our previous work [1], an unsupervised image segmentation method based on Multiphase Multiple Piecewise Constant and Geodesic Active Contour (MMPC-GAC) model is proposed.", "probabilities": [0.12639783322811127, 0.025101710110902786, 0.3203147351741791, 0.5281856656074524]}, {"string": "To test the performance of the proposed Heuristic Four-Color Labeling (HFCL) method, especially on those complex natural images, we first give a comprehensive comparison with the Random Four-Color Labeling (RFCL) strategy on the BSDS500 dataset [44], which contains 500 images from a broad range of scenarios.", "probabilities": [0.08566363900899887, 0.009697637520730495, 0.35786035656929016, 0.5467783808708191]}, {"string": "The parameters of MLG share the same values with [1].", "probabilities": [0.2492607831954956, 0.07193563133478165, 0.44365426898002625, 0.2351493388414383]}, {"string": "Like most other approaches, we take BSDS300 [43] for the comparison.", "probabilities": [0.14316722750663757, 0.025217870250344276, 0.48046788573265076, 0.35114699602127075]}, {"string": "Mean shift [3] is served as the over-segmentation method for initialization, whose bandwidth parameter is h=(hr,hs)=(8,10) and minimum region area is M=200.", "probabilities": [0.08576548844575882, 0.009713665582239628, 0.35775911808013916, 0.5467617511749268]}, {"string": "More recently, Wang et al. [27] introduced normalized and average tree partitioning methods to perform normalized and average cut over a tree.", "probabilities": [0.7142741084098816, 0.15641102194786072, 0.03765396028757095, 0.09166091680526733]}, {"string": "Mumford Shah model [31] for segmentation aims to seek piece-wise smooth approximation and minimal edge cost.", "probabilities": [0.6837084889411926, 0.021483518183231354, 0.04761180654168129, 0.2471962571144104]}, {"string": "We have shown in [1] that the four-color setting can significantly increase the efficiency while achieving equivalent level of performance.", "probabilities": [0.07763835042715073, 0.8864786028862, 0.018333086743950844, 0.017549950629472733]}, {"string": "Yin et al. [20] proposed a conditional Gaussian process dynamic model (CGPDM) for capturing both the spatial and temporal dynamics of group activities.", "probabilities": [0.8353211879730225, 0.046410512179136276, 0.02843349240720272, 0.08983471989631653]}, {"string": "Some classes contained only few samples, therefore, we ignored those classes in our tasks and we conducted an experiment on six class classifications following a previous work [18]: Approach, Fight, InGroup, RunTogether, Split, and WalkTogether.", "probabilities": [0.08637897670269012, 0.009860158897936344, 0.35722222924232483, 0.5465387105941772]}, {"string": "It is also ineffective when the region of a person occupies less than 5% of a scene [25].", "probabilities": [0.8486902713775635, 0.036241114139556885, 0.026532601565122604, 0.08853603899478912]}, {"string": "Visin et al. [45] proposed a model that replaced convolution and pooling layer of convolutional neural network (CNN) with GRU for object recognition.", "probabilities": [0.7093034386634827, 0.15715676546096802, 0.03993410989642143, 0.09360568970441818]}, {"string": "The BEHAVE dataset [31] was recorded from a surveillance system, and it consists of 10 group activities: InGroup, Approach, WalkTogether, Meet, Split, Ignore, Chase, Fight, RunTogether, and Following.", "probabilities": [0.08668026328086853, 0.00982066709548235, 0.3566126525402069, 0.5468863844871521]}, {"string": "Zhang et al. [22] measured the causalities between people for group activity recognition.", "probabilities": [0.8480879068374634, 0.037071749567985535, 0.02650895155966282, 0.08833139389753342]}, {"string": "Our method achieved 4.99% higher recognition accuracy than the state-of-the-art method, GIZ [18].", "probabilities": [0.6597001552581787, 0.08695303648710251, 0.05467776209115982, 0.19866904616355896]}, {"string": "New Collective Activity dataset [33] consists of six classes: Gathering, Talking, Dismissal, Walking, Chasing, and Queuing.", "probabilities": [0.2133106291294098, 0.07037076354026794, 0.49349483847618103, 0.22282376885414124]}, {"string": "Fisher Vector (FV [26]) coding is a widely-used second-order feature coding method.", "probabilities": [0.1990850418806076, 0.014113986864686012, 0.24403247237205505, 0.5427684783935547]}, {"string": "The original affine subspace method [28] only has the forward operation, yet our affine subspace layer is viewed as the 1   1 convolutional layer and can be trained by the end-to-end pattern.", "probabilities": [0.09848511219024658, 0.010810942389070988, 0.34812504053115845, 0.542578935623169]}, {"string": "In the proposed model, the affine subspace method in [28] is used to reduce the dimension.", "probabilities": [0.1354854702949524, 0.015019118785858154, 0.2996532917022705, 0.549842119216919]}, {"string": "Describing textures database (DTD)[34] is a texture database, consisting of 5640 images, organized according to a list of 47 classes inspired from human perception.", "probabilities": [0.24940083920955658, 0.5934978723526001, 0.08148504793643951, 0.07561623305082321]}, {"string": "In our experiments, the feature maps from the last convolutional layer of the VGG-VD [38] network are used as the CNN features, all the images are resized to 448   448 pixels, and the images are augmented by random crop and random mirror.", "probabilities": [0.08633898198604584, 0.009800986386835575, 0.35709983110427856, 0.5467602014541626]}, {"string": "We use the suggested training/test split in [34] to get the experimental results.", "probabilities": [0.0853874534368515, 0.009679706767201424, 0.3582512438297272, 0.5466815829277039]}, {"string": "MIT indoors (MIT)[35] dataset is a difficult indoor scene benchmark which contains 15,620 images of 67 indoor scenes.", "probabilities": [0.2798164486885071, 0.08146238327026367, 0.41116514801979065, 0.22755594551563263]}, {"string": "In image classification, feature coding [20] is an important model and has been widely studied over the past several years.", "probabilities": [0.20447063446044922, 0.06944959610700607, 0.5051540732383728, 0.2209257334470749]}, {"string": "In the MIT-indoor [35] database, T is set as 7.", "probabilities": [0.20216716825962067, 0.06901871412992477, 0.5082886815071106, 0.22052551805973053]}, {"string": "Since the proposed LSO-VLADNet is very relative to the NetVLAD [27], a detailed classification comparison of NetVLAD and LSO-VLADNet is given.", "probabilities": [0.7039109468460083, 0.1179693192243576, 0.054621510207653046, 0.1234983429312706]}, {"string": "Since the structure of the CNN will significantly impact the classification result, for a fair comparison, we only compare those CNN methods which are based on the VGG-VD [38] network.", "probabilities": [0.09599369764328003, 0.010481334291398525, 0.3443271219730377, 0.5491978526115417]}, {"string": "In the DTD [34] dataset, T is set as 6.", "probabilities": [0.20225967466831207, 0.06909804791212082, 0.5081247687339783, 0.22051756083965302]}, {"string": "Zheng et al. [16] treated the mean field algorithm as a recurrent neural network (RNN), and then they jointly train the new structure RNN and a deep CNN to obtain the excellent image segmentation results.", "probabilities": [0.4141879677772522, 0.12080909311771393, 0.11974408477544785, 0.34525883197784424]}, {"string": "Wang et al. [5] combined the domain expertise of the sparse coding and the merits of the deep learning to achieve the state-of-art image super resolution results.", "probabilities": [0.8468797206878662, 0.03784499689936638, 0.0267227403819561, 0.0885525569319725]}, {"string": "Wang et al. [19] introduced a one-step sparse inference module to remove artifacts of JPEG compressed images.", "probabilities": [0.8206976652145386, 0.05798474699258804, 0.030348632484674454, 0.09096893668174744]}, {"string": "The LSO-VLADNet has obvious advantages than the B-CNN [44] and the NetVLAD which are end-to-end methods, this demonstrates the superiority of our end-to-end model.", "probabilities": [0.6267085671424866, 0.0706881433725357, 0.05553733557462692, 0.2470659762620926]}, {"string": "In the CUB200 [36] database, T is set as 5.", "probabilities": [0.2021702229976654, 0.06863778829574585, 0.5083227157592773, 0.2208692729473114]}, {"string": "In order to demonstrate the effectiveness of this scheme, we make some experiments on SUN397 [54] dataset which contains more than 100K scene images from 397 scene categories and is a relatively big dataset.", "probabilities": [0.1527860462665558, 0.04550575464963913, 0.2955787777900696, 0.5061294436454773]}, {"string": "Then, the features of the last convolutional layer are used to train the initialized dictionary {ck}k=1K, and the initialized dictionary is obtained by the K-means algorithm which is implemented by the VLFeat library [41].", "probabilities": [0.1423061341047287, 0.025309409946203232, 0.4661298990249634, 0.3662545382976532]}, {"string": "In the proposed LSO-VLADNet, the parameters in the Deep CNN are initialized by the weights and biases in the pre-trained VGG-VD [38] network.", "probabilities": [0.0853758230805397, 0.009676332585513592, 0.3582606017589569, 0.5466873049736023]}, {"string": "The projective matrixes Uk (k=1,2, ,K) are initialized by the affine subspace method in [28].", "probabilities": [0.08537771552801132, 0.009676321409642696, 0.3582572638988495, 0.5466887354850769]}, {"string": "Caltech-UCSD Birds (CUB200)[36] dataset is a widely used fine-grained image benchmark.", "probabilities": [0.09031395614147186, 0.009968144819140434, 0.3522460460662842, 0.5474718809127808]}, {"string": "We use the standard training/test split in [35] to report the classification results.", "probabilities": [0.08542201668024063, 0.009685487486422062, 0.35821375250816345, 0.5466787815093994]}, {"string": "Caltech 256[37] database is a large-scale object image benchmark which has 256 object classes with at least 80 images per category.", "probabilities": [0.21649183332920074, 0.07054761797189713, 0.48981767892837524, 0.2231428623199463]}, {"string": "We use the split setting in [36] to obtain the experimental results.", "probabilities": [0.0853690505027771, 0.009676339104771614, 0.3582703769207001, 0.5466842651367188]}, {"string": "Besides, the Adam [40] algorithm can avoid some local minima and obtain relatively low loss.", "probabilities": [0.28663310408592224, 0.02579318918287754, 0.18856187164783478, 0.4990118741989136]}, {"string": "Next, we use Adam [40] algorithm which is an adaptively stochastic optimization method to reduce the cost function until the convergence.", "probabilities": [0.08537353575229645, 0.009675995446741581, 0.3582618832588196, 0.5466885566711426]}, {"string": "In the training stage, LSO-VLADNet which utilizes the combination of the first-order and the second-order statistics is more time-consuming than the NetVLAD [27] which only uses first-order statistical information.", "probabilities": [0.626433253288269, 0.06970755010843277, 0.055730849504470825, 0.24812836945056915]}, {"string": "At last, intra-normalization [33] and L2-normalization are used to produce the final VLAD vector for image classification.", "probabilities": [0.08580663800239563, 0.009745415300130844, 0.3577822148799896, 0.5466656684875488]}, {"string": "In the Caltech256 [37] database, T is set as 7.", "probabilities": [0.20227646827697754, 0.06906869262456894, 0.5081228613853455, 0.22053201496601105]}, {"string": "In the proposed network, we use the affine subspace method in [28] for dimension reduction.", "probabilities": [0.08537929505109787, 0.009676612913608551, 0.3582545518875122, 0.5466895699501038]}, {"string": "Inspired by the great advantages of the deep learning model, Arandjelovic et al. [27] extended the traditional VLAD coding model to an end-to-end model called NetVLAD.", "probabilities": [0.4930676221847534, 0.029408203437924385, 0.09970391541719437, 0.3778201937675476]}, {"string": "The traditional feature coding methods are unsupervised; thus, the trained dictionary may not be optimal for image recognition, and the SIFT [12] feature used in the feature coding method doesn t have a strong representation ability.", "probabilities": [0.5226388573646545, 0.044032130390405655, 0.08758189529180527, 0.34574708342552185]}, {"string": "The most useful features of a pre-trained CNN are the feature of the last convolutional layer and the feature of the last fully connected layer [30].", "probabilities": [0.2029440701007843, 0.06921899318695068, 0.5072003602981567, 0.22063663601875305]}, {"string": "The CNN model utilized deep structure which consists of multiple convolutional layers, pooling layers and nonlinear activation functions to obtain more abstract and descriptive feature representation from the large-scale ImageNet [31] dateset.", "probabilities": [0.08719665557146072, 0.00987387914210558, 0.35595354437828064, 0.5469759702682495]}, {"string": "The image feature in Fig. 1 can be the SIFT [12] or the CNN feature.", "probabilities": [0.21055889129638672, 0.07062797993421555, 0.49695226550102234, 0.2218608856201172]}, {"string": "It is worth noting that the LSO-VLAD has the better performance than the FV-CNN [30] which is a state-of-art non-end-to-end feature coding method in visual recognition.", "probabilities": [0.634356677532196, 0.10016656666994095, 0.10322535783052444, 0.16225135326385498]}, {"string": "As Table 3 shows, LSO-VLADNet is better than the LSO-VLAD and NetVLAD [25].", "probabilities": [0.6296917200088501, 0.07035297900438309, 0.05538679659366608, 0.2445686012506485]}, {"string": "Compared with the non-end-to-end LSO-VLAD and the end-to-end NetVLAD [27], LSO-VLADNet improves 4.3% and 1.8%, respectively.", "probabilities": [0.700106143951416, 0.12762202322483063, 0.052740003913640976, 0.11953185498714447]}, {"string": "To overcome the drawbacks of convex NMF, in [22], a hierarchical convex NMF that can automatically adapt to the internal structures of a data set is proposed, which hence yields meaningful and interpretable clusters for non-convex data sets.", "probabilities": [0.685382604598999, 0.166690856218338, 0.047492869198322296, 0.10043361037969589]}, {"string": "Eq. (3) can be solved by using the optimizing rules in [3], which are presented in Table 1.", "probabilities": [0.10560762882232666, 0.011396769434213638, 0.3491964638233185, 0.5337991118431091]}, {"string": "In [23], an efficient hierarchical document clustering method is presented to reveal the hierarchical relations between the documents, which is based on a new algorithm for rank-2 NMF.", "probabilities": [0.19605788588523865, 0.0640534833073616, 0.28639063239097595, 0.4534980356693268]}, {"string": "In [1], an online variation of Bayes algorithm is proposed for LDA, which is based on online stochastic optimization with a natural gradient step.", "probabilities": [0.14614687860012054, 0.01714611053466797, 0.2893821597099304, 0.5473248958587646]}, {"string": "The reason can be two-fold: 1) HLDA uses the whole document set to train a model iteratively while OLDA does not; 2) HLDA generates more topics than OLDA, and the two methods have better performance with more topics [18].", "probabilities": [0.6467138528823853, 0.06412907689809799, 0.05450443550944328, 0.23465268313884735]}, {"string": "The result of our method is also better than the result in [26] (0.54), despite the influence of document partition strategy.", "probabilities": [0.7008932828903198, 0.12490230053663254, 0.05331578478217125, 0.12088872492313385]}, {"string": "For example, TOT (topic over time) model [15] captures how a topic changes over time, where each topic is associated with a continuous distribution, and the distribution over a topic is influenced by the timestamp of documents.", "probabilities": [0.8498340845108032, 0.035628244280815125, 0.02628922276198864, 0.0882483720779419]}, {"string": "The first dataset is a subset of the Nist Topic Detection and Tracking corpus (TDT2) [27], which consists of data collected during the first half of 1998 and taken from 6 sources, including 2 newswires (APW, NYT), 2 radio programs (VOA, PRI), and 2 television programs (CNN, ABC).", "probabilities": [0.20186983048915863, 0.06796913594007492, 0.5088128447532654, 0.22134824097156525]}, {"string": "The difference between t-NMF-l1 and fix-NMF-l1 is that in every time slot, t-NMF-l1 generates a new topic matrix and initializes with Wt-1.Hie-NMF-l1: Hie-Rank2 can cluster documents into hierarchies by repeatedly performing rank2-NMF [23].", "probabilities": [0.08537594228982925, 0.009676520712673664, 0.35826635360717773, 0.5466812252998352]}, {"string": "The NMF algorithm is implemented with multiplicative-updates rules and l1-norm regularization [5].t-NMF-l1: t-NMF-l1 uses the same NMF implementation as the fix-NMF-l1.", "probabilities": [0.08536560088396072, 0.00967560987919569, 0.3582737445831299, 0.5466850399971008]}, {"string": "The advantage of detecting topics in a hierarchical way is two-fold: First, it can reduce the overlaps between different topics and lead to more separable topics, which has been proven in [20].", "probabilities": [0.7505643963813782, 0.07961036264896393, 0.05187218263745308, 0.11795308440923691]}, {"string": "In [26], a method is proposed to get emerging topics from novel documents.", "probabilities": [0.5595523715019226, 0.026631737127900124, 0.07991798222064972, 0.3338979482650757]}, {"string": "Hie-NMF-l1 is performed in each time slot.JPP: JPP is a time-based collective factorization algorithm for topic discovery [5].", "probabilities": [0.17492783069610596, 0.03177325800061226, 0.43776389956474304, 0.35553494095802307]}, {"string": "In [19], HLDA is extended to a nonparametric topic-model tree to represent human choices by developing a new stick-breaking model.", "probabilities": [0.6157023906707764, 0.0859546959400177, 0.12609122693538666, 0.17225158214569092]}, {"string": "Similarly, in [5], a transition matrix is used to represent the relations between the topics.", "probabilities": [0.3303157389163971, 0.2566831111907959, 0.24251340329647064, 0.17048771679401398]}, {"string": "In [3], the task is solved from a different perspective, which finds the topics that fit the present data and the decomposition results of the past data.", "probabilities": [0.5596919655799866, 0.2779178023338318, 0.06076899543404579, 0.10162128508090973]}, {"string": "Its mathematical formulation is given in [3].", "probabilities": [0.14279067516326904, 0.025356873869895935, 0.46554920077323914, 0.3663032650947571]}, {"string": "Multilayer network [1] is a structure commonly used to describe and model the complex interaction between sets of entities/nodes.", "probabilities": [0.32072731852531433, 0.08443015813827515, 0.3669343590736389, 0.2279082089662552]}, {"string": "Multilayer network would also be a good choice for big data modelling because there are complex interactions between multiple sources or attributes due to the Variety property of big data [4].", "probabilities": [0.8265589475631714, 0.04091260954737663, 0.03331150859594345, 0.09921693801879883]}, {"string": "To compare the efficiency, we also implement standard NMF which does not consider the horizontal network H2708   2708 and Relational Topic Model (RTM) [48] which is a successful Bayesian model for the relational data and considers both horizontal network (document-word relationships) and vertical network (document network).", "probabilities": [0.08539596945047379, 0.009678690694272518, 0.35823550820350647, 0.5466898679733276]}, {"string": "Documents are a collection of papers from CiteSeer[50].", "probabilities": [0.2962498664855957, 0.07633869349956512, 0.3969007730484009, 0.23051074147224426]}, {"string": "The MDNet [20] tracker is the winner of VOT2015 competition.", "probabilities": [0.2023880034685135, 0.06908510625362396, 0.5079721212387085, 0.22055476903915405]}, {"string": "The 11 challenge factors as well as the evaluation metrics here are the same with the OTB-100 [9] dataset.", "probabilities": [0.11591402441263199, 0.016306467354297638, 0.42801687121391296, 0.43976259231567383]}, {"string": "TC-128: The TC-128 [10] dataset is presented by Liang et al. and focus on color information.", "probabilities": [0.20350104570388794, 0.0699295699596405, 0.5060464143753052, 0.22052299976348877]}, {"string": "The performance of the 23 trackers is quantitatively validated via two metrics [9] including distance precision (DP)(%) rate at a threshold of 20 pixels and overlap success (OS)(%) at an overlap threshold 0.5.", "probabilities": [0.08686911314725876, 0.009770232252776623, 0.3561505079269409, 0.547210156917572]}, {"string": "OTB-100: The OTB-100 [9] dataset is presented by Wu et al., which has been one of most commonly used benchmarks in evaluating online visual trackers.", "probabilities": [0.45238378643989563, 0.10362549126148224, 0.19893662631511688, 0.24505406618118286]}, {"string": "VOT2015: The VOT2015 [11] dataset consists of 60 short sequences annotated with 6 different attributes including occlusion, illumination change, motion change, size change, camera motion and unassigned.", "probabilities": [0.15761850774288177, 0.032854244112968445, 0.4773731827735901, 0.33215397596359253]}, {"string": "In [22], Tao et al. propose a siamese network model to match the object template and candidates for visual tracking, in which the optimal state can be determined based on the highest matching score.", "probabilities": [0.811620831489563, 0.058628104627132416, 0.03358146920800209, 0.0961696207523346]}, {"string": "In [26], Nam et al. effectively combine pre-trained convolutional layers and multiple fully-connected layers with a tree structure to achieve good tracking performance.", "probabilities": [0.7184270024299622, 0.07585599273443222, 0.07053770869970322, 0.13517938554286957]}, {"string": "By using a large set of tracking videos with ground truths, the MDNet [20] method pre-trains a CNN model with shared layers and multiple branches of domain-specific layers to obtain a generic object representation, shown as Fig. 5.", "probabilities": [0.08970027416944504, 0.010040080174803734, 0.3525201380252838, 0.5477395057678223]}, {"string": "Later on, He et al. [67] formulate network layers as learning residual functions with short-cut connections, which makes gradients flow through multiple layers easier and allows the training of extremely deep networks.", "probabilities": [0.8085779547691345, 0.04725724458694458, 0.03823929280042648, 0.10592557489871979]}, {"string": "For instance, some recent visual tracking methods [74] pre-train networks on object detection data sets.", "probabilities": [0.8433502316474915, 0.032677061855793, 0.0285726860165596, 0.09540008008480072]}, {"string": "In [70], the region proposal network is proposed and trained end-to-end with Fast R-CNN to generate high-quality region proposals.", "probabilities": [0.29239916801452637, 0.32713571190834045, 0.22835306823253632, 0.15211206674575806]}, {"string": "To address this issue, the EAO [11] rule is also introduced to rank different tracking algorithms.", "probabilities": [0.1544407159090042, 0.017848553135991096, 0.3442170321941376, 0.48349371552467346]}, {"string": "It should be noted that the 15 trackers here are the same as trackers tested on OTB-100 and TC-128 except for MCPF [85], the code of which is packaged and cannot be changed.", "probabilities": [0.6702383756637573, 0.04093467444181442, 0.04904279485344887, 0.23978404700756073]}, {"string": "The CNT [82] method propagates an input image forward in a CNN network to extract weak features, and then learns a classifier for distinguishing these features into positive and negative ones.", "probabilities": [0.2680826783180237, 0.03907245025038719, 0.20244449377059937, 0.4904003441333771]}, {"string": "Daniel et al. [79] use a two-path network followed by two LSTM blocks to remember the object appearance and motion.", "probabilities": [0.7394292950630188, 0.024431053549051285, 0.04189111292362213, 0.1942485272884369]}, {"string": "Similar idea can also be found in [93].", "probabilities": [0.8377698659896851, 0.03151208907365799, 0.03145624324679375, 0.09926185756921768]}, {"string": "The SO-DLT [74] method first pre-trains a CNN model to recognize what is an object and then generates a probability map instead of producing a simple class label.", "probabilities": [0.3820687532424927, 0.03708941861987114, 0.14255955815315247, 0.43828222155570984]}, {"string": "The CNT [82] method collects a series of positive and negative patches to learn many image filters, and builds a robust appearance model using convolutional operators.", "probabilities": [0.1112915500998497, 0.011443221010267735, 0.32997220754623413, 0.5472930073738098]}, {"string": "The well-known VGGNet [64] (offline trained on the large-scale ImageNet dataset) is widely used to design the tracking methods because of their satisfactory performance on image classification.", "probabilities": [0.6162704825401306, 0.020731667056679726, 0.06455252319574356, 0.29844531416893005]}, {"string": "In [22], the SINT method generates a lot of particles and calculates their similarity scores using the siamese network.", "probabilities": [0.09047312289476395, 0.009968296624720097, 0.3511061370372772, 0.5484524965286255]}, {"string": "Qi et al. [31] extract features from six convolutional layers and combine these layers using an adaptive weight scheme.", "probabilities": [0.6447580456733704, 0.16319791972637177, 0.0680619403719902, 0.12398210912942886]}, {"string": "In [36], Wang et al. demonstrate that feature extraction is also a very important issue in designing a robust tracker (such as the tracker with HOG features performs much better than that with Haar-like features).", "probabilities": [0.8466114401817322, 0.03700052946805954, 0.027041714638471603, 0.0893462523818016]}, {"string": "Regarding the music genre dataset, we used Marsyas,8 a standard music information retrieval tool, to extract a collection of commonly used features in this kind of tasks (Tzanetakis and Cook, 2002).", "probabilities": [0.08690851926803589, 0.00981018878519535, 0.35621377825737, 0.5470674633979797]}, {"string": "We used the same dataset produced with ms=32 as in Cataltepe et al. (2011).", "probabilities": [0.08536463975906372, 0.009675577282905579, 0.35827499628067017, 0.546684741973877]}, {"string": "The WebKB (Sen et al., 2008) dataset consists of web pages from different classes.", "probabilities": [0.1547974795103073, 0.0301049891859293, 0.46792566776275635, 0.3471718430519104]}, {"string": "The UCF11 dataset (Liu and Luo, 2009) consists in 11 action categories: basketball shooting, biking/cycling, diving, golf swinging, horse back riding, soccer juggling, swinging, tennis swinging, trampoline jumping, volleyball spiking, and walking with a dog.", "probabilities": [0.7106212973594666, 0.05567558482289314, 0.08290062099695206, 0.15080244839191437]}, {"string": "for the second dataset, the Oxford data, we use the published tracking results provided by Benfold (Benfold and Video, 2011).", "probabilities": [0.09344669431447983, 0.01083342358469963, 0.34911781549453735, 0.5466020703315735]}, {"string": "It contains 715 images which contains 8 semantic class labels [12].", "probabilities": [0.2039923220872879, 0.06949229538440704, 0.5057311654090881, 0.22078418731689453]}, {"string": "This dataset contains 9556 images with 515 object categories [39].", "probabilities": [0.20238125324249268, 0.0688880905508995, 0.5080432891845703, 0.2206874042749405]}, {"string": "We tested our algorithm on datasets from the two most recent PAN [11] competitions, which provide benchmark datasets for authorship attribution.", "probabilities": [0.08789895474910736, 0.010145343840122223, 0.3556799590587616, 0.5462757349014282]}, {"string": "The Lazebnik s texture dataset [41] has 1000 images of 25 different textures.", "probabilities": [0.2026989459991455, 0.06907624006271362, 0.5075932741165161, 0.22063158452510834]}, {"string": "We also used the CMU-PIE database [26], which includes 41,368 face images of 68 people captured under 13 poses, 43 illuminations conditions, and with four different expressions: neutral, smile, blinking, and talk.", "probabilities": [0.08546660095453262, 0.009694020263850689, 0.35816875100135803, 0.5466706156730652]}, {"string": "The first database contains eight scenes categories provided by Oliva and Torralba [28]: mountain (274 images), coast (360 images), highway (260 images), street (292 images), insidecity (308 images), forest (328 images), opencountry (410 images), and tallbuilding (356 images), where the size of each image is 256   256.", "probabilities": [0.21656128764152527, 0.07040447741746902, 0.4898242652416229, 0.22321000695228577]}, {"string": "In the CMU-PIE dataset [26] we focused on a two class classification problem: smile and neutral.", "probabilities": [0.09367687255144119, 0.011056778952479362, 0.34977689385414124, 0.5454894304275513]}, {"string": "Additionally, we used the Japanese Female Facial Expression (JAFFE) database [24], which contains only 213 images of female facial expression expressed by 10 subjects.", "probabilities": [0.0853683203458786, 0.009675854817032814, 0.35826995968818665, 0.5466858744621277]}, {"string": "These domain-specific sentiment words turn out to be a key determinant in identifying a sentiment from the past experiments, i.e. Ref. [2].", "probabilities": [0.8465064167976379, 0.03603483363986015, 0.027372131124138832, 0.09008650481700897]}, {"string": "The Pascal VOC 2007 [13] contains around 10 K images depicting 20 different object categories.", "probabilities": [0.20991206169128418, 0.06930828094482422, 0.49837663769721985, 0.22240297496318817]}, {"string": "The KTH-TIPS2-a dataset [8] contains 4395 images of 11 different texture materials acquired under different scales, poses and illumination conditions.", "probabilities": [0.20286118984222412, 0.06924697756767273, 0.5072857737541199, 0.22060610353946686]}, {"string": "We process the Iris dataset taken from the UCI repository [2].", "probabilities": [0.08536732941865921, 0.009676113724708557, 0.35827359557151794, 0.5466830134391785]}, {"string": "For this end, we manually built a database from frontal portraits from FRGC 2.0 [40] and from the web.", "probabilities": [0.08537044376134872, 0.00967649556696415, 0.3582685589790344, 0.5466845035552979]}, {"string": "  FERET: The database [41] contains more than 3500 face images from women and men (with different races such as African, Asian and Caucasian) involving different expressions and illumination conditions.", "probabilities": [0.20783565938472748, 0.06924483180046082, 0.500912070274353, 0.22200746834278107]}, {"string": "  AR: The images of database  AR  [34] were taken from 100 subjects (50 women and 50 men) with different facial expressions, illumination conditions, and occlusions with sun glasses and scarf (we used the cropped version).", "probabilities": [0.08540159463882446, 0.0096787940710783, 0.35822856426239014, 0.5466910600662231]}, {"string": "  GROUPS: The database [17] consists of 28,231 face images collected from Flickr images.", "probabilities": [0.2021743506193161, 0.06835334748029709, 0.508371114730835, 0.22110117971897125]}, {"string": "For comparison of performance, the normative data from [10] is used.", "probabilities": [0.5730897188186646, 0.06739909946918488, 0.0713646188378334, 0.2881465256214142]}, {"string": "For testing the recognition of basic emotions we used the Cohn-Kanade + Extended Cohn-Kanade [15] dataset (with 7 emotions).", "probabilities": [0.08538425713777542, 0.009676407091319561, 0.3582458198070526, 0.5466935634613037]}, {"string": "MSRC dataset and VOC2007 dataset [3]: MSRC dataset contains 4,323 images with 18 classes which are standard images.", "probabilities": [0.2023390531539917, 0.06910429149866104, 0.5080133676528931, 0.22054331004619598]}, {"string": "Because the indoor-scene-object dataset [4] already has the ground truth layout, semantic label, and detection except cuboid annotation, we generate our dataset from the dataset as follows to get the ground truth cuboid while avoiding manual annotation.", "probabilities": [0.08551008254289627, 0.00969627220183611, 0.3581852912902832, 0.5466083288192749]}, {"string": "The Object dataset [16] contains 51 object categories, with a total of 250,000 images including color and depth.", "probabilities": [0.20429033041000366, 0.0695880725979805, 0.505301833152771, 0.22081975638866425]}, {"string": "We conduct these experiments on the NYUD2 dataset [69].", "probabilities": [0.08538229763507843, 0.009678562171757221, 0.3582559823989868, 0.546683132648468]}, {"string": "The videos were taken from a challenging, recently published data set [4], called the YouCook data set, consisting of instructional videos of different cooking styles, such as assembling, baking, grilling, etc. This collection of videos is challenging with diverse scenarios containing cluttered backgrounds, different subjects, occlusion of objects, shot from different viewpoints, and different illumination conditions.", "probabilities": [0.29655763506889343, 0.04011675715446472, 0.18595775961875916, 0.4773678779602051]}, {"string": "This proposal is an extension of [21] where a non-symmetric operator defined in the specific context of nested partitions was presented.", "probabilities": [0.06534349918365479, 0.903914749622345, 0.015945520251989365, 0.0147962411865592]}, {"string": "Song et al. [12] defined a feature function for the aforementioned CRFs model using a series of complex heuristics.", "probabilities": [0.7202584743499756, 0.01940775476396084, 0.03945678472518921, 0.22087688744068146]}, {"string": "In addition, Tang et al. [7] proposed a max-margin method to model the temporal structure in a video.", "probabilities": [0.5198565125465393, 0.030463378876447678, 0.09691604971885681, 0.3527641296386719]}, {"string": "Wang and Mori [10] combined a part-based approach with large-scale template features to obtain a discriminative model based on max-margin CRFs.", "probabilities": [0.6049889326095581, 0.02226066030561924, 0.07110612094402313, 0.3016442656517029]}, {"string": "A method that applies Fourier analysis was also proposed in [30].", "probabilities": [0.2951410114765167, 0.017610393464565277, 0.18431854248046875, 0.5029299855232239]}, {"string": "Their aim was to infer complex actions from predefined useful basic-level action detectors.Sun and Nevatia [28] imposed a representation for low-level action events that includes statistical information of the action transition probabilities.", "probabilities": [0.8028708100318909, 0.043352726846933365, 0.04254378750920296, 0.11123259365558624]}, {"string": "In this paper, we model the temporal structure of videos following [9], which models the evolution of appearance or local motion using a learning-to-rank technique.", "probabilities": [0.08759672194719315, 0.009823314845561981, 0.35515573620796204, 0.5474241971969604]}, {"string": "Wang and Schmid [6] used dense sampled local visual features at several spatial scales.", "probabilities": [0.7653959393501282, 0.04011491313576698, 0.05844441428780556, 0.1360446959733963]}, {"string": "Jain et al. [4] proposed to determine the location of actions in the video and then use them to refine recognition.", "probabilities": [0.8410857319831848, 0.03825130686163902, 0.028651466593146324, 0.09201150387525558]}, {"string": "[15] used the curvature of the trajectories of local descriptors and other kinematic properties to improve performance of activity recognition methods.", "probabilities": [0.27768340706825256, 0.2463315725326538, 0.15186116099357605, 0.3241237998008728]}, {"string": "For the learning-to-rank method, we used the source code of [39].", "probabilities": [0.08536787331104279, 0.00967626366764307, 0.358271986246109, 0.5466839075088501]}, {"string": "For MKL method, we used the source code of [38], we set the value of   to 0.1 and the value of   of the radial basis function to 0.1.", "probabilities": [0.08560696244239807, 0.00970391370356083, 0.35797256231307983, 0.5467165112495422]}, {"string": "To simplify the problem further, Aiolli and Donini [38]", "probabilities": [0.7129114866256714, 0.07146912813186646, 0.07207120209932327, 0.14354822039604187]}, {"string": "To learn a representation of each video, we use the learning-to-rank technique, which consists of the following three steps: frame-level representation, sequence smoothing, and learning the representation of the whole video [9].1.Frame-level representation: Each video can be represented as follows: X=[x1,x2,...,xn], where xt contains the descriptors of a frame at time step t.2.Sequence smoothing: To avoid abrupt action changes in videos, we smooth the sequence X using time varying mean as follows: mt=1t i=1txi.", "probabilities": [0.08537083864212036, 0.009676314890384674, 0.35826730728149414, 0.546685516834259]}, {"string": "There is no overlap between the 33 movies in the training set and the 36 movies in the testing set [40].", "probabilities": [0.21146303415298462, 0.0716027244925499, 0.49525177478790283, 0.22168250381946564]}, {"string": "However, constructing robust models to recognize free-form activities of the same class still an open problem [1].", "probabilities": [0.8500220775604248, 0.035501182079315186, 0.02626626007258892, 0.08821046352386475]}, {"string": "A good example is Fisher vectors [6].", "probabilities": [0.7215484976768494, 0.034383971244096756, 0.07058627903461456, 0.17348121106624603]}, {"string": "As we see in Tables 1 4, the recognition results are improved when we combine the proposed method with the baseline of [39].", "probabilities": [0.1127626970410347, 0.013993073254823685, 0.32591310143470764, 0.5473310947418213]}, {"string": "Notation: In this work, we follow [38] to solve a classification problem with training samples defined as {(x1,y1),(x2,y2), ,(xl,yl)} and test samples defined {(xl+1,yl+1),(xl+2,yl+2), ,(xL,yL)}, where x   Rm and y { 1,1} if we solve a binary classification problem or y   {1, 2, .., C} if we solve a multi-class classification problem.", "probabilities": [0.1424112468957901, 0.025328435003757477, 0.4660138487815857, 0.3662464916706085]}, {"string": "This optimization problem can then be solved by any of standard optimization methods [37].", "probabilities": [0.7113233804702759, 0.0199351217597723, 0.04133234918117523, 0.22740915417671204]}, {"string": "To compute the HOF descriptors, optical flow displacement vectors in the horizontal and vertical directions were determined [6].", "probabilities": [0.08538566529750824, 0.009676695801317692, 0.35824698209762573, 0.5466906428337097]}, {"string": "Wang et al. [31] reported that HOG and HOF descriptors yield good results on different activity recognition datasets compared to classical descriptors.", "probabilities": [0.8467936515808105, 0.03788542002439499, 0.026734095066785812, 0.08858686685562134]}, {"string": "Dalal et al. [32] showed the robustness of MBH descriptor against camera and background motion.", "probabilities": [0.8446096181869507, 0.039539072662591934, 0.027052920311689377, 0.08879828453063965]}, {"string": "The orientations were quantized into eight bins for HOG and nine bins for HOF as described in [3].", "probabilities": [0.08536721765995026, 0.009675777517259121, 0.35827159881591797, 0.5466853976249695]}, {"string": "The problem of learning the kernel combination is posed as a min max problem over variables   and  . Aiolli and Donini [38] The optimal   is a regularized minimizer of  2-norm of the distances vector.", "probabilities": [0.7355014085769653, 0.08441951125860214, 0.05801766365766525, 0.12206149846315384]}, {"string": "A Scalable multiple kernel learning algorithm (EasyMKL): In this paper, we use the EasyMKL method [38] because it can easily deal with hundreds of thousands of kernels and more.", "probabilities": [0.11697117984294891, 0.014929321594536304, 0.321591854095459, 0.5465076565742493]}, {"string": "Jain et al. [24] showed how they can take advantage of objects in the videos for action classification.", "probabilities": [0.8500216603279114, 0.035617124289274216, 0.026232313364744186, 0.08812884986400604]}, {"string": "Fernando et al. [39] used the time varying mean method for smoothing the feature vectors in order to cope with abrupt changes in human action.", "probabilities": [0.6314311623573303, 0.06604429334402084, 0.05484625697135925, 0.24767829477787018]}, {"string": "The model proposed in [13] took approximately one day for one temporal CNN on a system consisting of four NVIDIA Titan cards (it took 3.1 times the training time if a single GPU was used).", "probabilities": [0.6265770792961121, 0.0709407702088356, 0.05557350441813469, 0.24690856039524078]}, {"string": "The CNN model proposed in [13] has a good performance because it represents a higher level of semantic concepts, but it has a high time complexity and requires complicated training passes.", "probabilities": [0.5965250134468079, 0.08623553067445755, 0.0623604953289032, 0.2548789381980896]}, {"string": "It is clear from Table 5 that the results of the proposed method with HMDB51 and Hollywood2 datasets are comparable to the ones of [39], as a result of the existence of a big number of slow actions in the datasets.", "probabilities": [0.7010061740875244, 0.0713239312171936, 0.07293125241994858, 0.15473854541778564]}, {"string": "For instance, [19] represented different parts of human body with motion and appearance based descriptors to extract features from human pose.", "probabilities": [0.8442434668540955, 0.03783300146460533, 0.027653397992253304, 0.09027016907930374]}, {"string": "The second approach stacks optical flow frames and inputs them into a CNN [13].", "probabilities": [0.08553694188594818, 0.009684653021395206, 0.35803231596946716, 0.5467460751533508]}, {"string": "An example of this approach was proposed in [18].", "probabilities": [0.6860508918762207, 0.08619428426027298, 0.050008732825517654, 0.17774598300457]}, {"string": "Another popular way to tackle action recognition using deep neural networks is to combine the CNN and LSTM as demonstrated in [23].", "probabilities": [0.6301379799842834, 0.07253206521272659, 0.05532241240143776, 0.24200747907161713]}, {"string": "Du et al. [22] showed that skeleton based action recognition can be managed using a hierarchical recurrent neural network.", "probabilities": [0.8432167768478394, 0.038881465792655945, 0.027756910771131516, 0.09014489501714706]}, {"string": "Srivastava et al. [21] used the state of the long-short term memory (LSTM) encoder after observing the last input frame as a video representation.", "probabilities": [0.7184547185897827, 0.020266825333237648, 0.03994538635015488, 0.22133305668830872]}, {"string": "In [20], two CNNs were used to capture both appearance and motion based features in action tubes.", "probabilities": [0.10415912419557571, 0.013945087790489197, 0.3378824293613434, 0.5440133810043335]}, {"string": "HMDB51 is a generic action classification dataset [41].", "probabilities": [0.21947365999221802, 0.07277917116880417, 0.4848325550556183, 0.22291454672813416]}, {"string": "The proposed method works better than the baseline of [39] in the case of slow actions since it makes the input sequences more generalized and lightens the unnecessary details.", "probabilities": [0.6263805627822876, 0.07074911147356033, 0.055604804307222366, 0.2472655028104782]}, {"string": "In general, the main limitation of tracking methods in complex and crowded scenes is the presence of occluded objects, which degrade the anomaly detection performance [10].", "probabilities": [0.8501207232475281, 0.03555087000131607, 0.02621608041226864, 0.08811238408088684]}, {"string": "For instance, the one-class Support Vector Machine (SVM) was used by Xu and Ricci [10].", "probabilities": [0.7202577590942383, 0.019202833995223045, 0.039470821619033813, 0.2210685908794403]}, {"string": "The learned filters in the deconvolutional layers serve as the base to reconstruct the shape of the input, taking into account the required reshape of the output, as presented in [18].", "probabilities": [0.4961400032043457, 0.042956288903951645, 0.18466630578041077, 0.27623745799064636]}, {"string": "They can also be built using deep architectures to learn a compressed representation of the input, by limiting the number of hidden units [21].", "probabilities": [0.6099978685379028, 0.0673086866736412, 0.13871923089027405, 0.1839742511510849]}, {"string": "For the UCSD Ped2 dataset, our result (FR+ED case) is close to that obtained by Xu and Ricci [10].", "probabilities": [0.7008951902389526, 0.12490327656269073, 0.0533144511282444, 0.12088701128959656]}, {"string": "The classification of human behavior in videos has been a subject of great interest in computer vision [1].", "probabilities": [0.850446879863739, 0.035421669483184814, 0.02613627351820469, 0.08799523115158081]}, {"string": "As a result, some features may perform well in particular domains and drive classifiers to bad classification accuracy in others, even combining motion and appearance features [10].", "probabilities": [0.8501492738723755, 0.03549414128065109, 0.026220450177788734, 0.08813624083995819]}, {"string": "Considering that Kolmogorov complexity is not directly computable, we used the bzip2 algorithm [41] as real-world compressor, so as to provide an approximation to NCR(x), according to Eq. (7).", "probabilities": [0.0853893905878067, 0.009677305817604065, 0.35824036598205566, 0.546692967414856]}, {"string": "The weights were initialized using the Xavier algorithm [47] that automatically determines the scale of initialization based on the number of input and output neurons.", "probabilities": [0.08538512885570526, 0.009676801972091198, 0.35824570059776306, 0.546692430973053]}, {"string": "Long ago, Cilibrasi and Vit nyi [38] proposed the use of the Kolmogorov complexity to measure image complexity.", "probabilities": [0.850141167640686, 0.0354979932308197, 0.026218630373477936, 0.08814225345849991]}, {"string": "In [27], a probability model that takes the spatial and temporal contextual information into account is learned.", "probabilities": [0.31071680784225464, 0.23959338665008545, 0.27270182967185974, 0.17698799073696136]}, {"string": "In [10], an appearance and motion SDAE was proposed to extract features of video surveillance datasets.", "probabilities": [0.1981012374162674, 0.04026751220226288, 0.2566884458065033, 0.5049428343772888]}, {"string": "The AE was introduced by Rumelhart et al. [19] and is regarded as an unsupervised fully connected one-hidden-layer neural network to learn from unlabeled datasets.", "probabilities": [0.6558581590652466, 0.22300265729427338, 0.03747536242008209, 0.08366382867097855]}, {"string": "To optimize the loss function, we use stochastic gradient descent with the adaptive sub-gradient method AdaGrad [46].", "probabilities": [0.0853649154305458, 0.00967553723603487, 0.3582744896411896, 0.5466850996017456]}, {"string": "Our architecture is similar to the model recently proposed by Hasan et al. [18].", "probabilities": [0.7009190917015076, 0.12486802786588669, 0.05331040173768997, 0.12090248614549637]}, {"string": "Considering that a pixel located at (x, y) in the frame t with the intensity I(x, y, t) moves by  x,  y and  t in the subsequent frame, by the brightness constancy assumption [43]", "probabilities": [0.20730333030223846, 0.06968057155609131, 0.5015345811843872, 0.22148148715496063]}, {"string": "The limitation of this type of sensor is the field of view and resolution of the camera [23].", "probabilities": [0.6530453562736511, 0.22407840192317963, 0.038436513394117355, 0.084439717233181]}, {"string": "For the Avenue dataset, our result (FR+ED case) was better than those achieved by Hasan et al. [18].", "probabilities": [0.7008708119392395, 0.12492358684539795, 0.053317710757255554, 0.1208878830075264]}, {"string": "On other hand, for the UCSD Ped1 dataset, our result is much worse than that presented by Saligrama and Chen [48].", "probabilities": [0.700888991355896, 0.12490886449813843, 0.053314998745918274, 0.12088713049888611]}, {"string": "Image tagging, also known as multi-label classification, is an important topic in computer vision and machine learning for its wide applications on scene classification and information retrieval [1].", "probabilities": [0.8458828926086426, 0.03628513589501381, 0.02754892222583294, 0.09028308093547821]}, {"string": "In all the following experiments, we first transferred the parameters of the trained CNN model [23] to our scene categorization problems.", "probabilities": [0.08816765248775482, 0.010024853982031345, 0.35488149523735046, 0.546925961971283]}, {"string": "Prior research in [1] found that using more sources of information can lead to better performance.", "probabilities": [0.6983678340911865, 0.031081246212124825, 0.04980234056711197, 0.22074860334396362]}, {"string": "There are different methods to obtain the weight vector W. For our purpose we will use a linguistic quantifier guided aggregation, a process in which the decision maker selects a quantifier representing the proportion of criteria necessary for a good solution [20]", "probabilities": [0.1216779500246048, 0.011634455062448978, 0.31158292293548584, 0.555104672908783]}, {"string": "In order to compare the results to that of an expert, a grounded truth was created similar to [11].", "probabilities": [0.7008479833602905, 0.12501144409179688, 0.05329807847738266, 0.12084250152111053]}, {"string": "Information acquired explicitly requires input from the reviewer, whereas information acquired implicitly entails eliciting information in an automated way [9].", "probabilities": [0.8420689702033997, 0.03409847617149353, 0.028996821492910385, 0.09483569115400314]}, {"string": "In this section we provide a summary of the OWA operator introduced by Yager [19] which will be applied in the proposed methodology.Definition 3.1An OWA operator of dimension n is a mapping of  :Rp R with an associated weighting vector W such that wh   [0, 1] and  h=1pwh=1.", "probabilities": [0.09234567731618881, 0.010320567525923252, 0.3519178330898285, 0.545415997505188]}, {"string": "Second, we compared the reviewer to paper assignments with the quality index (QI) defined in [16].", "probabilities": [0.1438596397638321, 0.02551998756825924, 0.48138222098350525, 0.34923815727233887]}, {"string": "Originally introduced in [2], it is an unsupervised topic modeling method.", "probabilities": [0.5630627274513245, 0.12835615873336792, 0.07702977955341339, 0.23155131936073303]}, {"string": "LDA has been used in other reviewer assignment systems such as the Toronto Paper Matching System [3].", "probabilities": [0.6935774087905884, 0.027289479970932007, 0.0444498248398304, 0.23468329012393951]}, {"string": "A reviewer may bid on papers for their novelty rather than their alignment with his/her research interests [1].", "probabilities": [0.8503215312957764, 0.03544839471578598, 0.026173891499638557, 0.08805627375841141]}, {"string": "In addition, reviewers may search for papers using keywords and bid on papers returned in their search rather than considering all the papers in the conference [3].", "probabilities": [0.839670717716217, 0.03751855343580246, 0.029450906440615654, 0.09335973858833313]}, {"string": "As argued in [18], there exists imprecision associated with reviewer expertise levels.", "probabilities": [0.8028333187103271, 0.050060052424669266, 0.0397113598883152, 0.10739526897668839]}, {"string": "Therefore, we consider an Ordered Weighted Averaging (OWA) aggregation function [19] to summarize the information coming from different sources and rank the candidate reviewers for each paper.", "probabilities": [0.08583229035139084, 0.009700603783130646, 0.3575971722602844, 0.5468699932098389]}, {"string": "Since Neural Turing Machine [8] can capture very long-term information with an external memory matrix, we utilize it as a further encoder of the CNN features to extract the temporal dynamics.", "probabilities": [0.17111292481422424, 0.057585205882787704, 0.2775404751300812, 0.4937613308429718]}, {"string": "We utilize Neural Turing Machine [8] to memorize the long-term temporal information.", "probabilities": [0.08537441492080688, 0.009675996378064156, 0.3582604229450226, 0.5466891527175903]}, {"string": "We follow the splits made by [7], separating the datasets into a training set of 1200 clips, a validation set of 100 clips and a test set of 670 clips.", "probabilities": [0.08538009226322174, 0.009677601978182793, 0.3582589030265808, 0.5466834306716919]}, {"string": "Venugopalan et al. first apply it to generate descriptions of video clips with a CNN-based encoder and a LSTM-based decoder [16].", "probabilities": [0.8384256362915039, 0.032427623867988586, 0.03016914799809456, 0.09897751361131668]}, {"string": "Compared with SA [7], our method additionally utilizes memory networks to capture temporal dynamics for more intact visual representations.", "probabilities": [0.6230901479721069, 0.07062409818172455, 0.056632429361343384, 0.24965336918830872]}, {"string": "Besides SA, M3[23] is the most similar method to ours.", "probabilities": [0.6189664602279663, 0.06874240934848785, 0.05788830667734146, 0.2544028162956238]}, {"string": "Our model outperforms all other methods except M3-google [23] in terms of BLEU.", "probabilities": [0.7012102603912354, 0.12072616070508957, 0.05459681898355484, 0.12346671521663666]}, {"string": "Previous work [1] has shown the potential of vision and text joint modeling for image classification.", "probabilities": [0.7198806405067444, 0.14660677313804626, 0.03956202045083046, 0.09395059198141098]}, {"string": "Unfortunately, previous work has pointed out the weakness of LSTM for modeling very long sequence [8] and current video captioning benchmark datasets generally include videos with long sequences.", "probabilities": [0.8000765442848206, 0.043387521058321, 0.0400567390024662, 0.11647915840148926]}, {"string": "Among such investigations, Neural Turing Machine [8] shows a great advantage of memorizing long-range information.", "probabilities": [0.8478589653968811, 0.036416277289390564, 0.02678518183529377, 0.0889395922422409]}, {"string": "Dropout [38] with a rate of 0.5 is utilized in our experiments as another regularization.", "probabilities": [0.14644667506217957, 0.03449356183409691, 0.30014511942863464, 0.5189145803451538]}, {"string": "Moreover, we convert all captions to lower case and tokenize the sentences using the PTBTokenizer in Stanford CoreNLP tools [37].", "probabilities": [0.08538118749856949, 0.009676482528448105, 0.35825103521347046, 0.546691358089447]}, {"string": "Following the split of [32], we divide the whole dataset into a training set of 6513 clips, a validation set of 497 clips and a test set of 2990 clips.", "probabilities": [0.08613783121109009, 0.009789323434233665, 0.3586118817329407, 0.5454609990119934]}, {"string": "Based on the evaluation of [35], METEOR is always better than BLEU in terms of consistency with human judgment and CIDER gets similar results compared with METEOR.", "probabilities": [0.7004601359367371, 0.12511438131332397, 0.05343044921755791, 0.12099507451057434]}, {"string": "All of our experiments are implemented with Keras and Theano [40].", "probabilities": [0.08548954129219055, 0.00968229304999113, 0.35809406638145447, 0.546734094619751]}, {"string": "Moreover, we train our model with the Adadelta [39] optimization algorithm and a clipnorm of 10.", "probabilities": [0.08542441576719284, 0.009684856981039047, 0.3582077622413635, 0.5466830134391785]}, {"string": "It memorizes long-term information including both previously generated words and the visual attention history to guide the attention mechanism [23].", "probabilities": [0.8255146741867065, 0.03720128536224365, 0.034592002630233765, 0.10269201546907425]}, {"string": "In contrast, Yao et al. first introduce a soft attention mechanism into video captioning to weight input CNN features given previously generated words [7].", "probabilities": [0.7838401794433594, 0.07951386272907257, 0.036420851945877075, 0.10022514313459396]}, {"string": "Moreover, Zhu et al. train a Multirate Visual Recurrent Model (MVRM) to better incorporate both past and future temporal information [18].", "probabilities": [0.7161862254142761, 0.056397851556539536, 0.0798487737774849, 0.14756710827350616]}, {"string": "The memory matrix interacts with the internal states of neural networks by reading and writing which relied on a unique addressing mechanism [8].", "probabilities": [0.35835808515548706, 0.0766494870185852, 0.33469682931900024, 0.23029564321041107]}, {"string": "However, we outperform M3 on all metrics on MSR-VTT [32] dataset with longer videos, which implies the importance of temporal dynamics for video captioning.", "probabilities": [0.08593795448541641, 0.009748604148626328, 0.35785049200057983, 0.5464629530906677]}, {"string": "Also, we achieve better performance in METEOR compared with other methods except HRNE [20] because HRNE focus more on building a fine-grained hierarchical structure for captioning.", "probabilities": [0.6299428343772888, 0.06693816930055618, 0.05513235554099083, 0.24798664450645447]}, {"string": "GWB uses Bayesian posterior estimation method to calculate the geodesic distance between superpixels [2] which based on feature map, and it is described by a fixed weight by statistic in advance.", "probabilities": [0.0859939306974411, 0.009769422933459282, 0.3575577437877655, 0.5466789603233337]}, {"string": "Based on these methods, we test the performance of GWB [16], AIM and AO-AIM model.", "probabilities": [0.08910291641950607, 0.010131116956472397, 0.35373440384864807, 0.547031581401825]}, {"string": "Borji et al. [4] summarizes the experiment results of 40 state-of-the-art models on 6 datasets, and provides performance evaluation indicators for saliency detection, which develops a test criteria for the study of saliency detection.", "probabilities": [0.8392493724822998, 0.042889419943094254, 0.02803167514503002, 0.08982952684164047]}, {"string": "The basic feature of color, texture and other feature are widely used in saliency classification, while edge feature plays an important role on boundary expression [18].", "probabilities": [0.09968513995409012, 0.01174512505531311, 0.37404629588127136, 0.5145233869552612]}, {"string": ", this model reveals that the overall probability of connecting two voxels is determined by each cue used individually, so that we can adjust the influence of cues dynamically on the basis of the attributes of voxels.", "probabilities": [0.7517916560173035, 0.0495736226439476, 0.06218700483441353, 0.1364476978778839]}, {"string": "The first one is a general outdoor building facades scene, consisting of terrestrial laser scanning point clouds from the large-scale point cloud classification benchmark datasets published by ETH Zurich [8], which is the largest known labeled 3D point cloud dataset of natural scenes.", "probabilities": [0.20728799700737, 0.0695258229970932, 0.5016486644744873, 0.2215375453233719]}, {"string": "Recently, automatically creation of the as-built structures for Building Information Model (BIM) [3] is drawing increasingly attention in the fields of engineering and construction.", "probabilities": [0.6859657764434814, 0.019646773114800453, 0.04719933867454529, 0.24718818068504333]}, {"string": "Moreover, we also conduct experiments using both laser scanned and photogrammetric point clouds from the same scene of a construction site, to compare and analyze the performance of the segmentation algorithms when dealing with datasets of significantly different sources [23].", "probabilities": [0.08544383198022842, 0.00968319084495306, 0.35817083716392517, 0.5467021465301514]}, {"string": "In model-based segmentation methods, parametric objects are directly fitted to the points [14].", "probabilities": [0.11546318978071213, 0.014923778362572193, 0.4022059738636017, 0.46740713715553284]}, {"string": "In complex situations, since there are plenty of non-watertight and intersecting surfaces, without the help of the information of the entire structure, we can hardly identify and segment such complex surfaces [4].", "probabilities": [0.771587610244751, 0.05284439027309418, 0.05239684879779816, 0.12317117303609848]}, {"string": "The aim of using an octree to create the voxel structure is threefold [24]: (1) Construction of a rasterized representation of the points, which simplifies the datasets and can overcome the uneven distributed point density; (2) Organization of the points by indexing the unorganized points with generated voxels in a tree structure; (3) Definition of neighboring relations of generated voxels.", "probabilities": [0.17193585634231567, 0.02507726661860943, 0.2732514441013336, 0.5297354459762573]}, {"string": "In this section we illustrate the proposed feature selection framework with a hypothetical toy dataset used by Pinheiro et al., [23] consisting of 2 classes with 13 documents and 9 terms.", "probabilities": [0.09690819680690765, 0.010773850604891777, 0.3453945219516754, 0.5469234585762024]}, {"string": "Due to the abundant and unwieldy size of the text content available on the web, manual organization of the text into many categories cannot be done [30].", "probabilities": [0.8500266671180725, 0.035531185567379, 0.026252754032611847, 0.08818934112787247]}, {"string": "CIFAR models were trained with whitened data from PyLearn2 [12] and we applied a random horizontal flip on the input image to simulate a larger training dataset and avoid over-fitting.", "probabilities": [0.2988332211971283, 0.031924787908792496, 0.18312983214855194, 0.48611214756965637]}, {"string": "Blue curves are the original version [14] (third architecture on Fig. 2) and red the shared version.", "probabilities": [0.2031485140323639, 0.06909938156604767, 0.5070232152938843, 0.22072891891002655]}, {"string": "For comparison, we also add the Inception v2 [39] model performance, in which the authors perform convolution factorization at inception module level to reduce model size.", "probabilities": [0.08565773814916611, 0.009719613008201122, 0.3582896590232849, 0.5463329553604126]}, {"string": "CNNs were introduced in [23] for hand written digits recognition.", "probabilities": [0.824868381023407, 0.0425616018474102, 0.033655475825071335, 0.09891446679830551]}, {"string": "GoogLeNet [38] introduced a multiscale approach using the inception module, composed of parallel convolutions with different kernel sizes.", "probabilities": [0.10156907141208649, 0.010954032652080059, 0.33872687816619873, 0.5487500429153442]}, {"string": "In Hashnets [5] uses a hash function to regroup connections that will share the same weights.", "probabilities": [0.8485421538352966, 0.0358162559568882, 0.026710927486419678, 0.0889306291937828]}, {"string": "For state of the art algorithms, if we apply the Spectral Regression Discriminant Analysis algorithm in [30], the time complexity of LDA can reach O(NSm).", "probabilities": [0.6084704399108887, 0.021240098401904106, 0.06661248207092285, 0.3036770820617676]}, {"string": "But the separation method is not restricted to these two methods in practice.1.Linear Discriminant AnalysisLinear Discriminant Analysis (LDA) is a common scheme for feature extraction and dimensional reduction [11].", "probabilities": [0.6998952031135559, 0.02368776872754097, 0.04345513880252838, 0.23296189308166504]}, {"string": "We can use the transformation matrix provided by LDA as P in the source projection or H in the overall projection.2.Graph EmbeddingThe Graph Embedding method (GE) is defined as an algorithm that finds low-dimensional vector representation relationships among the vertices of graph G that best preserve similarities between the vertex pairs in G [21].", "probabilities": [0.22873610258102417, 0.03990953788161278, 0.22785204648971558, 0.503502368927002]}, {"string": "this assumption may not be valid for many applications such as object recognition, where only visual features are available [9].", "probabilities": [0.8465678095817566, 0.03760543093085289, 0.026890715584158897, 0.08893604576587677]}, {"string": "In multivariate statistical analysis, discrimination (also called separation as a more descriptive term) is a technique used for separating distinct sets of observations, and often provides the basis for a classification rule [10].", "probabilities": [0.7191869616508484, 0.01922769472002983, 0.03968632221221924, 0.22189904749393463]}, {"string": "According to the supervision information in the target domain, our approach belongs to the supervised heterogeneous domain adaptation in which only labeled data are used in the target domain [12].", "probabilities": [0.49540671706199646, 0.0960284173488617, 0.17704318463802338, 0.23152169585227966]}, {"string": "Therefore, ART-c is not appropriate for large problems [14].", "probabilities": [0.8228521347045898, 0.0452519915997982, 0.03315640240907669, 0.09873945266008377]}, {"string": "The second data set is a subset of the Reuters RCV1/RCV2 collections [26].", "probabilities": [0.20220007002353668, 0.06902693212032318, 0.5082442760467529, 0.220528706908226]}, {"string": "The algorithms are as follows: SVM_T: SVM_T trains an SVM classifier for each category only on the labeled instances from the target domain. KCCA: KCCA learns a common feature subspace by maximizing the correlation between source- and target-domain instances without using any label information [27]. HeMap: HeMap uses the spectral embedding technique to locate two projection matrices for a common subspace.", "probabilities": [0.08578649163246155, 0.009722852148115635, 0.3577558696269989, 0.5467347502708435]}, {"string": "We compare our results with the results reported in [9], which use HDA approaches on classification tasks.", "probabilities": [0.6991381049156189, 0.12653854489326477, 0.05347893387079239, 0.12084437161684036]}, {"string": "provided by Pennington et al. [26] as the representation of words in the datasets.", "probabilities": [0.7711980938911438, 0.05995699390769005, 0.049744151532649994, 0.11910076439380646]}, {"string": "Chung et al. [22] have demonstrated the superiority of the gated units in both effectiveness and efficiency.", "probabilities": [0.8489879369735718, 0.036304812878370285, 0.026404399424791336, 0.08830277621746063]}, {"string": "As for the SRNN model, it does not converge within the 60 epochs in training, which is due to the poor capacity of learning and the problem of vanishing gradient [27].", "probabilities": [0.7358047962188721, 0.022703049704432487, 0.040241073817014694, 0.20125111937522888]}, {"string": "Before optimization, all the weights in the networks are initialized simply by sampling from a flat uniform distribution in the range [ 0.1, 0.1], as Graves et al. [18] experimentally proved that the results of the RNN models are insensitive to either the distribution or the range of weight initialization.", "probabilities": [0.11099562048912048, 0.011087708175182343, 0.33059999346733093, 0.5473166704177856]}, {"string": "Enlightened by Jordan Networks [9], each input of the output layer node in the OLSRNN model not only comes from the output of the current hidden layer but also that of the output layer at the previous time step.", "probabilities": [0.6885234117507935, 0.10502336174249649, 0.07189919799566269, 0.1345539540052414]}, {"string": "Moreover, we equip the proposed model with a new hidden layer structure, which transforms some units based on the hidden layer structure of the LSTM model [10].", "probabilities": [0.1125020906329155, 0.010967192240059376, 0.32310929894447327, 0.5534213185310364]}, {"string": "Opinion target recognition can be deemed a problem of token-level sequence labelling, and each word in the sentence is assigned to a label with the standard BIO tagging scheme [3]: Label B means that the word is the beginning of an opinion target; Label I means that the word is the continuation of an opinion target; and label O means that the words are not part of any opinion targets.", "probabilities": [0.800764262676239, 0.04464961588382721, 0.04300818219780922, 0.11157792806625366]}, {"string": "For the first group, the most representative work using frequent term mining approach is proposed by Hu and Liu [4].", "probabilities": [0.6600150465965271, 0.043346554040908813, 0.050027891993522644, 0.2466103881597519]}, {"string": "Htay and Lynn [5] addressed the problem with the POS patterns of opinion targets and achieved an acceptable result.", "probabilities": [0.8463969230651855, 0.0384596548974514, 0.026715749874711037, 0.0884275957942009]}, {"string": "Hu and Liu [4] extracted nouns and noun phrases that appear at high frequency in the datasets by using association-rule mining; then, they used pruning search to obtain the true opinion targets in the candidate set.", "probabilities": [0.14558780193328857, 0.016322093084454536, 0.28933313488960266, 0.5487569570541382]}, {"string": "Jakob and Gurevych [7] trained a CRF model on the datasets of several domains together to solve the problem of cross-domain adaption.", "probabilities": [0.8364598155021667, 0.03553963452577591, 0.029960498213768005, 0.09804009646177292]}, {"string": "Other researchers comparatively preferred to use the conditional random field model [14] because of the HMM''s failure to handle long-range dependencies and the label bias problem in MaxEnt.", "probabilities": [0.815008282661438, 0.06364204734563828, 0.030670231208205223, 0.09067948907613754]}, {"string": "Following [17], we used linear SVM for simplicity and high performance.", "probabilities": [0.08745218813419342, 0.01005244068801403, 0.35610225796699524, 0.546393096446991]}, {"string": "Consider the extreme case,  =1, then lrp=rp.After replacing posterior estimators by maximum likelihood estimators for  1,  0 in rp,  share the same part, n11/n10, while the expression of rp is contradictory to rf.", "probabilities": [0.2325291782617569, 0.07654440402984619, 0.4662686288356781, 0.2246577888727188]}, {"string": "Nova was downloaded from the IJCNN 2006 performance prediction challenge, which comes from the 20 Newsgroup dataset [11].", "probabilities": [0.08674121648073196, 0.009778427891433239, 0.35638052225112915, 0.5470998883247375]}, {"string": "The kNN algorithm was from the Matlab toolbox, and linear SVM was from LIBSVM-3.21, integrated software [4].", "probabilities": [0.08537857234477997, 0.009677133522927761, 0.35825738310813904, 0.5466869473457336]}, {"string": "For multi-class text collections, the micro-averaged F1 measure was adopted [23].", "probabilities": [0.08542776852846146, 0.009679972194135189, 0.3581865429878235, 0.5467057228088379]}, {"string": "Another approach is to use terms weighted on the Kullback Leibler (KL) divergence measure between pairs of class-conditional term probabilities, and Jensen Shannon (JS) divergence for multi-class data [20].", "probabilities": [0.5005923509597778, 0.06848829984664917, 0.21438118815422058, 0.21653825044631958]}, {"string": "Another important step is to assign appropriate weights to terms according to their different semantic contribution in a document [23].", "probabilities": [0.6623880863189697, 0.06306035071611404, 0.107315793633461, 0.16723573207855225]}, {"string": "One approach is to weight terms based on their statistical confidence intervals [24].", "probabilities": [0.8318759799003601, 0.03857957944273949, 0.032165005803108215, 0.09737949073314667]}, {"string": "Following the rf method [17], we call this the relevance probability (rp).", "probabilities": [0.0853925421833992, 0.009677528403699398, 0.3582371771335602, 0.5466927886009216]}, {"string": "The matching score is the summation of the log probability ratios, which are used to derive relevance weighting functions [22].", "probabilities": [0.1641189604997635, 0.03689281642436981, 0.4875008761882782, 0.3114873468875885]}, {"string": "For HOFM, we use the parameter settings as per [9] where a regular grid of size 30   30   3 is created.", "probabilities": [0.08536689728498459, 0.009675530716776848, 0.3582712411880493, 0.5466862916946411]}, {"string": "For 3DCNN, a pre-trained network trained on the sports 1M dataset as per [21] was used to obtain the features.", "probabilities": [0.08617361634969711, 0.009761510416865349, 0.35724076628685, 0.5468240976333618]}, {"string": "After the final M-step i.e. estimation of T and   matrices, the action-vector for a given clip can be represented using the mean of its posterior distribution as .", "probabilities": [0.08558467775583267, 0.009698206558823586, 0.35799217224121094, 0.5467249155044556]}, {"string": "Using EM algorithm [20], we iteratively estimate the posterior mean and covariance in the E-step and use the same to update T and   in the M-step.", "probabilities": [0.08537226915359497, 0.009676492772996426, 0.3582656681537628, 0.5466856360435486]}, {"string": "For extracting both appearance and motion features simultaneously, in [12], 3D convolutional layers were added to the AE network above.", "probabilities": [0.20194239914417267, 0.06804186850786209, 0.5086938142776489, 0.22132189571857452]}, {"string": "Even when conducted in crowded scenes as in [1], the entire pickpocket incident is staged with apriori knowledge of how the incident is going to take place which makes analysis a lot easier.", "probabilities": [0.8290805220603943, 0.03664345294237137, 0.032743051648139954, 0.1015329584479332]}, {"string": "In [9], apart from magnitude and direction of motion, entropy information was further added to form a combined histogram of flow orientation, motion, and entropy (HOFME) descriptor.", "probabilities": [0.22628279030323029, 0.06559702754020691, 0.4596627354621887, 0.24845737218856812]}, {"string": "Peng et al. [25] changed the way by simply predicting a single dominant emotion, and used the modern deep learning to predict the emotion distribution of images and established a new database, i.e., Emotion6.", "probabilities": [0.7171996831893921, 0.019557500258088112, 0.040088292211294174, 0.2231544852256775]}, {"string": "Limmer and Lensch [29] proposed a method for transferring the RGB color spectrum to near-infrared (NIR) images using deep multi-scale convolutional neural networks.", "probabilities": [0.7250644564628601, 0.019422154873609543, 0.03898768872022629, 0.21652567386627197]}, {"string": "We establish the emotion classification network inspired by the image classification network structure [30], in which the global information of the image is acquired by full convolution.", "probabilities": [0.08546873182058334, 0.009684533812105656, 0.35815295577049255, 0.546693742275238]}, {"string": "In the field of cinema industry, when a movie image is analyzed emotionally, it can greatly enhance the performance of the work and express the specific emotion [1].", "probabilities": [0.8490675091743469, 0.03570644557476044, 0.026550205424427986, 0.08867592364549637]}, {"string": "Huang et al. [4] proposed a colorization approach to prevent the colorization process from bleeding over object boundaries.", "probabilities": [0.8442093729972839, 0.039662040770053864, 0.027157286182045937, 0.08897129446268082]}, {"string": "They adopted a 5-color theme in the work [20] and built a color theme database with emotion words.", "probabilities": [0.16158001124858856, 0.031121598556637764, 0.46187445521354675, 0.34542402625083923]}, {"string": "Datasets: In our experiments we have used real Internet traffic datasets [9] that are openly available from the WAND Network Research Group from the University of Waikato, New Zealand.", "probabilities": [0.08540418744087219, 0.009680703282356262, 0.35822728276252747, 0.5466878414154053]}, {"string": "Our datasets contain three real networks: (i) A Facebook-like social network containing 1899 users and 20,296 directed edges [18].", "probabilities": [0.20205409824848175, 0.068754643201828, 0.508484423160553, 0.22070692479610443]}, {"string": "Video streaming solutions fit into three general categories: streaming, downloading, and pseudo-streaming [9].", "probabilities": [0.22628676891326904, 0.07107061892747879, 0.47773510217666626, 0.22490756213665009]}, {"string": "The performance of Model I (Table 5) outperforms state-of-the-art recognizers [16] applied on the same dataset.", "probabilities": [0.6012645959854126, 0.21234825253486633, 0.07009156793355942, 0.11629553139209747]}, {"string": "And finally, we conduct our analysis on a longitudinal dataset involving 71 volunteer users carrying a smartphone over 17 months, from the Lausanne Data Collection Campaign [10].", "probabilities": [0.08607526868581772, 0.009798791259527206, 0.357529878616333, 0.5465959906578064]}, {"string": "Our research is based on two different mobile datasets: a large dataset from the NOKIA Lausanne Data Collection Campaign [16] and a preliminary dataset that we have collected during an unrelated experiment that was organized separately.", "probabilities": [0.08693475276231766, 0.009879887104034424, 0.3563920259475708, 0.5467933416366577]}, {"string": "To evaluate, we use an annotated acceleration dataset collected under controlled condition from 20 individuals [25].", "probabilities": [0.08536706864833832, 0.009676003828644753, 0.3582725524902344, 0.5466843843460083]}, {"string": "The fourth dataset is the interleaved activities of daily living (labelled as IAA) dataset from the CASAS smart home project [33].", "probabilities": [0.17856991291046143, 0.04656488448381424, 0.49908924102783203, 0.2757759094238281]}, {"string": "The third dataset is the PlaceLab Couple dataset [16].", "probabilities": [0.2021390199661255, 0.06897922605276108, 0.508343517780304, 0.22053825855255127]}, {"string": "We use the WiFi data from the StudentLife dataset [17] for this experiment.", "probabilities": [0.0853654146194458, 0.009675748646259308, 0.3582743704319, 0.5466845035552979]}, {"string": "In this experiment, we demonstrate our SECC model in extracting proximity contexts and communities of users that have similar proximity behaviors simultaneously from the Bluetooth data of the Reality Mining dataset [15].", "probabilities": [0.09237300604581833, 0.01069154217839241, 0.35033345222473145, 0.5466020107269287]}, {"string": "The StudentLife dataset2 [17] was collected from 48 students of Dartmouth College during 10 week spring term in 2013.", "probabilities": [0.0855364054441452, 0.009694492444396019, 0.35811522603034973, 0.5466539263725281]}, {"string": "The USC human activity dataset (USC-HAD)1 [14] was collected using the MotionNode sensing platform.", "probabilities": [0.08540810644626617, 0.009679215028882027, 0.3582160174846649, 0.5466966032981873]}, {"string": "Here, all compared algorithms assume the training set is restricted to the data collected on the same floor as the test location, an approaches replicated from a similar experimental environment [48].", "probabilities": [0.6262556910514832, 0.0577213354408741, 0.06070031598210335, 0.2553226947784424]}, {"string": "USC Human Activity Dataset (HAD): The dataset includes the most basic and common low-level human activities in daily life from a diverse group of human subjects [67].", "probabilities": [0.22770759463310242, 0.07123816758394241, 0.4759061336517334, 0.22514812648296356]}, {"string": "This dataset [61] was collected from sensors deployed in three rooms of an apartment of a 26-year-old male.", "probabilities": [0.08576975762844086, 0.00972768198698759, 0.35778599977493286, 0.5467166304588318]}, {"string": "The skill model is based on the concept presented in [1] and is expanded here to include online manual parametrization.", "probabilities": [0.22182847559452057, 0.49546799063682556, 0.09682964533567429, 0.18587394058704376]}, {"string": "The first experiment is an outdoor environment, the City Centre dataset, which was collected by Cummins and Newman [16].", "probabilities": [0.18466433882713318, 0.05088648200035095, 0.5013654828071594, 0.26308366656303406]}, {"string": "The City Centre dataset was collected by Cummins and Newman [16].", "probabilities": [0.08968158066272736, 0.009893359616398811, 0.352093905210495, 0.5483311414718628]}, {"string": "This dataset was collected from the corridor of a building without any moving objects by Angeli et al. [16].", "probabilities": [0.276374489068985, 0.016633393242955208, 0.19448374211788177, 0.5125083327293396]}, {"string": "A recent work [27] presents two approaches for extracting lines from laser data.", "probabilities": [0.8182199597358704, 0.05991979315876961, 0.030624166131019592, 0.09123603999614716]}, {"string": "[24] uses a hierarchical clustering approach to extract lines from the points obtained from laser data.", "probabilities": [0.17998364567756653, 0.02476620301604271, 0.2679177224636078, 0.5273324847221375]}, {"string": "Experimentation was performed using omni-directional images from two of our own outdoor datasets and panoramic images from the popular NewCollege dataset [16], all of which are publicly available.", "probabilities": [0.08541130274534225, 0.009680652990937233, 0.3582175672054291, 0.5466904640197754]}, {"string": "In a first experiment we evaluated the performance of FAB-MAP [6] (using the openFAB-MAP implementation) on the Nordland dataset.", "probabilities": [0.0853852927684784, 0.009677820838987827, 0.35824868083000183, 0.5466881990432739]}, {"string": "More experiments were carried out with the famous Killian Court dataset, collected and shared by Bosse et al. [50].", "probabilities": [0.16905330121517181, 0.015204768627882004, 0.26758819818496704, 0.5481536984443665]}, {"string": "We use the iCub humanoid robot [36] to collect a new dataset.", "probabilities": [0.08536610007286072, 0.00967563595622778, 0.35827288031578064, 0.5466854572296143]}, {"string": "(2) The trajectories and maps built from the open dataset provided by [25].", "probabilities": [0.19487357139587402, 0.060357656329870224, 0.5073320269584656, 0.23743680119514465]}, {"string": "In addition to these objects, we created models from the image dataset provided by Nister and Stewenius [25].", "probabilities": [0.08537030220031738, 0.009675910696387291, 0.3582668900489807, 0.5466868877410889]}, {"string": "We evaluate our system in five different datasets with sets of from 7 to 500 objects: the Desktop dataset, used for testing purposes; one of the sequences of the RGB-D SLAM Dataset [46], which provides ground truth of the camera pose; the Aroa s room dataset, a child s real room with dozens of different objects; the Snack dataset, a sequence that shows several instances of the same object models and forces camera relocation; and the Snack with clutter dataset, a small area with repeated objects in a small space with occlusion and background clutter.", "probabilities": [0.08650773763656616, 0.009851209819316864, 0.3569871485233307, 0.5466538667678833]}, {"string": "This dataset [3] consists of left and right view images collected  roughly  with a spatial frequency of 1.5 m by a Segway robot along a 2 km path in a urban environment.", "probabilities": [0.0942402184009552, 0.011162137612700462, 0.37542036175727844, 0.5191773772239685]}, {"string": "This dataset [3] consists of left and right images collected with a spatial frequency of 1.5 m by a Segway robot along a 1.9 km path in a university campus.", "probabilities": [0.08543789386749268, 0.009683375246822834, 0.3582795560359955, 0.5465991497039795]}, {"string": "This dataset [41] was acquired in a university parking area using an electric car equipped with two Firewire colour cameras.", "probabilities": [0.08545314520597458, 0.009683415293693542, 0.35815757513046265, 0.5467059016227722]}, {"string": "To further test the proposed algorithm, we use the Bicocca 25b dataset from the RAWSEEDS project [40].", "probabilities": [0.08536406606435776, 0.009675558656454086, 0.3582758605480194, 0.5466845035552979]}, {"string": "In particular, a qualitative test is conducted on the New College dataset [39], where we examine the different types of bases (raw images and descriptors) in order to show the flexibility of basis representation of our approach as well as the robustness to dynamics in the scene.", "probabilities": [0.2530735731124878, 0.0457448735833168, 0.21269378066062927, 0.4884878396987915]}, {"string": "Our model extends the recent state-of-the-art semantic segmentation graphical model from [7 by incorporating boat roll and pitch measurements from the on-board IMU.", "probabilities": [0.23211118578910828, 0.29451531171798706, 0.18603824079036713, 0.28733521699905396]}, {"string": "This paper extends and revises [59] with an extended introduction motivating the design of Object Grammars, additional details on the implementation of Object Grammars in Ens  (Section 5), an additional case-study to evaluate Object Grammars (Section 6), and additional directions for further research (Section 8).", "probabilities": [0.22852882742881775, 0.6379311084747314, 0.06854116171598434, 0.06499888747930527]}, {"string": "This paper revises and extends the work presented in [6] by addressing the explicit modeling of assumptions about the environment in specifications, independent from those about adaptation impact.", "probabilities": [0.055353835225105286, 0.9174538254737854, 0.014370175078511238, 0.01282207015901804]}, {"string": "This paper extends our previous work [12] in several ways.", "probabilities": [0.09761786460876465, 0.8458542227745056, 0.028614496812224388, 0.027913356199860573]}, {"string": "In our experiments, we selected a color harmony evaluation subset (CHE-dataset) from AVA dataset [38] for the training and evaluation purpose.", "probabilities": [0.08548744767904282, 0.009689648635685444, 0.3581206798553467, 0.5467021465301514]}, {"string": "To evaluate our algorithm, we have conducted extensive experiments and made the comparisons on a number of datasets regarding different multimedia applications: ADL-RFID[50]: It contains 10 housekeeping activities (vacuuming, ironing, dusting, brooming, mopping, cleaning windows, making bed, watering plants, washing dishes, and setting the table).", "probabilities": [0.0953032597899437, 0.011012173257768154, 0.3468290865421295, 0.5468555092811584]}, {"string": "In total, KTH contains 599 video clips (2391 sequences) with the resolution of 160 120 pixels. MIML dataset[53]: It consists of 2000 natural scene images belonging to the classes desert, mountains, sea, sunset, and trees.", "probabilities": [0.2470143735408783, 0.10567347705364227, 0.4313820004463196, 0.21593011915683746]}, {"string": "The experimental data are mainly based on the collection of TREC video retrieval evaluation (TRECVID) [29] provided by the National Institute of Standards and Technology (NIST).", "probabilities": [0.08642605692148209, 0.009734610095620155, 0.35674765706062317, 0.5470916628837585]}, {"string": "To have a fair comparison, we have implemented the method proposed by Lv and Wang [14] and evaluated it on the same dataset in this experiment.", "probabilities": [0.08686871081590652, 0.009953717701137066, 0.3567289710044861, 0.5464485883712769]}, {"string": "The original image dataset contains 500 different images which are in BMP format and of various sizes; some of them are selected from internet [27] and some are captured with digital cameras by ourselves.", "probabilities": [0.22782251238822937, 0.07540592551231384, 0.473280131816864, 0.22349151968955994]}, {"string": "UCF50[4]: It contains 50 action categories and 6617 action videos which are realistic videos taken from Youtube.", "probabilities": [0.2073482871055603, 0.07213147729635239, 0.49992597103118896, 0.22059427201747894]}, {"string": "As can be seen, even one degree of error in the initial estimation of only two interferers  directions, causes significant SINR degradation for all methods except our proposed method and method in [22].", "probabilities": [0.6270382404327393, 0.07012483477592468, 0.05551127716898918, 0.247325599193573]}, {"string": "Increasing the number of nulls in the transmit beam-pattern with fixed number of transmit antennas, decreases the nulls depths [22].", "probabilities": [0.21686142683029175, 0.07061699777841568, 0.48931077122688293, 0.22321079671382904]}, {"string": "The optimum value of wLMr 1 using MVDR [31]", "probabilities": [0.211244136095047, 0.02137405052781105, 0.23458945751190186, 0.5327923893928528]}, {"string": "Consider a point like target at direction  0 and Q signal-dependent interference sources (point like scatterers or digital radio frequency memory repeat jammer [32]) at directions  i,i=1,.,Q.", "probabilities": [0.22081974148750305, 0.07138034701347351, 0.4841017425060272, 0.22369816899299622]}, {"string": "Finally, the BPSK waveforms which realise this covariance matrix are generated as follows [13] where X is the transmit waveforms matrix and   is a matrix including zero mean and unit variance Gaussian random variables.", "probabilities": [0.20214498043060303, 0.06895875185728073, 0.5083274245262146, 0.22056885063648224]}, {"string": "At first, we evaluate the null generating efficiency of the proposed method compared to the method in [22].", "probabilities": [0.6259844303131104, 0.07082541286945343, 0.05570409074425697, 0.24748606979846954]}, {"string": "Also, because of decreasing the number of total antennas compared to the previous simulation, the performance of the method in [23] outperforms phased array counterpart.", "probabilities": [0.6272205114364624, 0.07129935920238495, 0.05568767338991165, 0.2457924336194992]}, {"string": "Obtaining data from the Web has become second nature to most users [29], and the Internet provides a familiar interface for both interacting with and controlling a simulation.", "probabilities": [0.8500696420669556, 0.03552327677607536, 0.026241907849907875, 0.08816507458686829]}, {"string": "This dataset includes a total of 340,184 traffic accident records [19].", "probabilities": [0.20335577428340912, 0.06931469589471817, 0.5066307783126831, 0.220698744058609]}, {"string": "Lee et al. (2009) combined words, reading and natural speech.", "probabilities": [0.715354859828949, 0.05940850451588631, 0.07914076000452042, 0.1460959017276764]}, {"string": "Moreover, the voice of people with Down syndrome showed significantly reduced formant amplitude intensity levels (Pentz Jr, 1987).", "probabilities": [0.8364869952201843, 0.038393229246139526, 0.03037986531853676, 0.09473980963230133]}, {"string": "A similar methodology was used by Escudero et al. (2017), where the characteristic prosodic patterns of the style of different groups of speakers was investigated.", "probabilities": [0.6421409845352173, 0.05671747028827667, 0.05311775207519531, 0.24802377820014954]}, {"string": "The procedure described in Escudero et al. (2017) for transferring prosody is used in the experiments reported in this paper.", "probabilities": [0.6467652320861816, 0.02669869177043438, 0.055852584540843964, 0.2706834673881531]}, {"string": "Many DS individuals have some physiological peculiarities that affect their voice production, such as a smaller vocal tract with respect to the tongue size or soft palatal shape, among others Guimaraes et al. (2008).", "probabilities": [0.849949836730957, 0.03558693453669548, 0.026262061670422554, 0.08820126205682755]}, {"string": "Acoustic low-level descriptors (LLD) and temporal features were automatically extracted from each recording using the openSmile toolkit (Eyben et al., 2013).", "probabilities": [0.08537346869707108, 0.009675951674580574, 0.35826191306114197, 0.5466887354850769]}, {"string": "Therefore, the Praat software (Boersma, 2006) was used to extract all silences from each recording and these silences were excluded from the analysis process.", "probabilities": [0.08598870038986206, 0.009710350073873997, 0.35737067461013794, 0.5469302535057068]}, {"string": "The unweighted average recall (UAR) (Schuller et al., 2016) was also used.", "probabilities": [0.08560588210821152, 0.009689421392977238, 0.3579280972480774, 0.5467766523361206]}, {"string": "Albertini et al. (2010) found lower shimmer (amplitude perturbations) in male adults with Down syndrome than in adults without intellectual disabilities.", "probabilities": [0.6547584533691406, 0.22527936100959778, 0.03712157532572746, 0.08284061402082443]}, {"string": "Once the segmentation was corrected, a prosody transfer algorithm implemented in Praat (Boersma, 2006) was executed.", "probabilities": [0.08580374717712402, 0.009703737683594227, 0.3576541543006897, 0.5468382835388184]}, {"string": "The current work extends the study discussed in Ref. [42] based on a One-class classification paradigm.", "probabilities": [0.18030378222465515, 0.6508484482765198, 0.055910903960466385, 0.11293693631887436]}, {"string": "The Dispatcher gathers raw logs and data from event sources through data transfer protocols like Syslog [10].", "probabilities": [0.21877795457839966, 0.014722149819135666, 0.22904115915298462, 0.5374587774276733]}, {"string": "Relationships that exist professionally and personally are susceptible to structural shifts as smartphones effect personal privacy while blending and expanding social networks (Lugano, 2008).", "probabilities": [0.850361704826355, 0.03544720634818077, 0.026157941669225693, 0.08803312480449677]}, {"string": "Van Dijk (1989) defines discourse as a specific form of language that expands beyond the boundaries of semantic presentation.", "probabilities": [0.8504871129989624, 0.03541414439678192, 0.02612389624118805, 0.08797478675842285]}, {"string": "Building upon the robustness of TAM, researchers expanded the model to TAM2, adding the impacts of three interrelated social forces on individuals  adoption of a new technology (Venkatesh and Davis, 2000).", "probabilities": [0.6325582265853882, 0.05801858752965927, 0.12344731390476227, 0.1859758198261261]}, {"string": "Network externalities mean that users can get additional values as mobile IM user network expands (Strader et al., 2007).", "probabilities": [0.7304491400718689, 0.06755781918764114, 0.06523852050304413, 0.13675451278686523]}, {"string": "Several studies, as reported by McDonald and Ho (2002), indicated that unless extreme values of skewness and kurtosis are detected, the use of the Maximum Likelihood estimation generates robustness in the case of multivariate nonnormality and, hence, the parameter estimates maintain their validity.", "probabilities": [0.7429631352424622, 0.06714468449354172, 0.059977348893880844, 0.1299148052930832]}, {"string": "A study by Gross (2004) reported that both genders were embracing the Internet as a means of communicating with their friends.", "probabilities": [0.847517192363739, 0.037662945687770844, 0.026543866842985153, 0.08827590942382812]}, {"string": "All queries, like the descriptions in Section 4.3, are specified in Manchester Syntax [28].", "probabilities": [0.2512006461620331, 0.07263307273387909, 0.4480190873146057, 0.22814719378948212]}, {"string": "The third dataset is composed of 45 pairs of real-world ontologies coming from the Consensus Workshop track [33] of the OAEI contest 2006 (pairs result from all combinations per two).", "probabilities": [0.18322762846946716, 0.04957235977053642, 0.5005566477775574, 0.2666434049606323]}, {"string": "The first dataset has been derived from the benchmarking series of the OAEI contest [31].", "probabilities": [0.10786785185337067, 0.01207756157964468, 0.3309875428676605, 0.5490670204162598]}, {"string": "The first dataset, which we dubbed the GIS Transportation Dataset, was created from instance data of the Road and Ferries package of a GIS data model known as GDF (Geographic Data Files) [21].", "probabilities": [0.08869346231222153, 0.009876781143248081, 0.3535962700843811, 0.5478335022926331]}, {"string": "Resources comprise RDF data from DBpedia [3] and documents from Wikipedia.", "probabilities": [0.2685473561286926, 0.06401920318603516, 0.42191481590270996, 0.24551869928836823]}, {"string": "The Hermes system [42] targets semantic search in a multi-dataset scenario.", "probabilities": [0.2665698826313019, 0.016825634986162186, 0.20537815988063812, 0.5112262964248657]}, {"string": "In terms of diversity, the corpus consists of RDF from 783 pay-level-domains [32].", "probabilities": [0.7606019377708435, 0.0564374104142189, 0.05592266470193863, 0.12703797221183777]}, {"string": "This resulted in rich datasets, most prominently the DBpedia [3] corpus extracted from semi-structured WikipediA articles.", "probabilities": [0.20369303226470947, 0.06730709969997406, 0.5065430998802185, 0.22245672345161438]}, {"string": "Most recently, Grimnes [22] provided a detailed analysis of the BTC-201010 dataset.", "probabilities": [0.8500033020973206, 0.03550633043050766, 0.026271747425198555, 0.0882185623049736]}, {"string": "Data is provided by different, connected data sources using different publishing strategies like static RDF documents and SPARQL endpoints [1].", "probabilities": [0.08593686670064926, 0.009705008007586002, 0.35744357109069824, 0.5469145774841309]}, {"string": "The Restaurant data set [23] contains a set of records from the Fodor s and Zagat s restaurant guides.", "probabilities": [0.20270667970180511, 0.06774533540010452, 0.5077611207962036, 0.22178688645362854]}, {"string": "Thus, we have built up a dataset of 2260 web resources from BioCatalogue [27], which is a popular registry in the Life Sciences domain.", "probabilities": [0.08572065830230713, 0.009736682288348675, 0.3579406142234802, 0.5466020703315735]}, {"string": "We use the MSH-WSD dataset [22] for evaluating this phase.", "probabilities": [0.08536601811647415, 0.009675797075033188, 0.3582735061645508, 0.5466846823692322]}, {"string": "Like in data integration systems [2], the mapping is crucial for keeping the conceptual representation of the domain independent from the implementation issues, and for masking the user from all the details and the idiosyncrasies of the data sources.", "probabilities": [0.8363436460494995, 0.03792452812194824, 0.030204540118575096, 0.09552732855081558]}, {"string": "The second baseline, Harvard General Inquirer (INQ) [31], has words manually classified syntactically, semantically and grammatically.", "probabilities": [0.2033504992723465, 0.06926588714122772, 0.5066689848899841, 0.22071465849876404]}, {"string": "For our next experiment we use the MUTAG dataset, which is distributed as an example dataset for the DL-Learner9 [37] toolkit.", "probabilities": [0.08544065803289413, 0.009688043966889381, 0.35819169878959656, 0.546679675579071]}, {"string": "For our fifth classification experiment we use a dataset from the Amsterdam Museum [38], which contains information about around 70000 artifacts in the museum s collection.", "probabilities": [0.08541514724493027, 0.009683629497885704, 0.3582189381122589, 0.5466822981834412]}, {"string": "For our third experiment we use a dataset from the British Geological Survey (BGS),10 as we did earlier in [9].", "probabilities": [0.08651284873485565, 0.00988137349486351, 0.3570737838745117, 0.5465319752693176]}, {"string": "The sources were extracted from 8 online datasets, some referenced on the Billion Triples Challenge (BTC) 2012 Dataset webpage [36].", "probabilities": [0.08649244904518127, 0.00973287783563137, 0.35663431882858276, 0.54714035987854]}, {"string": "All resources related to the experiments, including dataset and queries, can be found on [35] (queries are also included in Appendix A).", "probabilities": [0.839368462562561, 0.037305254489183426, 0.029677152633666992, 0.09364911913871765]}, {"string": "In particular, the method exploits the Articles Categories dataset, which links Wikipedia titles to categories using the SKOS vocabulary [13].", "probabilities": [0.11806178838014603, 0.011174364015460014, 0.3166826665401459, 0.5540811419487]}, {"string": "In addition to NOAA, LSM sensor data sources [4] offer weather data of 60,000 places around the worlds via Web-based APIs from weather data providers.", "probabilities": [0.09211315214633942, 0.010480650700628757, 0.362261027097702, 0.5351451635360718]}, {"string": "The news article dataset contains 100 randomly picked Reuters articles from the CoNLL-YAGO dataset [20].", "probabilities": [0.20194093883037567, 0.0677952691912651, 0.5087396502494812, 0.22152410447597504]}, {"string": "We create a test dataset by randomly sampling 12k documents from the FACC1 corpus [65], a preprocessed version of the ClueWeb dataset where entity mentions have been linked to their corresponding Freebase identifiers.", "probabilities": [0.0854148119688034, 0.009683859534561634, 0.35823187232017517, 0.5466694831848145]}, {"string": "We used the Los Angeles Times/Washington Post (henceforth LTW) part of the English Gigaword v5 corpus [62].", "probabilities": [0.08536230027675629, 0.009675279259681702, 0.3582778871059418, 0.5466845631599426]}, {"string": "This paper extends the findings from this study [9 and reviews the initial requirements, to feed into the current exploratory and analytical stage.", "probabilities": [0.05537400394678116, 0.9174212217330933, 0.014377572573721409, 0.01282726414501667]}, {"string": "Sternitzke [11] expands, among others, on the way the German PATON patent information centre, Ilmenau can handle IP support programmes (in that particular case, the German SIGNO programme which extends financial support to first-time SME patentees).", "probabilities": [0.8459957838058472, 0.037656959146261215, 0.027075953781604767, 0.08927127718925476]}, {"string": "Parts of this work were presented in IRFS 2011 [33].", "probabilities": [0.7627300024032593, 0.024370891973376274, 0.03996928036212921, 0.17292983829975128]}, {"string": "In clustering analysis, we select the squared Euclidean distance [17] for measuring the distances of variables and between-groups linkage method for clustering groups.", "probabilities": [0.08543828874826431, 0.009681258350610733, 0.358174592256546, 0.5467059016227722]}, {"string": "As an example, consider European publication EP1598090 described by C. Denis and R. Menidjel [21].", "probabilities": [0.8482721447944641, 0.035192910581827164, 0.026869699358940125, 0.0896652489900589]}, {"string": "It would be worthwhile for the managers if the approaches are made more efficient and flexible to offer multiple suggestions for devising strategies [25].", "probabilities": [0.6988488435745239, 0.0764128714799881, 0.07727940380573273, 0.14745894074440002]}, {"string": "For instance, the tools using SAO based extraction techniques also extract certain irrelevant structures [34].", "probabilities": [0.7163504958152771, 0.02006206475198269, 0.040217895060777664, 0.2233695536851883]}, {"string": "The SAO structures are extracted directly from the patent documents [33].", "probabilities": [0.23497331142425537, 0.015242243185639381, 0.21866987645626068, 0.5311145186424255]}, {"string": "For example we know that users at the beginning of their tasks, are less likely to start their initial queries by introducing all the search terms [34].", "probabilities": [0.8357911705970764, 0.03919690474867821, 0.03025933727622032, 0.0947524905204773]}, {"string": "To support feature computation and combination in GATE (see Section 2), we make use of the SUMMA summarization library [25].", "probabilities": [0.0853649452328682, 0.009675581008195877, 0.35827454924583435, 0.5466849207878113]}, {"string": "To query the lexical databases we use the open source thesaurus management software TheW32 [29].", "probabilities": [0.08536235243082047, 0.00967526063323021, 0.3582777678966522, 0.5466846227645874]}, {"string": "To compute the score, we use a series of features, similar to those used in state-of-the-art extractive summarization (see, e.g. [31]), but adapted to patent material.", "probabilities": [0.08740340173244476, 0.009799939580261707, 0.35539010167121887, 0.5474066138267517]}, {"string": "Using the strategy described in Section 3.3, we extract the most important concepts for each time slice in the stream and display them as word clouds [33].", "probabilities": [0.08545944094657898, 0.009680679067969322, 0.35813745856285095, 0.546722412109375]}, {"string": "It is computed by comparing all patents in the past of a given patent to all follow up ( future ) patents [5].", "probabilities": [0.7065417766571045, 0.05685962364077568, 0.08487214148044586, 0.15172655880451202]}]